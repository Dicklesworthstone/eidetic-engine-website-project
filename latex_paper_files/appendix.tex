\documentclass[12pt,a4paper]{article}

% Include only essential packages needed for the code
\usepackage{fontspec}
\usepackage{minted}
\usepackage{tcolorbox}
\usepackage{caption}
\usepackage[a4paper, margin=1.5cm]{geometry} % Changed from 2cm to 1.5cm
\tcbuselibrary{minted,breakable,skins}
\usepackage{amssymb} % Needed for checkmark etc symbols if not using emoji font directly
\usepackage{enumitem} % For customizing lists if needed

% Define your emoji commands (ensure Noto Color Emoji is installed)
% Or use text symbols if emoji font is problematic
\usepackage{iftex} % To check engine
\ifTUTeX % XeTeX or LuaTeX
    \newfontfamily\emojifont{Noto Color Emoji}[Renderer=Harfbuzz] % Use Harfbuzz with LuaTeX if needed
    \newcommand{\emoji}[1]{\emojifont #1}
    \newcommand{\checkmark}{\emoji{‚úÖ}}
    \newcommand{\xmark}{\emoji{‚ùå}}
    \newcommand{\skipmark}{\emoji{‚è≠}}
    \newcommand{\waiting}{\emoji{‚è≥}}
    \newcommand{\question}{\emoji{‚ùì}}
    \newcommand{\refresh}{\emoji{üîÑ}}
    \newcommand{\search}{\emoji{üîç}}
    \newcommand{\note}{\emoji{üìù}}
    \newcommand{\save}{\emoji{üíæ}}
    \newcommand{\warning}{\emoji{‚ö†Ô∏è}}
    \newcommand{\timer}{\emoji{‚è±Ô∏è}}
    \newcommand{\gear}{\emoji{‚öôÔ∏è}}
    \newcommand{\rocket}{\emoji{üöÄ}}
    \newcommand{\database}{\emoji{üõ¢Ô∏è}} % Alternative if specific emoji not found
    \newcommand{\brain}{\emoji{üß†}}
    \newcommand{\error}{\emoji{üÜò}} % Alternative if needed
    \newcommand{\whitecheckmark}{\emoji{‚úÖ}} % Map to standard checkmark
    \newcommand{\rewind}{\emoji{‚è™}}
\else % pdfLaTeX
    \newcommand{\checkmark}{\checkmark} % Use amssymb checkmark
    \newcommand{\xmark}{\ding{55}} % Use pifont cross
    \newcommand{\skipmark}{$\gg$}
    \newcommand{\waiting}{$\dots$}
    \newcommand{\question}{?}
    \newcommand{\refresh}{$\circlearrowleft$} % Use standard symbol
    \newcommand{\search}{$\bigcirc$\kern-0.7em\raisebox{0.1ex}{\tiny\bf ?}} % Simple representation
    \newcommand{\note}{$\square$}
    \newcommand{\save}{[S]}
    \newcommand{\warning}{[!] }
    \newcommand{\timer}{[T]}
    \newcommand{\gear}{$\gear$} % Use standard symbol if available
    \newcommand{\rocket}{[R]}
    \newcommand{\database}{[DB]}
    \newcommand{\brain}{[B]}
    \newcommand{\error}{[E]}
    \newcommand{\whitecheckmark}{\checkmark}
    \newcommand{\rewind}{$\ll$}
    \usepackage{pifont} % For cross mark
    \usepackage{amsmath, amssymb} % For math symbols
\fi

% Inline code command
\newcommand{\code}[1]{\texttt{#1}}

% Define your pageablecode environment
\definecolor{verylightgray}{RGB}{248, 248, 248}
\newtcolorbox{pageablecode}[1][]{
  breakable,
  enhanced,
  colback=verylightgray,
  colframe=verylightgray,
  arc=3mm,
  boxrule=0pt,
  left=3mm,
  right=3mm,
  top=3mm,
  bottom=3mm,
  before skip=8pt,
  after skip=8pt,
  boxsep=0pt,
  sharp corners=southeast,
  sharp corners=northeast,
  sharp corners=southwest,
  sharp corners=northwest,
  #1
}

% Caption setup
\usepackage{titlesec} % Required for caption setup below
\DeclareCaptionFont{smallfont}{\footnotesize}
\captionsetup{
  font={smallfont},
  labelfont=bf,
  singlelinecheck=false,
  skip=10pt
}

% Adjust list spacing for dense technical analysis
\setlist{noitemsep, topsep=3pt, partopsep=0pt}

\begin{document}

\appendix
\section{Appendix A: Unified Memory System (UMS) Technical Analysis}

% --- START OF UMS TECHNICAL ANALYSIS ---

\subsection*{Technical Analysis of Unified Agent Memory and Cognitive System}

\subsubsection*{System Overview and Architecture}

The provided code implements a sophisticated `Unified Agent Memory and Cognitive System` designed for LLM agents. This system combines a structured memory hierarchy with process tracking, reasoning capabilities, and knowledge management. It's built as an asynchronous Python module using SQLite for persistence with sophisticated memory organization patterns.

\subsubsection*{Core Architecture}

The system implements a cognitive architecture with four distinct memory levels:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Working Memory}: Temporarily active information (30-minute default TTL)
    \item \textbf{Episodic Memory}: Experiences and event records (7-day default TTL)
    \item \textbf{Semantic Memory}: Knowledge, facts, and insights (30-day default TTL)
    \item \textbf{Procedural Memory}: Skills and procedures (90-day default TTL)
\end{enumerate}

These are implemented through a SQLite database using `aiosqlite` for asynchronous operations, with optimized configuration:

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
DEFAULT_DB_PATH = os.environ.get("AGENT_MEMORY_DB_PATH", "unified_agent_memory.db")
MAX_TEXT_LENGTH = 64000  # Maximum for text fields
CONNECTION_TIMEOUT = 10.0  # seconds
ISOLATION_LEVEL = None  # SQLite autocommit mode

# Memory management parameters
MAX_WORKING_MEMORY_SIZE = int(os.environ.get("MAX_WORKING_MEMORY_SIZE", "20"))
DEFAULT_TTL = {
    "working": 60 * 30,       # 30 minutes
    "episodic": 60 * 60 * 24 * 7, # 7 days
    "semantic": 60 * 60 * 24 * 30, # 30 days
    "procedural": 60 * 60 * 24 * 90 # 90 days
}
MEMORY_DECAY_RATE = float(os.environ.get("MEMORY_DECAY_RATE", "0.01"))  # Per hour
\end{minted}
\end{pageablecode}

The system uses various SQLite optimizations through pragmas:

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
SQLITE_PRAGMAS = [
    "PRAGMA journal_mode=WAL",  # Write-Ahead Logging
    "PRAGMA synchronous=NORMAL",  # Balance durability and performance
    "PRAGMA foreign_keys=ON",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA cache_size=-32000",  # ~32MB cache
    "PRAGMA mmap_size=2147483647",  # Memory-mapped I/O
    "PRAGMA busy_timeout=30000"  # 30-second timeout
]
\end{minted}
\end{pageablecode}

\subsubsection*{Type System and Enumerations}

The code defines comprehensive type hierarchies through enumerations:

\subsubsection*{Workflow and Action Status}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
class WorkflowStatus(str, Enum):
    ACTIVE = "active"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    ABANDONED = "abandoned"

class ActionStatus(str, Enum):
    PLANNED = "planned"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"
\end{minted}
\end{pageablecode}

\subsubsection*{Content Classification}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
class ActionType(str, Enum):
    TOOL_USE = "tool_use"
    REASONING = "reasoning"
    PLANNING = "planning"
    RESEARCH = "research"
    ANALYSIS = "analysis"
    DECISION = "decision"
    OBSERVATION = "observation"
    REFLECTION = "reflection"
    SUMMARY = "summary"
    CONSOLIDATION = "consolidation"
    MEMORY_OPERATION = "memory_operation"

class ArtifactType(str, Enum):
    FILE = "file"
    TEXT = "text"
    IMAGE = "image"
    TABLE = "table"
    CHART = "chart"
    CODE = "code"
    DATA = "data"
    JSON = "json"
    URL = "url"

class ThoughtType(str, Enum):
    GOAL = "goal"
    QUESTION = "question"
    HYPOTHESIS = "hypothesis"
    INFERENCE = "inference"
    EVIDENCE = "evidence"
    CONSTRAINT = "constraint"
    PLAN = "plan"
    DECISION = "decision"
    REFLECTION = "reflection"
    CRITIQUE = "critique"
    SUMMARY = "summary"
\end{minted}
\end{pageablecode}

\subsubsection*{Memory System Types}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
class MemoryLevel(str, Enum):
    WORKING = "working"
    EPISODIC = "episodic"
    SEMANTIC = "semantic"
    PROCEDURAL = "procedural"

class MemoryType(str, Enum):
    OBSERVATION = "observation"
    ACTION_LOG = "action_log"
    TOOL_OUTPUT = "tool_output"
    ARTIFACT_CREATION = "artifact_creation"
    REASONING_STEP = "reasoning_step"
    FACT = "fact"
    INSIGHT = "insight"
    PLAN = "plan"
    QUESTION = "question"
    SUMMARY = "summary"
    REFLECTION = "reflection"
    SKILL = "skill"
    PROCEDURE = "procedure"
    PATTERN = "pattern"
    CODE = "code"
    JSON = "json"
    URL = "url"
    TEXT = "text"

class LinkType(str, Enum):
    RELATED = "related"
    CAUSAL = "causal"
    SEQUENTIAL = "sequential"
    HIERARCHICAL = "hierarchical"
    CONTRADICTS = "contradicts"
    SUPPORTS = "supports"
    GENERALIZES = "generalizes"
    SPECIALIZES = "specializes"
    FOLLOWS = "follows"
    PRECEDES = "precedes"
    TASK = "task"
    REFERENCES = "references"
\end{minted}
\end{pageablecode}

\subsubsection*{Database Schema}

The system uses a sophisticated relational database schema with 15+ tables and numerous indices:

\begin{enumerate}[label=\arabic*.]
    \item \code{workflows}: Tracks high-level workflow containers
    \item \code{actions}: Records agent actions and tool executions
    \item \code{artifacts}: Stores outputs and files created during workflows
    \item \code{though\1\_chains}: Groups related thoughts (reasoning processes)
    \item \code{thoughts}: Individual reasoning steps and insights
    \item \code{memories}: Core memory storage with metadata and classification
    \item \code{memor\1\_links}: Associative connections between memories
    \item \code{embeddings}: Vector embeddings for semantic search
    \item \code{cognitiv\1\_states}: Snapshots of agent cognitive state
    \item \code{reflections}: Meta-cognitive analysis outputs
    \item \code{memor\1\_operations}: Audit log of memory system operations
    \item \code{tags, workflo\1\_tags, actio\1\_tags, artifac\1\_tags}: Tagging system
    \item \code{dependencies}: Tracks dependencies between actions
    \item \code{memor\1\_fts}: Virtual FTS5 table for full-text search
\end{enumerate}

Each table has appropriate foreign key constraints and indexes for performance optimization. The schema includes circular references between memories and thoughts, implemented with deferred constraints.

\subsubsection*{Connection Management}

The database connection is managed through a sophisticated singleton pattern:

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
class DBConnection:
    """Context manager for database connections using aiosqlite."""

    _instance: Optional[aiosqlite.Connection] = None
    _lock = asyncio.Lock()
    _db_path_used: Optional[str] = None
    _init_lock_timeout = 15.0  # seconds

    # Methods for connection management, initialization, transaction handling, etc.
\end{minted}
\end{pageablecode}

Key features include:
\begin{itemize}
    \item Asynchronous context manager pattern with \1\__aente\1\__` and \1\__aexi\1\__`
    \item Lock-protected singleton initialization with timeout
    \item Transaction context manager with automatic commit/rollback
    \item Schema initialization on first connection
    \item Custom SQLite function registration
\end{itemize}

\subsubsection*{Utility Functions}

The system includes several utility classes and functions:

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def to_iso_z(ts: float) -> str:
    """Converts Unix timestamps to ISO-8601 with Z suffix."""
    # Implementation

class MemoryUtils:
    """Utility methods for memory operations."""

    @staticmethod
    def generate_id() -> str:
        """Generate a unique UUID V4 string for database records."""
        return str(uuid.uuid4())

    # Methods for serialization, validation, sequence generation, etc.
\end{minted}
\end{pageablecode}

Additional utility methods include:
\begin{itemize}
    \item JSON serialization with robust error handling and truncation
    \item SQL identifier validation to prevent injection
    \item Tag processing to maintain taxonomies
    \item Access tracking to update statistics
    \item Operation logging for audit trails
\end{itemize}

\subsubsection*{Vector Embeddings and Semantic Search}

The system integrates with an external embedding service:

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# Embedding configuration
DEFAULT_EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSION = 384  # For the default model
SIMILARITY_THRESHOLD = 0.75
\end{minted}
\end{pageablecode}

Implementation includes:
\begin{itemize}
    \item \1\_stor\1\_embedding()`: Generates and stores vector embeddings with error handling
    \item \1\_fin\1\_simila\1\_memories()`: Performs semantic search with cosine similarity and filtering
    \item Integration with scikit-learn for similarity calculations
\end{itemize}

\subsubsection*{Memory Relevance Calculation}

The system implements a sophisticated memory relevance scoring algorithm:

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _compute_memory_relevance(importance, confidence, created_at, access_count, last_accessed):
    """Computes a relevance score based on multiple factors."""
    now = time.time()
    age_hours = (now - created_at) / 3600 if created_at else 0
    recency_factor = 1.0 / (1.0 + (now - (last_accessed or created_at)) / 86400)
    decayed_importance = max(0, importance * (1.0 - MEMORY_DECAY_RATE * age_hours))
    usage_boost = min(1.0 + (access_count / 10.0), 2.0) if access_count else 1.0
    relevance = (decayed_importance * usage_boost * confidence * recency_factor)
    return min(max(relevance, 0.0), 10.0)
\end{minted}
\end{pageablecode}

This function factors in:
\begin{itemize}
    \item Base importance score (1-10 scale)
    \item Time-based decay of importance
    \item Usage frequency boost
    \item Confidence weighting
    \item Recency bias
\end{itemize}

\subsubsection*{Core Memory Operations}

The system implements a comprehensive set of operations for memory management through tool functions, each designed with standardized error handling and metrics tracking via decorators (`@wit\1\_too\1\_metrics`, `@wit\1\_erro\1\_handling`).

\subsubsection*{Memory Creation and Storage}

The primary function for creating memories is `stor\1\_memory()`:

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def store_memory(
    workflow_id: str,
    content: str,
    memory_type: str,
    memory_level: str = MemoryLevel.EPISODIC.value,
    importance: float = 5.0,
    confidence: float = 1.0,
    description: Optional[str] = None,
    reasoning: Optional[str] = None,
    source: Optional[str] = None,
    tags: Optional[List[str]] = None,
    ttl: Optional[int] = None,
    context_data: Optional[Dict[str, Any]] = None,
    generate_embedding: bool = True,
    suggest_links: bool = True,
    link_suggestion_threshold: float = SIMILARITY_THRESHOLD,
    max_suggested_links: int = 3,
    action_id: Optional[str] = None,
    thought_id: Optional[str] = None,
    artifact_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}

This function:
\begin{enumerate}[label=\arabic*.]
    \item Validates input parameters (checking enum values, numeric ranges)
    \item Generates a UUID for the memory
    \item Records a timestamp
    \item Establishes database connections
    \item Performs existence checks for foreign keys
    \item Inserts the memory record with all metadata
    \item Optionally generates and stores vector embeddings for semantic search
    \item Identifies and suggests semantic links to related memories
    \item Updates workflow timestamps and logs the operation
    \item Returns a structured result with memory details and suggested links
\end{enumerate}

Key parameters include:
\begin{itemize}
    \item \code{workflo\1\_id}: Required container for the memory
    \item \code{content}: The actual memory content text
    \item \code{memor\1\_type}: Classification (e.g., "observation", "fact", "insight")
    \item \code{memor\1\_level}: Cognitive level (e.g., "episodic", "semantic")
    \item \code{importance}/\code{confidence}: Scoring for relevance calculations (1.0-10.0/0.0-1.0)
    \item \code{generat\1\_embedding}: Whether to create vector embeddings for semantic search
    \item \code{sugges\1\_links}: Whether to automatically find related memories
\end{itemize}

Memory creation automatically handles:
\begin{itemize}
    \item Tag normalization and storage
    \item TTL determination (using defaults if not specified)
    \item Importance and confidence validation
    \item Creation of bidirectional links to similar memories
\end{itemize}

\subsubsection*{Memory Retrieval and Search}

The system offers multiple retrieval mechanisms:

\paragraph{Direct Retrieval by ID}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def get_memory_by_id(
    memory_id: str,
    include_links: bool = True,
    include_context: bool = True,
    context_limit: int = 5,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Fetches specific memory by ID
    \item Updates access statistics
    \item Optionally includes outgoing and incoming links
    \item Optionally includes semantically similar memories as context
    \item Checks TTL expiration
\end{enumerate}

\paragraph{Keyword/Criteria-Based Search}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def query_memories(
    workflow_id: Optional[str] = None,
    memory_level: Optional[str] = None,
    memory_type: Optional[str] = None,
    search_text: Optional[str] = None,
    tags: Optional[List[str]] = None,
    min_importance: Optional[float] = None,
    max_importance: Optional[float] = None,
    min_confidence: Optional[float] = None,
    min_created_at_unix: Optional[int] = None,
    max_created_at_unix: Optional[int] = None,
    sort_by: str = "relevance",
    sort_order: str = "DESC",
    include_content: bool = True,
    include_links: bool = False,
    link_direction: str = "outgoing",
    limit: int = 10,
    offset: int = 0,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function provides powerful filtering capabilities:
\begin{itemize}
    \item Workflow, level, type filters
    \item Full-text search via SQLite FTS5
    \item Tag filtering with array containment
    \item Importance/confidence ranges
    \item Creation time ranges
    \item Custom sorting options (relevance, importance, created\_at, updated\_at, etc.)
    \item Pagination via limit/offset
    \item Link inclusion options
\end{itemize}

\paragraph{Semantic/Vector Search}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def search_semantic_memories(
    query: str,
    workflow_id: Optional[str] = None,
    limit: int = 5,
    threshold: float = SIMILARITY_THRESHOLD,
    memory_level: Optional[str] = None,
    memory_type: Optional[str] = None,
    include_content: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This implements vector similarity search:
\begin{enumerate}[label=\arabic*.]
    \item Generates embeddings for the query
    \item Finds memories with similar embeddings using cosine similarity
    \item Applies threshold and filters
    \item Updates access statistics for retrieved memories
\end{enumerate}

\paragraph{Hybrid Search (Keyword + Vector)}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def hybrid_search_memories(
    query: str,
    workflow_id: Optional[str] = None,
    limit: int = 10,
    offset: int = 0,
    semantic_weight: float = 0.6,
    keyword_weight: float = 0.4,
    memory_level: Optional[str] = None,
    memory_type: Optional[str] = None,
    tags: Optional[List[str]] = None,
    min_importance: Optional[float] = None,
    max_importance: Optional[float] = None,
    min_confidence: Optional[float] = None,
    min_created_at_unix: Optional[int] = None,
    max_created_at_unix: Optional[int] = None,
    include_content: bool = True,
    include_links: bool = False,
    link_direction: str = "outgoing",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This sophisticated search function:
\begin{enumerate}[label=\arabic*.]
    \item Combines semantic and keyword search results
    \item Normalizes and weights scores from both approaches
    \item Applies comprehensive filtering options
    \item Performs efficient batched database operations for large result sets
    \item Returns hybrid-scored results with detailed metadata
\end{enumerate}

\subsubsection*{Memory Updating and Maintenance}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def update_memory(
    memory_id: str,
    content: Optional[str] = None,
    importance: Optional[float] = None,
    confidence: Optional[float] = None,
    description: Optional[str] = None,
    reasoning: Optional[str] = None,
    tags: Optional[List[str]] = None,
    ttl: Optional[int] = None,
    memory_level: Optional[str] = None,
    regenerate_embedding: bool = False,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function allows updating memory attributes:
\begin{enumerate}[label=\arabic*.]
    \item Dynamically builds SQL UPDATE clauses for changed fields
    \item Optionally regenerates embeddings when content changes
    \item Maintains timestamps and history
    \item Returns detailed update information
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def delete_expired_memories(db_path: str = DEFAULT_DB_PATH) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This maintenance function:
\begin{enumerate}[label=\arabic*.]
    \item Identifies memories that have reached their TTL
    \item Removes them in efficient batches
    \item Handles cascading deletions via foreign key constraints
    \item Logs operations for each affected workflow
\end{enumerate}

\subsubsection*{Memory Linking and Relationships}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def create_memory_link(
    source_memory_id: str,
    target_memory_id: str,
    link_type: str,
    strength: float = 1.0,
    description: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function creates directional associations between memories:
\begin{enumerate}[label=\arabic*.]
    \item Prevents self-linking
    \item Validates link types against \code{LinkType} enum
    \item Ensures link strength is in valid range (0.0-1.0)
    \item Uses UPSERT pattern for idempotency
    \item Returns link details
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def get_linked_memories(
    memory_id: str,
    direction: str = "both",
    link_type: Optional[str] = None,
    limit: int = 10,
    include_memory_details: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This retrieval function:
\begin{enumerate}[label=\arabic*.]
    \item Gets outgoing and/or incoming links
    \item Optionally filters by link type
    \item Includes detailed information about linked memories
    \item Updates access statistics
    \item Returns structured link information
\end{enumerate}

\subsubsection*{Thought Chains and Reasoning}

The system implements a sophisticated thought chain mechanism for tracking reasoning:

\subsubsection*{Thought Chain Creation and Management}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def create_thought_chain(
    workflow_id: str,
    title: str,
    initial_thought: Optional[str] = None,
    initial_thought_type: str = "goal",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Creates a container for related thoughts
    \item Optionally adds an initial thought (goal, hypothesis, etc.)
    \item Ensures atomicity through transaction management
    \item Returns chain details with ID and creation timestamp
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def record_thought(
    workflow_id: str,
    content: str,
    thought_type: str = "inference",
    thought_chain_id: Optional[str] = None,
    parent_thought_id: Optional[str] = None,
    relevant_action_id: Optional[str] = None,
    relevant_artifact_id: Optional[str] = None,
    relevant_memory_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH,
    conn: Optional[aiosqlite.Connection] = None
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function records individual reasoning steps:
\begin{enumerate}[label=\arabic*.]
    \item Validates thought type against \code{ThoughtType} enum
    \item Handles complex foreign key relationships
    \item Automatically determines target thought chain if not specified
    \item Manages parent-child relationships for hierarchical reasoning
    \item Creates links to related actions, artifacts, and memories
    \item Automatically creates semantic memory entries for important thoughts
    \item Supports transaction nesting through optional connection parameter
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def get_thought_chain(
    thought_chain_id: str,
    include_thoughts: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This retrieval function:
\begin{enumerate}[label=\arabic*.]
    \item Fetches chain metadata
    \item Optionally includes all thoughts in sequence
    \item Returns formatted timestamps and structured data
\end{enumerate}

\subsubsection*{Thought Chain Visualization}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def visualize_reasoning_chain(
    thought_chain_id: str,
    output_format: str = "mermaid",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function generates visualizations:
\begin{enumerate}[label=\arabic*.]
    \item Retrieves the complete thought chain
    \item For Mermaid format:
    \begin{itemize}
        \item Generates a directed graph representation
        \item Creates node definitions with appropriate shapes based on thought types
        \item Handles parent-child relationships with connections
        \item Adds external links to related entities
        \item Implements CSS styling for different thought types
    \end{itemize}
    \item For JSON format:
    \begin{itemize}
        \item Creates a hierarchical tree structure
        \item Maps parent-child relationships
        \item Includes all metadata
    \end{itemize}
    \item Returns the visualization content in the requested format
\end{enumerate}

The Mermaid generation happens through a helper function \1\_generat\1\_though\1\_chai\1\_mermaid()` that constructs a detailed graph with styling:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _generate_thought_chain_mermaid(thought_chain: Dict[str, Any]) -> str:
    # Implementation creates a complex Mermaid diagram with:
    # - Header node for the chain
    # - Nodes for each thought with type-specific styling
    # - Parent-child connections
    # - External links to actions, artifacts, memories
    # - Comprehensive styling definitions
\end{minted}
\end{pageablecode}

\subsubsection*{Working Memory Management}

The system implements sophisticated working memory with capacity management:

\subsubsection*{Working Memory Operations}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def get_working_memory(
    context_id: str,
    include_content: bool = True,
    include_links: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Retrieves the current active memory set for a context
    \item Updates access statistics
    \item Optionally includes memory content
    \item Optionally includes links between memories
    \item Returns a structured view of working memory
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def focus_memory(
    memory_id: str,
    context_id: str,
    add_to_working: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Sets a specific memory as the current focus of attention
    \item Optionally adds the memory to working memory if not present
    \item Ensures memory and context workflow consistency
    \item Updates cognitive state records
    \item Returns focus update confirmation
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _add_to_active_memories(conn: aiosqlite.Connection, context_id: str, memory_id: str) -> bool:
\end{minted}
\end{pageablecode}
This internal helper function implements working memory capacity management:
\begin{enumerate}[label=\arabic*.]
    \item Checks if memory is already in working memory
    \item Enforces the \code{MA\1\_WORKIN\1\_MEMOR\1\_SIZE} limit
    \item When capacity is reached, computes relevance scores for all memories
    \item Removes least relevant memory to make space
    \item Returns success/failure status
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def optimize_working_memory(
    context_id: str,
    target_size: int = MAX_WORKING_MEMORY_SIZE,
    strategy: str = "balanced",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function performs optimization:
\begin{enumerate}[label=\arabic*.]
    \item Implements multiple strategies:
    \begin{itemize}
        \item \code{balanced}: Considers all relevance factors
        \item \code{importance}: Prioritizes importance scores
        \item \code{recency}: Prioritizes recently accessed memories
        \item \code{diversity}: Ensures variety of memory types
    \end{itemize}
    \item Scores memories based on strategy
    \item Selects optimal subset to retain
    \item Updates the cognitive state
    \item Returns detailed optimization results
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def auto_update_focus(
    context_id: str,
    recent_actions_count: int = 3,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function implements automatic attention shifting:
\begin{enumerate}[label=\arabic*.]
    \item Analyzes memories currently in working memory
    \item Scores them based on relevance and recent activity
    \item Uses the \1\_calculat\1\_focu\1\_score()` helper with sophisticated heuristics
    \item Updates focus to the highest-scoring memory
    \item Returns details of the focus shift
\end{enumerate}

The focus scoring implements multiple weight factors:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _calculate_focus_score(memory: Dict, recent_action_ids: List[str], now_unix: int) -> float:
    """Calculate focus priority score based on multiple factors."""
    score = 0.0

    # Base relevance (importance, confidence, recency, usage)
    relevance = _compute_memory_relevance(...)
    score += relevance * 0.6  # Heavily weighted

    # Boost for recent action relationship
    if memory.get("action_id") in recent_action_ids:
        score += 3.0  # Significant boost

    # Type-based boosts for attention-worthy types
    if memory.get("memory_type") in ["question", "plan", "insight"]:
        score += 1.5

    # Memory level boosts
    if memory.get("memory_level") == MemoryLevel.SEMANTIC.value:
        score += 0.5
    elif memory.get("memory_level") == MemoryLevel.PROCEDURAL.value:
        score += 0.7

    return max(0.0, score)
\end{minted}
\end{pageablecode}

\subsubsection*{Cognitive State Management}

The system implements cognitive state persistence for context restoration:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def save_cognitive_state(
    workflow_id: str,
    title: str,
    working_memory_ids: List[str],
    focus_area_ids: Optional[List[str]] = None,
    context_action_ids: Optional[List[str]] = None,
    current_goal_thought_ids: Optional[List[str]] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Validates that all provided IDs exist and belong to the workflow
    \item Marks previous states as not latest
    \item Serializes state components
    \item Records a timestamped cognitive state snapshot
    \item Returns confirmation with state ID
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def load_cognitive_state(
    workflow_id: str,
    state_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Loads either a specific state or the latest state
    \item Deserializes state components
    \item Logs the operation
    \item Returns full state details
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def get_workflow_context(
    workflow_id: str,
    recent_actions_limit: int = 10,
    important_memories_limit: int = 5,
    key_thoughts_limit: int = 5,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function builds a comprehensive context summary:
\begin{enumerate}[label=\arabic*.]
    \item Fetches workflow metadata (title, goal, status)
    \item Gets latest cognitive state
    \item Retrieves recent actions
    \item Includes important memories
    \item Adds key thoughts (goals, decisions, reflections)
    \item Returns a structured context overview
\end{enumerate}

\subsubsection*{Action and Artifact Tracking}

The system tracks all agent actions and created artifacts:

\subsubsection*{Action Management}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def record_action_start(
    workflow_id: str,
    action_type: str,
    reasoning: str,
    tool_name: Optional[str] = None,
    tool_args: Optional[Dict[str, Any]] = None,
    title: Optional[str] = None,
    parent_action_id: Optional[str] = None,
    tags: Optional[List[str]] = None,
    related_thought_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Validates action type against \code{ActionType} enum
    \item Requires reasoning explanation
    \item Validates references to workflow, parent action, and related thought
    \item Auto-generates title if not provided
    \item Creates a corresponding episodic memory entry
    \item Returns action details with ID and start time
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def record_action_completion(
    action_id: str,
    status: str = "completed",
    tool_result: Optional[Any] = None,
    summary: Optional[str] = None,
    conclusion_thought: Optional[str] = None,
    conclusion_thought_type: str = "inference",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Validates completion status (completed, failed, skipped)
    \item Records tool execution result
    \item Updates the action record
    \item Optionally adds a concluding thought
    \item Updates the linked episodic memory with outcome
    \item Returns completion confirmation
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def get_action_details(
    action_id: Optional[str] = None,
    action_ids: Optional[List[str]] = None,
    include_dependencies: bool = False,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Retrieves details for one or more actions
    \item Deserializes tool args and results
    \item Includes associated tags
    \item Optionally includes dependency relationships
    \item Returns comprehensive action information
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def get_recent_actions(
    workflow_id: str,
    limit: int = 5,
    action_type: Optional[str] = None,
    status: Optional[str] = None,
    include_tool_results: bool = True,
    include_reasoning: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Gets the most recent actions for a workflow
    \item Applies type and status filters
    \item Controls inclusion of potentially large fields (tool results, reasoning)
    \item Returns a time-ordered action list
\end{enumerate}

\subsubsection*{Action Dependencies}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def add_action_dependency(
    source_action_id: str,
    target_action_id: str,
    dependency_type: str = "requires",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Creates an explicit dependency relationship between actions
    \item Ensures actions belong to the same workflow
    \item Handles duplicate dependency declarations
    \item Returns dependency details
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def get_action_dependencies(
    action_id: str,
    direction: str = "downstream",
    dependency_type: Optional[str] = None,
    include_details: bool = False,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Retrieves actions that depend on this one (downstream) or
    \item Retrieves actions this one depends on (upstream)
    \item Optionally filters by dependency type
    \item Optionally includes full action details
    \item Returns structured dependency information
\end{enumerate}

\subsubsection*{Artifact Management}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def record_artifact(
    workflow_id: str,
    name: str,
    artifact_type: str,
    action_id: Optional[str] = None,
    description: Optional[str] = None,
    path: Optional[str] = None,
    content: Optional[str] = None,
    metadata: Optional[Dict[str, Any]] = None,
    is_output: bool = False,
    tags: Optional[List[str]] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Validates artifact type against \code{ArtifactType} enum
    \item Handles content truncation for large text artifacts
    \item Creates a corresponding episodic memory entry
    \item Records relationships to creating action
    \item Applies tags and metadata
    \item Returns artifact details with ID
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def get_artifacts(
    workflow_id: str,
    artifact_type: Optional[str] = None,
    tag: Optional[str] = None,
    is_output: Optional[bool] = None,
    include_content: bool = False,
    limit: int = 10,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Lists artifacts for a workflow with filtering
    \item Controls inclusion of potentially large content
    \item Deserializes metadata
    \item Returns artifact list with details
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def get_artifact_by_id(
    artifact_id: str,
    include_content: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Retrieves a specific artifact by ID
    \item Updates access stats for related memory
    \item Returns complete artifact details
\end{enumerate}

\subsubsection*{Meta-Cognitive Capabilities}

The system implements sophisticated meta-cognitive functions:

\subsubsection*{Memory Consolidation}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def consolidate_memories(
    workflow_id: Optional[str] = None,
    target_memories: Optional[List[str]] = None,
    consolidation_type: str = "summary",
    query_filter: Optional[Dict[str, Any]] = None,
    max_source_memories: int = 20,
    prompt_override: Optional[str] = None,
    provider: str = LLMGatewayProvider.OPENAI.value,
    model: Optional[str] = None,
    store_result: bool = True,
    store_as_level: str = MemoryLevel.SEMANTIC.value,
    store_as_type: Optional[str] = None,
    max_tokens: int = 1000,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function implements memory consolidation:
\begin{enumerate}[label=\arabic*.]
    \item Allows selecting source memories:
    \begin{itemize}
        \item Explicit memory ID list or
        \item Query-based filtering or
        \item Recent important memories from workflow
    \end{itemize}
    \item Supports multiple consolidation types:
    \begin{itemize}
        \item \code{summary}: Comprehensive integration of information
        \item \code{insight}: Pattern recognition and implications
        \item \code{procedural}: Generalized steps or methods
        \item \code{question}: Key information gaps or uncertainties
    \end{itemize}
    \item Generates LLM prompts with detailed instructions
    \item Makes external LLM API calls to process memories
    \item Automatically stores the result as a new memory
    \item Creates bidirectional links to source memories
    \item Returns consolidated content and details
\end{enumerate}

The consolidation prompt generation is handled by \1\_generat\1\_consolidatio\1\_prompt()`:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _generate_consolidation_prompt(memories: List[Dict], consolidation_type: str) -> str:
    # Formats memory details with truncation
    # Adds type-specific instruction templates:
    # - summary: comprehensive integration
    # - insight: pattern identification
    # - procedural: generalized methods
    # - question: information gaps
\end{minted}
\end{pageablecode}

\subsubsection*{Reflection Generation}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def generate_reflection(
    workflow_id: str,
    reflection_type: str = "summary",
    recent_ops_limit: int = 30,
    provider: str = LLMGatewayProvider.OPENAI.value,
    model: Optional[str] = None,
    max_tokens: int = 1000,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This meta-cognitive function:
\begin{enumerate}[label=\arabic*.]
    \item Analyzes recent memory operations (from the operation log)
    \item Supports multiple reflection types:
    \begin{itemize}
        \item \code{summary}: Overview of recent activity
        \item \code{progress}: Analysis of goal advancement
        \item \code{gaps}: Knowledge and understanding deficits
        \item \code{strengths}: Effective patterns and insights
        \item \code{plan}: Strategic next steps
    \end{itemize}
    \item Generates sophisticated prompts using \1\_generat\1\_reflectio\1\_prompt()`
    \item Makes external LLM calls to perform analysis
    \item Stores the reflection in the reflection table
    \item Returns reflection content and metadata
\end{enumerate}

\subsubsection*{Memory Promotion and Evolution}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def promote_memory_level(
    memory_id: str,
    target_level: Optional[str] = None,
    min_access_count_episodic: int = 5,
    min_confidence_episodic: float = 0.8,
    min_access_count_semantic: int = 10,
    min_confidence_semantic: float = 0.9,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function implements memory evolution:
\begin{enumerate}[label=\arabic*.]
    \item Checks if a memory meets criteria for promotion to a higher level
    \item Implements promotion paths:
    \begin{itemize}
        \item Episodic ‚Üí Semantic (experiences to knowledge)
        \item Semantic ‚Üí Procedural (knowledge to skills, with type constraints)
    \end{itemize}
    \item Applies configurable criteria based on:
    \begin{itemize}
        \item Access frequency (demonstrates importance)
        \item Confidence level (demonstrates reliability)
        \item Memory type (suitability for procedural level)
    \end{itemize}
    \item Updates the memory level if criteria are met
    \item Returns promotion status with reason
\end{enumerate}

\subsubsection*{Text Summarization}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def summarize_text(
    text_to_summarize: str,
    target_tokens: int = 500,
    prompt_template: Optional[str] = None,
    provider: str = "openai",
    model: Optional[str] = None,
    workflow_id: Optional[str] = None,
    record_summary: bool = False,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This utility function:
\begin{enumerate}[label=\arabic*.]
    \item Summarizes text content using LLM
    \item Uses configurable prompt templates
    \item Controls summary length via token targeting
    \item Optionally stores summary as memory
    \item Returns summary text and metadata
\end{enumerate}

\subsubsection*{Context Summarization}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def summarize_context_block(
    text_to_summarize: str,
    target_tokens: int = 500,
    context_type: str = "actions",
    workflow_id: Optional[str] = None,
    provider: str = LLMGatewayProvider.ANTHROPIC.value,
    model: Optional[str] = "claude-3-5-haiku-20241022",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This specialized function:
\begin{enumerate}[label=\arabic*.]
    \item Summarizes specific types of context (actions, memories, thoughts)
    \item Uses custom prompts optimized for each context type
    \item Designed for agent context window management
    \item Returns focused summaries with compression ratio
\end{enumerate}

\subsubsection*{Reporting and Visualization}

The system implements sophisticated reporting capabilities:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def generate_workflow_report(
    workflow_id: str,
    report_format: str = "markdown",
    include_details: bool = True,
    include_thoughts: bool = True,
    include_artifacts: bool = True,
    style: Optional[str] = "professional",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function creates comprehensive reports:
\begin{enumerate}[label=\arabic*.]
    \item Fetches complete workflow details
    \item Supports multiple formats:
    \begin{itemize}
        \item \code{markdown}: Text-based structured report
        \item \code{html}: Web-viewable report with CSS
        \item \code{json}: Machine-readable structured data
        \item \code{mermaid}: Diagrammatic representation
    \end{itemize}
    \item Implements multiple styling options:
    \begin{itemize}
        \item \code{professional}: Formal business report style
        \item \code{concise}: Brief summary focused on key points
        \item \code{narrative}: Story-like descriptive format
        \item \code{technical}: Data-oriented technical format
    \end{itemize}
    \item Uses helper functions for specific formats:
    \begin{itemize}
        \item \1\_generat\1\_professiona\1\_report()`
        \item \1\_generat\1\_concis\1\_report()`
        \item \1\_generat\1\_narrativ\1\_report()`
        \item \1\_generat\1\_technica\1\_report()`
        \item \1\_generat\1\_mermai\1\_diagram()`
    \end{itemize}
    \item Returns report content with metadata
\end{enumerate}

Memory network visualization is implemented through:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def visualize_memory_network(
    workflow_id: Optional[str] = None,
    center_memory_id: Optional[str] = None,
    depth: int = 1,
    max_nodes: int = 30,
    memory_level: Optional[str] = None,
    memory_type: Optional[str] = None,
    output_format: str = "mermaid",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Creates a visual representation of memory relationships
    \item Supports workflow-wide view or centered on specific memory
    \item Uses breadth-first search to explore links to depth limit
    \item Applies memory type and level filters
    \item Generates Mermaid diagram with:
    \begin{itemize}
        \item Nodes styled by memory level
        \item Links showing relationship types
        \item Center node highlighting
    \end{itemize}
    \item Returns complete diagram code
\end{enumerate}

\subsection*{Detailed Key Tool Functions (Additional Core Functionality)}

Below I'll cover several more important tool functions in detail that implement key functionality:

\subsubsection*{LLM Integration}

The system integrates with external LLM providers through the \code{ll\1\_gateway} module:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
from llm_gateway.constants import Provider as LLMGatewayProvider
from llm_gateway.core.providers.base import get_provider
\end{minted}
\end{pageablecode}
This enables:
\begin{enumerate}[label=\arabic*.]
    \item Dynamic provider selection (OpenAI, Anthropic, etc.)
    \item Model specification
    \item Standardized prompting
    \item Response handling
\end{enumerate}

Example LLM integration in consolidation:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
provider_instance = await get_provider(provider)
llm_result = await provider_instance.generate_completion(
    prompt=prompt, model=model_to_use, max_tokens=max_tokens, temperature=0.7
)
reflection_content = llm_result.text.strip()
\end{minted}
\end{pageablecode}

\subsubsection*{System Statistics and Metrics}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def compute_memory_statistics(
    workflow_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Computes comprehensive system statistics
    \item Supports global or workflow-specific scope
    \item Collects metrics on:
    \begin{itemize}
        \item Total memory counts
        \item Distribution by level and type
        \item Confidence and importance averages
        \item Temporal metrics (newest/oldest)
        \item Link statistics by type
        \item Tag frequencies
        \item Workflow statuses
    \end{itemize}
    \item Returns structured statistical data
\end{enumerate}

\subsubsection*{Workflow Listing and Management}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def list_workflows(
    status: Optional[str] = None,
    tag: Optional[str] = None,
    after_date: Optional[str] = None,
    before_date: Optional[str] = None,
    limit: int = 10,
    offset: int = 0,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Lists workflows with filtering options
    \item Supports status, tag, and date range filters
    \item Includes pagination
    \item Returns workflow list with counts
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def create_workflow(
    title: str,
    description: Optional[str] = None,
    goal: Optional[str] = None,
    tags: Optional[List[str]] = None,
    metadata: Optional[Dict[str, Any]] = None,
    parent_workflow_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Creates a new workflow container
    \item Creates default thought chain
    \item Adds initial goal thought if provided
    \item Supports workflow hierarchies via parent reference
    \item Returns workflow details with IDs
\end{enumerate}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def update_workflow_status(
    workflow_id: str,
    status: str,
    completion_message: Optional[str] = None,
    update_tags: Optional[List[str]] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function:
\begin{enumerate}[label=\arabic*.]
    \item Updates workflow status (active, paused, completed, failed, abandoned)
    \item Adds completion thought for terminal statuses
    \item Updates tags
    \item Returns status update confirmation
\end{enumerate}

\subsection*{Database Schema Details and Implementation}

The system's database schema represents a sophisticated cognitive architecture designed for tracking agent workflows, actions, thoughts, and memories. Let's examine its detailed structure:

\subsubsection*{Schema Creation and Initialization}

The schema is defined in the \code{SCHEM\1\_SQL} constant, which contains all DDL statements. The system uses a transactional approach to schema initialization:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# Initialize schema if needed
cursor = await conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='workflows'")
table_exists = await cursor.fetchone()
await cursor.close()
if not table_exists:
    logger.info("Database schema not found. Initializing...", emoji_key="gear")
    await conn.execute("PRAGMA foreign_keys = ON;")
    await conn.executescript(SCHEMA_SQL)
    logger.success("Database schema initialized successfully.", emoji_key="white_check_mark")
\end{minted}
\end{pageablecode}
The schema includes several critical components:

\subsubsection*{Base Tables}

\begin{enumerate}[label=\arabic*.]
    \item \code{workflows}: The top-level container
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS workflows (
       workflow_id TEXT PRIMARY KEY,
       title TEXT NOT NULL,
       description TEXT,
       goal TEXT,
       status TEXT NOT NULL,
       created_at INTEGER NOT NULL,
       updated_at INTEGER NOT NULL,
       completed_at INTEGER,
       parent_workflow_id TEXT,
       metadata TEXT,
       last_active INTEGER
   );
    \end{minted}
    \end{pageablecode}

    \item \code{actions}: Records of agent activities
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS actions (
       action_id TEXT PRIMARY KEY,
       workflow_id TEXT NOT NULL,
       parent_action_id TEXT,
       action_type TEXT NOT NULL,
       title TEXT,
       reasoning TEXT,
       tool_name TEXT,
       tool_args TEXT,
       tool_result TEXT,
       status TEXT NOT NULL,
       started_at INTEGER NOT NULL,
       completed_at INTEGER,
       sequence_number INTEGER,
       FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
       FOREIGN KEY (parent_action_id) REFERENCES actions(action_id) ON DELETE SET NULL
   );
    \end{minted}
    \end{pageablecode}

    \item \code{artifacts}: Outputs and files created during workflows
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS artifacts (
       artifact_id TEXT PRIMARY KEY,
       workflow_id TEXT NOT NULL,
       action_id TEXT,
       artifact_type TEXT NOT NULL,
       name TEXT NOT NULL,
       description TEXT,
       path TEXT,
       content TEXT,
       metadata TEXT,
       created_at INTEGER NOT NULL,
       is_output BOOLEAN DEFAULT FALSE,
       FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
       FOREIGN KEY (action_id) REFERENCES actions(action_id) ON DELETE SET NULL
   );
    \end{minted}
    \end{pageablecode}

    \item \code{memories}: Core memory storage
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS memories (
       memory_id TEXT PRIMARY KEY,
       workflow_id TEXT NOT NULL,
       content TEXT NOT NULL,
       memory_level TEXT NOT NULL,
       memory_type TEXT NOT NULL,
       importance REAL DEFAULT 5.0,
       confidence REAL DEFAULT 1.0,
       description TEXT,
       reasoning TEXT,
       source TEXT,
       context TEXT,
       tags TEXT,
       created_at INTEGER NOT NULL,
       updated_at INTEGER NOT NULL,
       last_accessed INTEGER,
       access_count INTEGER DEFAULT 0,
       ttl INTEGER DEFAULT 0,
       embedding_id TEXT,
       action_id TEXT,
       thought_id TEXT,
       artifact_id TEXT,
       FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
       FOREIGN KEY (embedding_id) REFERENCES embeddings(id) ON DELETE SET NULL,
       FOREIGN KEY (action_id) REFERENCES actions(action_id) ON DELETE SET NULL,
       FOREIGN KEY (artifact_id) REFERENCES artifacts(artifact_id) ON DELETE SET NULL
   );
    \end{minted}
    \end{pageablecode}

    \item \code{though\1\_chains} and \code{thoughts}: Reasoning structure
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS thought_chains (
       thought_chain_id TEXT PRIMARY KEY,
       workflow_id TEXT NOT NULL,
       action_id TEXT,
       title TEXT NOT NULL,
       created_at INTEGER NOT NULL,
       FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
       FOREIGN KEY (action_id) REFERENCES actions(action_id) ON DELETE SET NULL
   );

   CREATE TABLE IF NOT EXISTS thoughts (
       thought_id TEXT PRIMARY KEY,
       thought_chain_id TEXT NOT NULL,
       parent_thought_id TEXT,
       thought_type TEXT NOT NULL,
       content TEXT NOT NULL,
       sequence_number INTEGER NOT NULL,
       created_at INTEGER NOT NULL,
       relevant_action_id TEXT,
       relevant_artifact_id TEXT,
       relevant_memory_id TEXT,
       FOREIGN KEY (thought_chain_id) REFERENCES thought_chains(thought_chain_id) ON DELETE CASCADE,
       FOREIGN KEY (parent_thought_id) REFERENCES thoughts(thought_id) ON DELETE SET NULL,
       FOREIGN KEY (relevant_action_id) REFERENCES actions(action_id) ON DELETE SET NULL,
       FOREIGN KEY (relevant_artifact_id) REFERENCES artifacts(artifact_id) ON DELETE SET NULL
   );
    \end{minted}
    \end{pageablecode}
\end{enumerate}

\subsubsection*{Advanced Features}

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Circular Foreign Key Constraints}: The schema implements circular references between memories and thoughts using deferred constraints:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   -- Deferrable Circular Foreign Key Constraints for thoughts <-> memories
   BEGIN IMMEDIATE TRANSACTION;
   PRAGMA defer_foreign_keys = ON;

   ALTER TABLE thoughts ADD CONSTRAINT fk_thoughts_memory
       FOREIGN KEY (relevant_memory_id) REFERENCES memories(memory_id)
       ON DELETE SET NULL DEFERRABLE INITIALLY DEFERRED;

   ALTER TABLE memories ADD CONSTRAINT fk_memories_thought
       FOREIGN KEY (thought_id) REFERENCES thoughts(thought_id)
       ON DELETE SET NULL DEFERRABLE INITIALLY DEFERRED;

   COMMIT;
    \end{minted}
    \end{pageablecode}
    This pattern allows creating memories that reference thoughts and thoughts that reference memories, resolving the chicken-and-egg problem typically encountered with circular foreign keys.

    \item \textbf{Full-Text Search}: The system implements sophisticated text search through SQLite's FTS5 virtual table:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE VIRTUAL TABLE IF NOT EXISTS memory_fts USING fts5(
       content, description, reasoning, tags,
       workflow_id UNINDEXED,
       memory_id UNINDEXED,
       content='memories',
       content_rowid='rowid',
       tokenize='porter unicode61'
   );
    \end{minted}
    \end{pageablecode}
    With synchronized triggers:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TRIGGER IF NOT EXISTS memories_after_insert AFTER INSERT ON memories BEGIN
       INSERT INTO memory_fts(rowid, content, description, reasoning, tags, workflow_id, memory_id)
       VALUES (new.rowid, new.content, new.description, new.reasoning, new.tags, new.workflow_id, new.memory_id);
   END;
    \end{minted}
    \end{pageablecode}

    \item \textbf{Vector Embeddings}: The schema includes an `embeddings` table for storing vector representations:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS embeddings (
       id TEXT PRIMARY KEY,
       memory_id TEXT UNIQUE,
       model TEXT NOT NULL,
       embedding BLOB NOT NULL,
       dimension INTEGER NOT NULL,
       created_at INTEGER NOT NULL
   );
    \end{minted}
    \end{pageablecode}
    With a back-reference from embeddings to memories:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   ALTER TABLE embeddings ADD CONSTRAINT fk_embeddings_memory FOREIGN KEY (memory_id) REFERENCES memories(memory_id) ON DELETE CASCADE;
    \end{minted}
    \end{pageablecode}

    \item \textbf{Memory Links}: Associative connections between memories:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS memory_links (
       link_id TEXT PRIMARY KEY,
       source_memory_id TEXT NOT NULL,
       target_memory_id TEXT NOT NULL,
       link_type TEXT NOT NULL,
       strength REAL DEFAULT 1.0,
       description TEXT,
       created_at INTEGER NOT NULL,
       FOREIGN KEY (source_memory_id) REFERENCES memories(memory_id) ON DELETE CASCADE,
       FOREIGN KEY (target_memory_id) REFERENCES memories(memory_id) ON DELETE CASCADE,
       UNIQUE(source_memory_id, target_memory_id, link_type)
   );
    \end{minted}
    \end{pageablecode}

    \item \textbf{Cognitive States}: Persistence of cognitive context:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS cognitive_states (
       state_id TEXT PRIMARY KEY,
       workflow_id TEXT NOT NULL,
       title TEXT NOT NULL,
       working_memory TEXT,
       focus_areas TEXT,
       context_actions TEXT,
       current_goals TEXT,
       created_at INTEGER NOT NULL,
       is_latest BOOLEAN NOT NULL,
       FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE
   );
    \end{minted}
    \end{pageablecode}

    \item \textbf{Meta-Cognitive Components}:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS reflections (
       reflection_id TEXT PRIMARY KEY,
       workflow_id TEXT NOT NULL,
       title TEXT NOT NULL,
       content TEXT NOT NULL,
       reflection_type TEXT NOT NULL,
       created_at INTEGER NOT NULL,
       referenced_memories TEXT,
       FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE
   );

   CREATE TABLE IF NOT EXISTS memory_operations (
       operation_log_id TEXT PRIMARY KEY,
       workflow_id TEXT NOT NULL,
       memory_id TEXT,
       action_id TEXT,
       operation TEXT NOT NULL,
       operation_data TEXT,
       timestamp INTEGER NOT NULL,
       FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
       FOREIGN KEY (memory_id) REFERENCES memories(memory_id) ON DELETE SET NULL,
       FOREIGN KEY (action_id) REFERENCES actions(action_id) ON DELETE SET NULL
   );
    \end{minted}
    \end{pageablecode}

    \item \textbf{Tagging System}: Comprehensive tagging with junction tables:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS tags (
       tag_id INTEGER PRIMARY KEY AUTOINCREMENT,
       name TEXT NOT NULL UNIQUE,
       description TEXT,
       category TEXT,
       created_at INTEGER NOT NULL
   );

   CREATE TABLE IF NOT EXISTS workflow_tags (
       workflow_id TEXT NOT NULL,
       tag_id INTEGER NOT NULL,
       PRIMARY KEY (workflow_id, tag_id),
       FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
       FOREIGN KEY (tag_id) REFERENCES tags(tag_id) ON DELETE CASCADE
   );
    \end{minted}
    \end{pageablecode}
    With similar structures for \code{actio\1\_tags} and \code{artifac\1\_tags}.

    \item \textbf{Dependencies}: Structured action dependencies:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
   CREATE TABLE IF NOT EXISTS dependencies (
       dependency_id INTEGER PRIMARY KEY AUTOINCREMENT,
       source_action_id TEXT NOT NULL,
       target_action_id TEXT NOT NULL,
       dependency_type TEXT NOT NULL,
       created_at INTEGER NOT NULL,
       FOREIGN KEY (source_action_id) REFERENCES actions (action_id) ON DELETE CASCADE,
       FOREIGN KEY (target_action_id) REFERENCES actions (action_id) ON DELETE CASCADE,
       UNIQUE(source_action_id, target_action_id, dependency_type)
   );
    \end{minted}
    \end{pageablecode}
\end{enumerate}

\subsubsection*{Schema Optimization}

The schema includes comprehensive indexing for performance optimization:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
-- Workflow indices
CREATE INDEX IF NOT EXISTS idx_workflows_status ON workflows(status);
CREATE INDEX IF NOT EXISTS idx_workflows_parent ON workflows(parent_workflow_id);
CREATE INDEX IF NOT EXISTS idx_workflows_last_active ON workflows(last_active DESC);
-- Action indices
CREATE INDEX IF NOT EXISTS idx_actions_workflow_id ON actions(workflow_id);
CREATE INDEX IF NOT EXISTS idx_actions_parent ON actions(parent_action_id);
CREATE INDEX IF NOT EXISTS idx_actions_sequence ON actions(workflow_id, sequence_number);
CREATE INDEX IF NOT EXISTS idx_actions_type ON actions(action_type);
\end{minted}
\end{pageablecode}
With over 25 carefully designed indices covering most query patterns. Foreign keys are indexed as well as search fields, and compound indices are used for common query patterns.

\subsection*{Custom SQLite Functions}

The system extends SQLite with custom functions for advanced querying capabilities:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
await conn.create_function("json_contains", 2, _json_contains, deterministic=True)
await conn.create_function("json_contains_any", 2, _json_contains_any, deterministic=True)
await conn.create_function("json_contains_all", 2, _json_contains_all, deterministic=True)
await conn.create_function("compute_memory_relevance", 5, _compute_memory_relevance, deterministic=True)
\end{minted}
\end{pageablecode}
These functions enable:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{JSON Array Operations}:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   def _json_contains(json_text, search_value):
       """Check if a JSON array contains a specific value."""
       if not json_text:
           return False
       try:
           return search_value in json.loads(json_text) if isinstance(json.loads(json_text), list) else False
       except Exception:
           return False
    \end{minted}
    \end{pageablecode}
    With similar functions for checking if any or all values from a list are present in a JSON array.

    \item \textbf{Memory Relevance Calculation}:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   def _compute_memory_relevance(importance, confidence, created_at, access_count, last_accessed):
       """Computes a relevance score based on multiple factors. Uses Unix Timestamps."""
       now = time.time()
       age_hours = (now - created_at) / 3600 if created_at else 0
       recency_factor = 1.0 / (1.0 + (now - (last_accessed or created_at)) / 86400)
       decayed_importance = max(0, importance * (1.0 - MEMORY_DECAY_RATE * age_hours))
       usage_boost = min(1.0 + (access_count / 10.0), 2.0) if access_count else 1.0
       relevance = (decayed_importance * usage_boost * confidence * recency_factor)
       return min(max(relevance, 0.0), 10.0)
    \end{minted}
    \end{pageablecode}
    This function is central to memory prioritization, implementing:
    \begin{itemize}
        \item Time-based decay of importance
        \item Recency boost for recently accessed memories
        \item Usage frequency boost
        \item Confidence weighting
        \item Bounded output range (0.0-10.0)
    \end{itemize}
\end{enumerate}

\subsection*{Error Handling and Decorators}

The system implements consistent error handling through decorators:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
@with_tool_metrics
@with_error_handling
async def function_name(...):
    # Implementation
\end{minted}
\end{pageablecode}
These decorators provide:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Error Standardization}:
    \begin{itemize}
        \item \code{ToolInputError}: For invalid parameters
        \item \code{ToolError}: For operational/system failures
        \item Robust exception conversion and logging
    \end{itemize}
    \item \textbf{Performance Metrics}:
    \begin{itemize}
        \item Timing for each operation
        \item Success/failure tracking
        \item Consistent result formatting
    \end{itemize}
    \item \textbf{Logging Integration}:
    \begin{itemize}
        \item Standardized log format with emojis
        \item Differentiated log levels (info, warning, error)
        \item Performance timing included
    \end{itemize}
\end{enumerate}

The pattern ensures all tool functions have consistent behavior:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# Example decorator patterns:
def with_error_handling(func):
    """Wrapper for standardized error handling in tool functions."""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        try:
            return await func(*args, **kwargs)
        except ToolInputError:
            # Re-raise with input validation errors
            raise
        except Exception as e:
            # Convert other exceptions to ToolError
            logger.error(f"Error in {func.__name__}: {e}", exc_info=True)
            raise ToolError(f"Operation failed: {str(e)}") from e
    return wrapper

def with_tool_metrics(func):
    """Wrapper for tracking metrics and standardizing tool function results."""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        processing_time = time.time() - start_time

        # Add standardized fields if result is a dict
        if isinstance(result, dict):
            result["success"] = True
            result["processing_time"] = processing_time

        logger.info(f"{func.__name__} completed in {processing_time:.3f}s")
        return result
    return wrapper
\end{minted}
\end{pageablecode}

\subsection*{Transaction Management}

The system implements sophisticated transaction management through a context manager:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
@contextlib.asynccontextmanager
async def transaction(self) -> AsyncIterator[aiosqlite.Connection]:
    """Provides an atomic transaction block using the singleton connection."""
    conn = await self.__aenter__()  # Acquire the connection instance
    try:
        await conn.execute("BEGIN DEFERRED TRANSACTION")
        logger.debug("DB Transaction Started.")
        yield conn  # Provide the connection to the 'async with' block
    except Exception as e:
        logger.error(f"Exception during transaction, rolling back: {e}", exc_info=True)
        await conn.rollback()
        logger.warning("DB Transaction Rolled Back.", emoji_key="rewind")
        raise  # Re-raise the exception after rollback
    else:
        await conn.commit()
        logger.debug("DB Transaction Committed.")
\end{minted}
\end{pageablecode}
This allows operations to be grouped atomically:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# Usage example
db_manager = DBConnection(db_path)
async with db_manager.transaction() as conn:
    # Multiple operations that should succeed or fail together
    await conn.execute("INSERT INTO ...")
    await conn.execute("UPDATE ...")
    # Auto-commits on success, rolls back on exception
\end{minted}
\end{pageablecode}
The transaction manager is used extensively throughout the codebase to ensure data integrity, particularly for:
\begin{itemize}
    \item Creating workflow and initial thought chain
    \item Recording actions and linked memories
    \item Creating thoughts with associated memory entries
    \item Complex dependency operations
\end{itemize}

\subsection*{Vector Embedding and Semantic Search Implementation}

\subsubsection*{Embedding Storage}

The system implements vector embedding storage:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _store_embedding(conn: aiosqlite.Connection, memory_id: str, text: str) -> Optional[str]:
    """Generates and stores an embedding for a memory using the EmbeddingService."""
    try:
        embedding_service = get_embedding_service()  # Get singleton instance
        if not embedding_service.client:
             logger.warning("EmbeddingService client not available. Cannot generate embedding.")
             return None

        # Generate embedding using the service (handles caching internally)
        embedding_list = await embedding_service.create_embeddings(texts=[text])
        if not embedding_list or not embedding_list[0]:
             logger.warning(f"Failed to generate embedding for memory {memory_id}")
             return None
        embedding_array = np.array(embedding_list[0], dtype=np.float32)
        if embedding_array.size == 0:
             logger.warning(f"Generated embedding is empty for memory {memory_id}")
             return None

        # Get the embedding dimension
        embedding_dimension = embedding_array.shape[0]

        # Generate a unique ID for this embedding entry
        embedding_db_id = MemoryUtils.generate_id()
        embedding_bytes = embedding_array.tobytes()
        model_used = embedding_service.default_model

        # Store embedding in DB
        await conn.execute(
            """
            INSERT INTO embeddings (id, memory_id, model, embedding, dimension, created_at)
            VALUES (?, ?, ?, ?, ?, ?)
            ON CONFLICT(memory_id) DO UPDATE SET
                id = excluded.id,
                model = excluded.model,
                embedding = excluded.embedding,
                dimension = excluded.dimension,
                created_at = excluded.created_at
            """,
            (embedding_db_id, memory_id, model_used, embedding_bytes, embedding_dimension, int(time.time()))
        )

        # Update memory record to link to embedding
        await conn.execute(
            "UPDATE memories SET embedding_id = ? WHERE memory_id = ?",
            (embedding_db_id, memory_id)
        )

        return embedding_db_id
    except Exception as e:
        logger.error(f"Failed to store embedding for memory {memory_id}: {e}", exc_info=True)
        return None
\end{minted}
\end{pageablecode}
Key aspects:
\begin{enumerate}[label=\arabic*.]
    \item Integration with external embedding service
    \item Numpy array serialization to binary BLOB
    \item Dimension tracking for compatibility
    \item UPSERT pattern for idempotent updates
    \item Error handling for service failures
\end{enumerate}

\subsubsection*{Semantic Search Implementation}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _find_similar_memories(
    conn: aiosqlite.Connection,
    query_text: str,
    workflow_id: Optional[str] = None,
    limit: int = 5,
    threshold: float = SIMILARITY_THRESHOLD,
    memory_level: Optional[str] = None,
    memory_type: Optional[str] = None
) -> List[Tuple[str, float]]:
    """Finds memories with similar semantic meaning using embeddings."""
    try:
        embedding_service = get_embedding_service()
        if not embedding_service.client:
            logger.warning("EmbeddingService client not available.")
            return []

        # 1. Generate query embedding
        query_embedding_list = await embedding_service.create_embeddings(texts=[query_text])
        if not query_embedding_list or not query_embedding_list[0]:
            logger.warning(f"Failed to generate query embedding")
            return []
        query_embedding = np.array(query_embedding_list[0], dtype=np.float32)
        query_dimension = query_embedding.shape[0]
        query_embedding_2d = query_embedding.reshape(1, -1)

        # 2. Build query for candidate embeddings with filters
        sql = """
        SELECT m.memory_id, e.embedding
        FROM memories m
        JOIN embeddings e ON m.embedding_id = e.id
        WHERE e.dimension = ?
        """
        params: List[Any] = [query_dimension]

        # Add filters
        if workflow_id:
            sql += " AND m.workflow_id = ?"
            params.append(workflow_id)
        if memory_level:
            sql += " AND m.memory_level = ?"
            params.append(memory_level.lower())
        if memory_type:
            sql += " AND m.memory_type = ?"
            params.append(memory_type.lower())

        # Add TTL check
        now_unix = int(time.time())
        sql += " AND (m.ttl = 0 OR m.created_at + m.ttl > ?)"
        params.append(now_unix)

        # Optimize with pre-filtering and candidate limit
        candidate_limit = max(limit * 5, 50)
        sql += " ORDER BY m.last_accessed DESC NULLS LAST LIMIT ?"
        params.append(candidate_limit)

        # 3. Fetch candidate embeddings with matching dimension
        candidates: List[Tuple[str, bytes]] = []
        async with conn.execute(sql, params) as cursor:
            candidates = await cursor.fetchall()

        if not candidates:
            logger.debug(f"No candidate memories found matching filters")
            return []

        # 4. Calculate similarities using scikit-learn
        similarities: List[Tuple[str, float]] = []
        for memory_id, embedding_bytes in candidates:
            try:
                # Deserialize embedding from bytes
                memory_embedding = np.frombuffer(embedding_bytes, dtype=np.float32)
                if memory_embedding.size == 0:
                    continue

                memory_embedding_2d = memory_embedding.reshape(1, -1)

                # Safety check for dimension mismatch
                if query_embedding_2d.shape[1] != memory_embedding_2d.shape[1]:
                    continue

                # Calculate cosine similarity
                similarity = sk_cosine_similarity(query_embedding_2d, memory_embedding_2d)[0][0]

                # 5. Filter by threshold
                if similarity >= threshold:
                    similarities.append((memory_id, float(similarity)))
            except Exception as e:
                logger.warning(f"Error processing embedding for memory {memory_id}: {e}")
                continue

        # 6. Sort by similarity (descending) and limit
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:limit]

    except Exception as e:
        logger.error(f"Failed to find similar memories: {e}", exc_info=True)
        return []
\end{minted}
\end{pageablecode}
Key aspects:
\begin{enumerate}[label=\arabic*.]
    \item Integration with embedding service API
    \item Efficient querying with dimension matching
    \item Candidate pre-filtering before similarity calculation
    \item Serialized binary embedding handling
    \item Scikit-learn integration for cosine similarity
    \item Threshold filtering and result ranking
    \item Comprehensive error handling for edge cases
\end{enumerate}

\subsection*{Mermaid Diagram Generation}

The system generates sophisticated visualization diagrams:

\subsubsection*{Workflow Diagram Generation}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _generate_mermaid_diagram(workflow: Dict[str, Any]) -> str:
    """Generates a detailed Mermaid flowchart representation of the workflow."""

    def sanitize_mermaid_id(uuid_str: Optional[str], prefix: str) -> str:
        """Creates a valid Mermaid node ID from a UUID, handling None."""
        if not uuid_str:
             return f"{prefix}_MISSING_{MemoryUtils.generate_id().replace('-', '_')}"
        sanitized = uuid_str.replace("-", "_")  # Hyphens cause issues in Mermaid
        return f"{prefix}_{sanitized}"

    diagram = ["```mermaid", "flowchart TD"]  # Top-Down flowchart

    # --- Generate Workflow Node ---
    wf_node_id = sanitize_mermaid_id(workflow.get('workflow_id'), "W")
    wf_title = _mermaid_escape(workflow.get('title', 'Workflow'))
    wf_status_class = f":::{workflow.get('status', 'active')}"
    diagram.append(f'    {wf_node_id}("{wf_title}"){wf_status_class}')

    # --- Generate Action Nodes ---
    action_nodes = {}  # Map action_id to mermaid_node_id
    parent_links = {}  # Map child_action_id to parent_action_id
    sequential_links = {}  # Map sequence_number to action_id

    for action in sorted(workflow.get("actions", []), key=lambda a: a.get("sequence_number", 0)):
        action_id = action.get("action_id")
        if not action_id:
            continue

        node_id = sanitize_mermaid_id(action_id, "A")
        action_nodes[action_id] = node_id

        # Create node label with type, title, and tool info
        action_type = action.get('action_type', 'Action').capitalize()
        action_title = _mermaid_escape(action.get('title', action_type))
        sequence_number = action.get("sequence_number", 0)
        label = f"<b>{action_type} #{sequence_number}</b><br/>{action_title}"
        if action.get('tool_name'):
            label += f"<br/><i>Tool: {_mermaid_escape(action['tool_name'])}</i>"

        # Style node based on status
        status = action.get('status', ActionStatus.PLANNED.value)
        node_style = f":::{status}"

        diagram.append(f'    {node_id}["{label}"]{node_style}')

        # Record parent relationship
        parent_action_id = action.get("parent_action_id")
        if parent_action_id:
            parent_links[action_id] = parent_action_id
        else:
            sequential_links[sequence_number] = action_id

    # --- Generate Action Links ---
    linked_actions = set()

    # Parent->Child links
    for child_id, parent_id in parent_links.items():
        if child_id in action_nodes and parent_id in action_nodes:
            child_node = action_nodes[child_id]
            parent_node = action_nodes[parent_id]
            diagram.append(f"    {parent_node} --> {child_node}")
            linked_actions.add(child_id)

    # Sequential links for actions without explicit parents
    last_sequential_node = wf_node_id
    for seq_num in sorted(sequential_links.keys()):
        action_id = sequential_links[seq_num]
        if action_id in action_nodes:
             node_id = action_nodes[action_id]
             diagram.append(f"    {last_sequential_node} --> {node_id}")
             last_sequential_node = node_id
             linked_actions.add(action_id)

    # --- Generate Artifact Nodes ---
    for artifact in workflow.get("artifacts", []):
        artifact_id = artifact.get("artifact_id")
        if not artifact_id:
            continue

        node_id = sanitize_mermaid_id(artifact_id, "F")
        artifact_name = _mermaid_escape(artifact.get('name', 'Artifact'))
        artifact_type = _mermaid_escape(artifact.get('artifact_type', 'file'))
        label = f"üìÑ<br/><b>{artifact_name}</b><br/>({artifact_type})"

        node_shape_start, node_shape_end = "[(", ")]"  # Database/capsule shape
        node_style = ":::artifact"
        if artifact.get('is_output'):
            node_style = ":::artifact_output"  # Special style for outputs

        diagram.append(f'    {node_id}{node_shape_start}"{label}"{node_shape_end}{node_style}')

        # Link from creating action
        creator_action_id = artifact.get("action_id")
        if creator_action_id and creator_action_id in action_nodes:
            creator_node = action_nodes[creator_action_id]
            diagram.append(f"    {creator_node} -- Creates --> {node_id}")
        else:
            # Link to workflow if no specific action
            diagram.append(f"    {wf_node_id} -.-> {node_id}")

    # --- Add Class Definitions for Styling ---
    diagram.append("\n    %% Stylesheets")
    diagram.append("    classDef workflow fill:#e7f0fd,stroke:#0056b3,stroke-width:2px,color:#000")
    diagram.append("    classDef completed fill:#d4edda,stroke:#155724,stroke-width:1px,color:#155724")
    diagram.append("    classDef failed fill:#f8d7da,stroke:#721c24,stroke-width:1px,color:#721c24")
    # ... many more style definitions ...

    diagram.append("```")
    return "\n".join(diagram)
\end{minted}
\end{pageablecode}
This intricate function:
\begin{enumerate}[label=\arabic*.]
    \item Sanitizes UUIDs for Mermaid compatibility
    \item Constructs a flowchart with workflow, actions, and artifacts
    \item Creates hierarchical relationships
    \item Handles parent-child and sequential relationships
    \item Implements detailed styling based on status
    \item Escapes special characters for Mermaid compatibility
\end{enumerate}

\subsubsection*{Memory Network Diagram Generation}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _generate_memory_network_mermaid(memories: List[Dict], links: List[Dict], center_memory_id: Optional[str] = None) -> str:
    """Helper function to generate Mermaid graph syntax for a memory network."""

    def sanitize_mermaid_id(uuid_str: Optional[str], prefix: str) -> str:
        """Creates a valid Mermaid node ID from a UUID, handling None."""
        if not uuid_str:
             return f"{prefix}_MISSING_{MemoryUtils.generate_id().replace('-', '_')}"
        sanitized = uuid_str.replace("-", "_")
        return f"{prefix}_{sanitized}"

    diagram = ["```mermaid", "graph TD"]  # Top-Down graph direction

    # --- Memory Node Definitions ---
    memory_id_to_node_id = {}  # Map full memory ID to sanitized Mermaid node ID
    for memory in memories:
        mem_id = memory.get("memory_id")
        if not mem_id:
            continue

        node_id = sanitize_mermaid_id(mem_id, "M")
        memory_id_to_node_id[mem_id] = node_id

        # Create node label with type, description, importance
        mem_type = memory.get("memory_type", "memory").capitalize()
        desc = _mermaid_escape(memory.get("description", mem_id))
        if len(desc) > 40:
            desc = desc[:37] + "..."
        importance = memory.get('importance', 5.0)
        label = f"<b>{mem_type}</b><br/>{desc}<br/><i>(I: {importance:.1f})</i>"

        # Choose node shape based on memory level
        level = memory.get("memory_level", MemoryLevel.EPISODIC.value)
        shape_start, shape_end = "[", "]"  # Default rectangle (Semantic)
        if level == MemoryLevel.EPISODIC.value:
            shape_start, shape_end = "(", ")"  # Round (Episodic)
        elif level == MemoryLevel.PROCEDURAL.value:
            shape_start, shape_end = "[[", "]]"  # Subroutine (Procedural)
        elif level == MemoryLevel.WORKING.value:
             shape_start, shape_end = "([", "])"  # Capsule (Working)

        # Style node based on level + highlight center
        node_style = f":::level{level}"
        if mem_id == center_memory_id:
            node_style += " :::centerNode"  # Highlight center node

        diagram.append(f'    {node_id}{shape_start}"{label}"{shape_end}{node_style}')

    # --- Memory Link Definitions ---
    for link in links:
        source_mem_id = link.get("source_memory_id")
        target_mem_id = link.get("target_memory_id")
        link_type = link.get("link_type", "related")

        # Only draw links where both ends are in the visualization
        if source_mem_id in memory_id_to_node_id and target_mem_id in memory_id_to_node_id:
            source_node = memory_id_to_node_id[source_mem_id]
            target_node = memory_id_to_node_id[target_mem_id]
            diagram.append(f"    {source_node} -- {link_type} --> {target_node}")

    # --- Add Class Definitions for Styling ---
    diagram.append("\n    %% Stylesheets")
    diagram.append("    classDef levelworking fill:#e3f2fd,stroke:#2196f3,color:#1e88e5,stroke-width:1px;")
    diagram.append("    classDef levelepisodic fill:#e8f5e9,stroke:#4caf50,color:#388e3c,stroke-width:1px;")
    # ... additional style definitions ...
    diagram.append("    classDef centerNode stroke-width:3px,stroke:#0d47a1,font-weight:bold;")

    diagram.append("```")
    return "\n".join(diagram)
\end{minted}
\end{pageablecode}
This visualization:
\begin{enumerate}[label=\arabic*.]
    \item Displays memories with level-specific shapes
    \item Shows relationship types on connection lines
    \item Provides visual cues for importance and type
    \item Highlights the center node when specified
    \item Implements sophisticated styling based on memory levels
\end{enumerate}

\subsection*{Character Escaping for Mermaid}

The system implements robust character escaping for Mermaid compatibility:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _mermaid_escape(text: str) -> str:
    """Escapes characters problematic for Mermaid node labels."""
    if not isinstance(text, str):
        text = str(text)
    # Replace quotes first, then other potentially problematic characters
    text = text.replace('"', '#quot;')
    text = text.replace('(', '#40;')
    text = text.replace(')', '#41;')
    text = text.replace('[', '#91;')
    text = text.replace(']', '#93;')
    text = text.replace('{', '#123;')
    text = text.replace('}', '#125;')
    text = text.replace(':', '#58;')
    text = text.replace(';', '#59;')
    text = text.replace('<', '#lt;')
    text = text.replace('>', '#gt;')
    # Replace newline with <br> for multiline labels
    text = text.replace('\n', '<br>')
    return text
\end{minted}
\end{pageablecode}
This function handles all special characters that could break Mermaid diagram syntax.

\subsection*{Serialization and Data Handling}

The system implements sophisticated serialization with robust error handling:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def serialize(obj: Any) -> Optional[str]:
    """Safely serialize an arbitrary Python object to a JSON string.

    Handles potential serialization errors and very large objects.
    Attempts to represent complex objects that fail direct serialization.
    If the final JSON string exceeds MAX_TEXT_LENGTH, it returns a
    JSON object indicating truncation.
    """
    if obj is None:
        return None

    json_str = None

    try:
        # Attempt direct JSON serialization
        json_str = json.dumps(obj, ensure_ascii=False, default=str)

    except TypeError as e:
        # Handle objects that are not directly serializable
        logger.debug(f"Direct JSON serialization failed for type {type(obj)}: {e}")
        try:
            # Fallback using string representation
            fallback_repr = str(obj)
            fallback_bytes = fallback_repr.encode('utf-8')

            if len(fallback_bytes) > MAX_TEXT_LENGTH:
                # Truncate if too large
                truncated_bytes = fallback_bytes[:MAX_TEXT_LENGTH]
                truncated_repr = truncated_bytes.decode('utf-8', errors='replace')

                # Advanced handling for multi-byte character truncation
                if truncated_repr.endswith('\ufffd') and MAX_TEXT_LENGTH > 1:
                     shorter_repr = fallback_bytes[:MAX_TEXT_LENGTH-1].decode('utf-8', errors='replace')
                     if not shorter_repr.endswith('\ufffd'):
                          truncated_repr = shorter_repr

                truncated_repr += "[TRUNCATED]"
                logger.warning(f"Fallback string representation truncated for type {type(obj)}.")
            else:
                truncated_repr = fallback_repr

            # Create structured representation of the error
            json_str = json.dumps({
                "error": f"Serialization failed for type {type(obj)}.",
                "fallback_repr": truncated_repr
            }, ensure_ascii=False)

        except Exception as fallback_e:
            # Final fallback if even string conversion fails
            logger.error(f"Could not serialize object of type {type(obj)} even with fallback: {fallback_e}")
            json_str = json.dumps({
                "error": f"Unserializable object type {type(obj)}. Fallback failed.",
                "critical_error": str(fallback_e)
            }, ensure_ascii=False)

    # Check final length regardless of serialization path
    if json_str is None:
         logger.error(f"Internal error: json_str is None after serialization attempt for object of type {type(obj)}")
         return json.dumps({
             "error": "Internal serialization error occurred.",
             "original_type": str(type(obj))
         }, ensure_ascii=False)

    # Check if final result exceeds max length
    final_bytes = json_str.encode('utf-8')
    if len(final_bytes) > MAX_TEXT_LENGTH:
        logger.warning(f"Serialized JSON string exceeds max length ({MAX_TEXT_LENGTH} bytes)")
        preview_str = json_str[:200] + ("..." if len(json_str) > 200 else "")
        return json.dumps({
            "error": "Serialized content exceeded maximum length.",
            "original_type": str(type(obj)),
            "preview": preview_str
        }, ensure_ascii=False)
    else:
        return json_str
\end{minted}
\end{pageablecode}
This highly sophisticated serialization function:
\begin{enumerate}[label=\arabic*.]
    \item Handles arbitrary Python objects
    \item Implements multiple fallback strategies
    \item Properly handles UTF-8 encoding and truncation
    \item Preserves information about serialization failures
    \item Returns structured error information
    \item Enforces maximum content length limits
\end{enumerate}

\subsection*{LLM Prompt Templates for Meta-Cognition}

The system uses sophisticated prompt templates for LLM-based reflection:

\subsubsection*{Consolidation Prompts}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _generate_consolidation_prompt(memories: List[Dict], consolidation_type: str) -> str:
    """Generates a prompt for memory consolidation."""
    # Format memories with metadata
    memory_texts = []
    for i, memory in enumerate(memories[:20], 1):
        desc = memory.get("description") or ""
        content_preview = (memory.get("content", "") or "")[:300]
        mem_type = memory.get("memory_type", "N/A")
        importance = memory.get("importance", 5.0)
        confidence = memory.get("confidence", 1.0)
        created_ts = memory.get("created_at", 0)
        created_dt_str = datetime.fromtimestamp(created_ts).strftime('%Y-%m-%d %H:%M') if created_ts else "Unknown Date"
        mem_id_short = memory.get("memory_id", "UNKNOWN")[:8]

        formatted = f"--- MEMORY #{i} (ID: {mem_id_short}..., Type: {mem_type}, Importance: {importance:.1f}, Confidence: {confidence:.1f}, Date: {created_dt_str}) ---\n"
        if desc:
            formatted += f"Description: {desc}\n"
        formatted += f"Content Preview: {content_preview}"
        # Indicate truncation
        if len(memory.get("content", "")) > 300:
            formatted += "...\n"
        else:
            formatted += "\n"
        memory_texts.append(formatted)

    memories_str = "\n".join(memory_texts)

    # Base prompt template
    base_prompt = f"""You are an advanced cognitive system processing and consolidating memories for an AI agent. Below are {len(memories)} memory items containing information, observations, and insights relevant to a task. Your goal is to perform a specific type of consolidation: '{consolidation_type}'.

Analyze the following memories carefully:

{memories_str}
--- END OF MEMORIES ---
"""

    # Add type-specific instructions
    if consolidation_type == "summary":
        base_prompt += """TASK: Create a comprehensive and coherent summary...
        [detailed instructions for summarization]
        """
    elif consolidation_type == "insight":
        base_prompt += """TASK: Generate high-level insights...
        [detailed instructions for insight generation]
        """
    # Additional consolidation types...

    return base_prompt
\end{minted}
\end{pageablecode}

\subsubsection*{Reflection Prompts}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _generate_reflection_prompt(
    workflow_name: str,
    workflow_desc: Optional[str],
    operations: List[Dict],
    memories: Dict[str, Dict],
    reflection_type: str
) -> str:
    """Generates a prompt for reflective analysis."""
    # Format operations with context
    op_texts = []
    for i, op_data in enumerate(operations[:30], 1):
        op_ts_unix = op_data.get("timestamp", 0)
        op_ts_str = datetime.fromtimestamp(op_ts_unix).strftime('%Y-%m-%d %H:%M:%S') if op_ts_unix else "Unknown Time"
        op_type = op_data.get('operation', 'UNKNOWN').upper()
        mem_id = op_data.get('memory_id')
        action_id = op_data.get('action_id')

        # Extract operation details
        op_details_dict = {}
        op_data_raw = op_data.get('operation_data')
        if op_data_raw:
             try:
                  op_details_dict = json.loads(op_data_raw)
             except (json.JSONDecodeError, TypeError):
                  op_details_dict = {"raw_data": str(op_data_raw)[:50]}

        # Build rich description
        desc_parts = [f"OP #{i} ({op_ts_str})", f"Type: {op_type}"]
        if mem_id:
            mem_info = memories.get(mem_id)
            mem_desc_text = f"Mem({mem_id[:6]}..)"
            if mem_info:
                 mem_desc_text += f" Desc: {mem_info.get('description', 'N/A')[:40]}"
                 if mem_info.get('memory_type'):
                      mem_desc_text += f" Type: {mem_info['memory_type']}"
            desc_parts.append(mem_desc_text)

        if action_id:
            desc_parts.append(f"Action({action_id[:6]}..)")

        # Add operation data details
        detail_items = []
        for k, v in op_details_dict.items():
             if k not in ['content', 'description', 'embedding', 'prompt']:
                  detail_items.append(f"{k}={str(v)[:30]}")
        if detail_items:
            desc_parts.append(f"Data({', '.join(detail_items)})")

        op_texts.append(" | ".join(desc_parts))

    operations_str = "\n".join(op_texts)

    # Base prompt template
    base_prompt = f"""You are an advanced meta-cognitive system analyzing an AI agent's workflow: "{workflow_name}".
Workflow Description: {workflow_desc or 'N/A'}
Your task is to perform a '{reflection_type}' reflection based on the recent memory operations listed below. Analyze these operations to understand the agent's process, progress, and knowledge state.

RECENT OPERATIONS (Up to 30):
{operations_str}
"""

    # Add type-specific instructions
    if reflection_type == "summary":
        base_prompt += """TASK: Create a reflective summary...
        [detailed instructions for reflective summarization]
        """
    elif reflection_type == "progress":
        base_prompt += """TASK: Analyze the progress...
        [detailed instructions for progress analysis]
        """
    # Additional reflection types...

    return base_prompt
\end{minted}
\end{pageablecode}
These templates implement:
\begin{enumerate}[label=\arabic*.]
    \item Rich context formatting with metadata
    \item Type-specific detailed instructions
    \item Structured memory representation
    \item Operation history formatting with context
    \item Guidance tailored to different meta-cognitive tasks
\end{enumerate}

\subsection*{Integration Patterns for Complex Operations}

The system implements several integration patterns for complex operations:

\subsubsection*{Workflow Creation with Initial Thought}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def create_workflow(
    title: str,
    description: Optional[str] = None,
    goal: Optional[str] = None,
    tags: Optional[List[str]] = None,
    metadata: Optional[Dict[str, Any]] = None,
    parent_workflow_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Creates a new workflow, including a default thought chain and initial goal thought if specified."""
    # Validation and initialization...

    try:
        async with DBConnection(db_path) as conn:
            # Check parent workflow existence...

            # Serialize metadata
            metadata_json = await MemoryUtils.serialize(metadata)

            # Insert the main workflow record
            await conn.execute("""INSERT INTO workflows...""")

            # Process and associate tags
            await MemoryUtils.process_tags(conn, workflow_id, tags or [], "workflow")

            # Create the default thought chain associated with this workflow
            thought_chain_id = MemoryUtils.generate_id()
            chain_title = f"Main reasoning for: {title}"
            await conn.execute("""INSERT INTO thought_chains...""")

            # If a goal was provided, add it as the first thought in the default chain
            if goal:
                thought_id = MemoryUtils.generate_id()
                seq_no = await MemoryUtils.get_next_sequence_number(conn, thought_chain_id, "thoughts", "thought_chain_id")
                await conn.execute("""INSERT INTO thoughts...""")

            # Commit the transaction
            await conn.commit()

            # Prepare and return result
            # ...
    except ToolInputError:
        raise
    except Exception as e:
        # Log the error and raise a generic ToolError
        logger.error(f"Error creating workflow: {e}", exc_info=True)
        raise ToolError(f"Failed to create workflow: {str(e)}") from e
\end{minted}
\end{pageablecode}
This pattern:
\begin{enumerate}[label=\arabic*.]
    \item Creates multiple related objects in one transaction
    \item Establishes default chain for reasoning
    \item Optionally adds initial thought/goal
    \item Ensures atomicity through transaction management
\end{enumerate}

\subsubsection*{Action Recording with Episodic Memory}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def record_action_start(
    workflow_id: str,
    action_type: str,
    reasoning: str,
    tool_name: Optional[str] = None,
    tool_args: Optional[Dict[str, Any]] = None,
    title: Optional[str] = None,
    parent_action_id: Optional[str] = None,
    tags: Optional[List[str]] = None,
    related_thought_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Records the start of an action within a workflow and creates a corresponding episodic memory."""
    # Validation and initialization...

    try:
        async with DBConnection(db_path) as conn:
            # Existence checks...

            # Determine sequence and auto-title...

            # Insert action record
            tool_args_json = await MemoryUtils.serialize(tool_args)
            await conn.execute("""INSERT INTO actions...""")

            # Process tags
            await MemoryUtils.process_tags(conn, action_id, tags or [], "action")

            # Link to related thought
            if related_thought_id:
                await conn.execute("UPDATE thoughts SET relevant_action_id = ? WHERE thought_id = ?",
                                 (action_id, related_thought_id))

            # Create linked episodic memory
            memory_id = MemoryUtils.generate_id()
            memory_content = f"Started action [{sequence_number}] '{auto_title}' ({action_type_enum.value}). Reasoning: {reasoning}"
            if tool_name:
                 memory_content += f" Tool: {tool_name}."
            mem_tags = ["action_start", action_type_enum.value] + (tags or [])
            mem_tags_json = json.dumps(list(set(mem_tags)))

            await conn.execute("""INSERT INTO memories...""")
            await MemoryUtils._log_memory_operation(conn, workflow_id, "create_from_action_start", memory_id, action_id)

            # Update workflow timestamp
            await conn.execute("UPDATE workflows SET updated_at = ?, last_active = ? WHERE workflow_id = ?",
                             (now_unix, now_unix, workflow_id))

            # Commit transaction
            await conn.commit()

            # Prepare and return result
            # ...
    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error recording action start: {e}", exc_info=True)
        raise ToolError(f"Failed to record action start: {str(e)}") from e
\end{minted}
\end{pageablecode}
This pattern:
\begin{enumerate}[label=\arabic*.]
    \item Records action details
    \item Automatically creates linked episodic memory
    \item Updates related entities (thoughts, workflow)
    \item Maintains bidirectional references
    \item Ensures proper tagging and categorization
\end{enumerate}

\subsubsection*{Thought Recording with Optional Memory Creation}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def record_thought(
    workflow_id: str,
    content: str,
    thought_type: str = "inference",
    thought_chain_id: Optional[str] = None,
    parent_thought_id: Optional[str] = None,
    relevant_action_id: Optional[str] = None,
    relevant_artifact_id: Optional[str] = None,
    relevant_memory_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH,
    conn: Optional[aiosqlite.Connection] = None
) -> Dict[str, Any]:
    """Records a thought in a reasoning chain, potentially linking to memory and creating an associated memory entry."""
    # Validation...

    thought_id = MemoryUtils.generate_id()
    now_unix = int(time.time())
    linked_memory_id = None

    async def _perform_db_operations(db_conn: aiosqlite.Connection):
        """Inner function to perform DB ops using the provided connection."""
        nonlocal linked_memory_id

        # Existence checks...

        # Determine target thought chain...

        # Get sequence number...

        # Insert thought record...

        # Update workflow timestamp...

        # Create linked memory for important thoughts
        important_thought_types = [
            ThoughtType.GOAL.value, ThoughtType.DECISION.value, ThoughtType.SUMMARY.value,
            ThoughtType.REFLECTION.value, ThoughtType.HYPOTHESIS.value
        ]

        if thought_type_enum.value in important_thought_types:
            linked_memory_id = MemoryUtils.generate_id()
            mem_content = f"Thought [{sequence_number}] ({thought_type_enum.value.capitalize()}): {content}"
            mem_tags = ["reasoning", thought_type_enum.value]
            mem_importance = 7.5 if thought_type_enum.value in [ThoughtType.GOAL.value, ThoughtType.DECISION.value] else 6.5

            await db_conn.execute("""INSERT INTO memories...""")
            await MemoryUtils._log_memory_operation(db_conn, workflow_id, "create_from_thought", linked_memory_id, None)

        return target_thought_chain_id, sequence_number

    try:
        target_thought_chain_id_res = None
        sequence_number_res = None

        if conn:
            # Use provided connection (transaction nesting)
            target_thought_chain_id_res, sequence_number_res = await _perform_db_operations(conn)
            # No commit - handled by outer transaction
        else:
            # Manage local transaction
            db_manager = DBConnection(db_path)
            async with db_manager.transaction() as local_conn:
                target_thought_chain_id_res, sequence_number_res = await _perform_db_operations(local_conn)
            # Commit handled by transaction manager

        # Prepare and return result
        # ...
    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error recording thought: {e}", exc_info=True)
        raise ToolError(f"Failed to record thought: {str(e)}") from e
\end{minted}
\end{pageablecode}
This pattern:
\begin{enumerate}[label=\arabic*.]
    \item Supports transaction nesting via optional connection parameter
    \item Conditionally creates memory entries for important thoughts
    \item Implements comprehensive linking between entities
    \item Uses inner functions for encapsulation
    \item Determines correct thought chain automatically
\end{enumerate}

\subsubsection*{Memory Consolidation with Linking}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def consolidate_memories(
    workflow_id: Optional[str] = None,
    target_memories: Optional[List[str]] = None,
    consolidation_type: str = "summary",
    query_filter: Optional[Dict[str, Any]] = None,
    max_source_memories: int = 20,
    prompt_override: Optional[str] = None,
    provider: str = LLMGatewayProvider.OPENAI.value,
    model: Optional[str] = None,
    store_result: bool = True,
    store_as_level: str = MemoryLevel.SEMANTIC.value,
    store_as_type: Optional[str] = None,
    max_tokens: int = 1000,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Consolidates multiple memories using an LLM to generate summaries, insights, etc."""
    # Validation...

    source_memories_list = []
    source_memory_ids = []
    effective_workflow_id = workflow_id

    try:
        async with DBConnection(db_path) as conn:
            # Select source memories (full logic)...

            # Generate consolidation prompt...

            # Call LLM via Gateway...
            provider_instance = await get_provider(provider)
            llm_result = await provider_instance.generate_completion(
                prompt=prompt, model=final_model, max_tokens=max_tokens, temperature=0.6
            )
            consolidated_content = llm_result.text.strip()

            # Store result as new memory...
            if store_result and consolidated_content:
                # Use derived importance and confidence...
                derived_importance = min(max(source_importances) + 0.5, 10.0)
                derived_confidence = min(sum(source_confidences) / len(source_confidences), 1.0)
                derived_confidence *= (1.0 - min(0.2, (len(source_memories_list) - 1) * 0.02))

                # Store the new memory...
                store_result_dict = await store_memory(
                    workflow_id=effective_workflow_id,
                    content=consolidated_content,
                    memory_type=result_type.value,
                    memory_level=result_level.value,
                    importance=round(derived_importance, 2),
                    confidence=round(derived_confidence, 3),
                    description=result_desc,
                    source=f"consolidation_{consolidation_type}",
                    tags=result_tags, context_data=result_context,
                    generate_embedding=True, db_path=db_path
                )
                stored_memory_id = store_result_dict.get("memory_id")

                # Link result to sources...
                if stored_memory_id:
                    link_tasks = []
                    for source_id in source_memory_ids:
                         link_task = create_memory_link(
                             source_memory_id=stored_memory_id,
                             target_memory_id=source_id,
                             link_type=LinkType.GENERALIZES.value,
                             description=f"Source for consolidated {consolidation_type}",
                             db_path=db_path
                         )
                         link_tasks.append(link_task)
                    await asyncio.gather(*link_tasks, return_exceptions=True)

            # Log operation...

            # Commit...

            # Prepare and return result...
    except (ToolInputError, ToolError):
        raise
    except Exception as e:
        logger.error(f"Failed to consolidate memories: {str(e)}", exc_info=True)
        raise ToolError(f"Failed to consolidate memories: {str(e)}") from e
\end{minted}
\end{pageablecode}
This pattern:
\begin{enumerate}[label=\arabic*.]
    \item Integrates with external LLM services
    \item Implements sophisticated source memory selection
    \item Derives importance and confidence heuristically
    \item Creates bidirectional links to source memories
    \item Uses asynchronous link creation with gather
\end{enumerate}

\subsubsection*{Hybrid Search with Weighted Scoring}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def hybrid_search_memories(
    query: str,
    workflow_id: Optional[str] = None,
    limit: int = 10,
    offset: int = 0,
    semantic_weight: float = 0.6,
    keyword_weight: float = 0.4,
    # Additional parameters...
) -> Dict[str, Any]:
    """Performs a hybrid search combining semantic similarity and keyword/filtered relevance."""
    # Validation...

    try:
        async with DBConnection(db_path) as conn:
            # --- Step 1: Semantic Search ---
            semantic_results: List[Tuple[str, float]] = []
            if norm_sem_weight > 0:
                try:
                    semantic_candidate_limit = min(max(limit * 5, 50), MAX_SEMANTIC_CANDIDATES)
                    semantic_results = await _find_similar_memories(
                        conn=conn,
                        query_text=query,
                        workflow_id=workflow_id,
                        limit=semantic_candidate_limit,
                        threshold=0.1,  # Lower threshold for hybrid
                        memory_level=memory_level,
                        memory_type=memory_type
                    )
                    for mem_id, score in semantic_results:
                        combined_scores[mem_id]["semantic"] = score
                except Exception as sem_err:
                    logger.warning(f"Semantic search part failed in hybrid search: {sem_err}")

            # --- Step 2: Keyword/Filtered Search ---
            if norm_key_weight > 0:
                # Build query with filters...
                # Execute query...
                # Calculate raw scores...
                # Normalize keyword scores...
                for mem_id, raw_score in raw_keyword_scores.items():
                    normalized_kw_score = min(max(raw_score / normalization_factor, 0.0), 1.0)
                    combined_scores[mem_id]["keyword"] = normalized_kw_score

            # --- Step 3: Calculate Hybrid Score ---
            if combined_scores:
                for _mem_id, scores in combined_scores.items():
                    scores["hybrid"] = (scores["semantic"] * norm_sem_weight) + (scores["keyword"] * norm_key_weight)

                # Sort by hybrid score
                sorted_ids_scores = sorted(combined_scores.items(), key=lambda item: item[1]["hybrid"], reverse=True)

                # Apply pagination after ranking
                paginated_ids_scores = sorted_ids_scores[offset : offset + limit]
                final_ranked_ids = [item[0] for item in paginated_ids_scores]
                final_scores_map = {item[0]: item[1] for item in paginated_ids_scores}

            # --- Step 4-7: Fetch details, links, reconstruct results, update access ---
            # ...

            # Return final results...
    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Hybrid search failed: {str(e)}", emoji_key="x", exc_info=True)
        raise ToolError(f"Hybrid search failed: {str(e)}") from e
\end{minted}
\end{pageablecode}
This pattern:
\begin{enumerate}[label=\arabic*.]
    \item Combines vector similarity and keyword search
    \item Implements weighted scoring with normalization
    \item Applies filters and pagination efficiently
    \item Handles score normalization for different ranges
    \item Optimizes database access with batched operations
\end{enumerate}

\subsection*{System Initialization and Configuration}

The system includes comprehensive initialization:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def initialize_memory_system(db_path: str = DEFAULT_DB_PATH) -> Dict[str, Any]:
    """Initializes the Unified Agent Memory system and checks embedding service status."""
    start_time = time.time()
    logger.info("Initializing Unified Memory System...", emoji_key="rocket")
    embedding_service_warning = None

    try:
        # Initialize/Verify Database Schema
        async with DBConnection(db_path) as conn:
             # Test connection with simple query
            cursor = await conn.execute("SELECT count(*) FROM workflows")
            _ = await cursor.fetchone()
            await cursor.close()
        logger.success("Unified Memory System database connection verified.", emoji_key="database")

        # Verify EmbeddingService functionality
        try:
            embedding_service = get_embedding_service()
            if embedding_service.client is not None:
                logger.info("EmbeddingService initialized and functional.", emoji_key="brain")
            else:
                embedding_service_warning = "EmbeddingService client not available. Embeddings disabled."
                logger.error(embedding_service_warning, emoji_key="warning")
                raise ToolError(embedding_service_warning)
        except Exception as embed_init_err:
             if not isinstance(embed_init_err, ToolError):
                 embedding_service_warning = f"Failed to initialize EmbeddingService: {str(embed_init_err)}"
                 logger.error(embedding_service_warning, emoji_key="error", exc_info=True)
                 raise ToolError(embedding_service_warning) from embed_init_err
             else:
                 raise embed_init_err

        # Return success status
        processing_time = time.time() - start_time
        logger.success("Unified Memory System initialized successfully.", emoji_key="white_check_mark", time=processing_time)

        return {
            "success": True,
            "message": "Unified Memory System initialized successfully.",
            "db_path": os.path.abspath(db_path),
            "embedding_service_functional": True,
            "embedding_service_warning": None,
            "processing_time": processing_time
        }
    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"Failed to initialize memory system: {str(e)}", emoji_key="x", exc_info=True, time=processing_time)
        if isinstance(e, ToolError):
            raise e
        else:
            raise ToolError(f"Memory system initialization failed: {str(e)}") from e
\end{minted}
\end{pageablecode}
This initialization:
\begin{enumerate}[label=\arabic*.]
    \item Verifies database connection and schema
    \item Checks embedding service functionality
    \item Provides detailed diagnostics
    \item Implements robust error handling
    \item Returns comprehensive status information
\end{enumerate}

\subsection*{System Architecture Summary}

The Unified Agent Memory and Cognitive System represents a sophisticated architecture for LLM agent cognitive modeling and workflow tracking. Its key architectural components include:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Multi-Level Memory Hierarchy}:
    \begin{itemize}
        \item Working memory for active processing
        \item Episodic memory for experiences and events
        \item Semantic memory for knowledge and facts
        \item Procedural memory for skills and procedures
    \end{itemize}

    \item \textbf{Workflow Tracking Structure}:
    \begin{itemize}
        \item Workflows as top-level containers
        \item Actions for agent activities and tool use
        \item Artifacts for outputs and files
        \item Thought chains for reasoning processes
    \end{itemize}

    \item \textbf{Associative Memory Graph}:
    \begin{itemize}
        \item Bidirectional links between memories
        \item Type-classified relationships
        \item Weighted link strengths
        \item Hierarchical organization
    \end{itemize}

    \item \textbf{Cognitive State Management}:
    \begin{itemize}
        \item Working memory management with capacity limits
        \item Focus tracking and automatic updating
        \item State persistence for context recovery
        \item Workflow context summarization
    \end{itemize}

    \item \textbf{Meta-Cognitive Capabilities}:
    \begin{itemize}
        \item Memory consolidation (summary, insight, procedural)
        \item Reflection generation (summary, progress, gaps, strengths, plan)
        \item Memory promotion based on usage patterns
        \item Complex visualization generation
    \end{itemize}

    \item \textbf{Vector-Based Semantic Search}:
    \begin{itemize}
        \item Integration with embedding services
        \item Cosine similarity calculation
        \item Hybrid search combining vector and keyword approaches
        \item Optimized candidate selection
    \end{itemize}

    \item \textbf{Operation Audit and Analytics}:
    \begin{itemize}
        \item Comprehensive operation logging
        \item Statistical analysis and reporting
        \item Performance measurement
        \item Memory access tracking
    \end{itemize}
\end{enumerate}

This architecture enables advanced agent cognition through:
\begin{enumerate}[label=\arabic*.]
    \item Systematic knowledge organization
    \item Context-aware reasoning
    \item Memory evolution and refinement
    \item Meta-cognitive reflection
    \item Structured workflow management
    \item Rich visualization and reporting
\end{enumerate}

The system provides a comprehensive foundation for sophisticated AI agent development with human-like memory organization and cognitive processes.

\subsection*{Architectural Motivation and Design Philosophy}

The Unified Agent Memory and Cognitive System emerges from a fundamental challenge in AI agent development: creating systems that can maintain context, learn from experiences, understand patterns, and exhibit increasingly human-like cognitive capabilities. Traditional approaches to LLM agent architecture frequently suffer from several limitations:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Context Window Constraints}: LLMs have finite context windows, making long-term memory management essential
    \item \textbf{Memory Organization}: Flat memory structures lack the nuanced organization that enables efficient retrieval
    \item \textbf{Cognitive Continuity}: Maintaining coherent agent identity and learning across sessions
    \item \textbf{Metacognitive Capabilities}: Enabling self-reflection and knowledge consolidation
\end{enumerate}

This memory system addresses these challenges through a cognitive architecture inspired by human memory models while being optimized for computational implementation. The four-tiered memory hierarchy (working, episodic, semantic, procedural) draws from established psychological frameworks but adapts them for practical AI implementation:
\begin{verbatim}
Working Memory  ‚Üí Episodic Memory  ‚Üí Semantic Memory  ‚Üí Procedural Memory
(Active focus)    (Experiences)       (Knowledge)        (Skills)
TTL: 30 minutes   TTL: 7 days         TTL: 30 days       TTL: 90 days
\end{verbatim}
This progression models how information flows through and evolves within the system, mimicking how human cognition transforms experiences into knowledge and eventually into skills.

\subsection*{Integration with Agent Architecture}

While not explicitly detailed in the code, the memory system is designed to integrate with a comprehensive agent architecture:
\begin{verbatim}
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Agent Architecture          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Perception‚îÇ  Reasoning  ‚îÇ   Action    ‚îÇ
‚îÇ           ‚îÇ             ‚îÇ  Generation ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ       Unified Memory System           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚î§
‚îÇ         Working Memory              ‚îÇ ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
‚îÇ  Episodic ‚îÇ Semantic ‚îÇ Procedural   ‚îÇM‚îÇ
‚îÇ  Memory   ‚îÇ Memory   ‚îÇ Memory       ‚îÇe‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§t‚îÇ
‚îÇ         Memory Operations           ‚îÇa‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§c‚îÇ
‚îÇ  Associative Memory Network         ‚îÇo‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§g‚îÇ
‚îÇ  Thought Chains & Reasoning         ‚îÇn‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§i‚îÇ
‚îÇ  Workflow & Action Tracking         ‚îÇt‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§i‚îÇ
‚îÇ  Cognitive State Management         ‚îÇo‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§n‚îÇ
‚îÇ  Structured Knowledge Storage       ‚îÇ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îò
\end{verbatim}
The system functions as the cognitive backbone of an agent, with:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Input Integration}: Perceptions, observations, and inputs flow into episodic memory
    \item \textbf{Reasoning Support}: Thought chains and semantic memory support reasoning processes
    \item \textbf{Action Context}: Actions are recorded with reasoning and outcomes for future reference
    \item \textbf{Metacognition}: Consolidation and reflection processes enable higher-order cognition
\end{enumerate}

Every part of the agent's functioning creates corresponding memory entries, allowing for persistent cognitive continuity across interactions.

\subsection*{Biomimetic Design and Cognitive Science Foundations}

The system incorporates several principles from cognitive science:

\subsubsection*{Spreading Activation and Associative Networks}

The memory link structure and semantic search implement a form of spreading activation, where retrieval of one memory activates related memories. Through functions like `ge\1\_linke\1\_memories()` and the working memory optimization in `aut\1\_updat\1\_focus()`, the system propagates attention and retrieval along associative pathways.

\subsubsection*{Memory Decay and Reinforcement}

The implementation of importance decay and access-based reinforcement mirrors human memory dynamics:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _compute_memory_relevance(importance, confidence, created_at, access_count, last_accessed):
    now = time.time()
    age_hours = (now - created_at) / 3600 if created_at else 0
    recency_factor = 1.0 / (1.0 + (now - (last_accessed or created_at)) / 86400)
    decayed_importance = max(0, importance * (1.0 - MEMORY_DECAY_RATE * age_hours))
    usage_boost = min(1.0 + (access_count / 10.0), 2.0) if access_count else 1.0
    relevance = (decayed_importance * usage_boost * confidence * recency_factor)
    return min(max(relevance, 0.0), 10.0)
\end{minted}
\end{pageablecode}
This function incorporates multiple cognitive principles:
\begin{itemize}
    \item Memories decay over time with a configurable rate
    \item Frequently accessed memories remain relevant longer
    \item Recently accessed memories are prioritized
    \item Confidence acts as a weighting factor for reliability
\end{itemize}

\subsubsection*{Memory Evolution Pathways}

The system models how information evolves through cognitive processing:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Observation ‚Üí Episodic}: Direct experiences and inputs enter as episodic memories
    \item \textbf{Episodic ‚Üí Semantic}: Through `promot\1\_memor\1\_level()`, frequently accessed episodic memories evolve into semantic knowledge
    \item \textbf{Semantic ‚Üí Procedural}: Knowledge that represents skills or procedures can be further promoted
    \item \textbf{Consolidation}: Through `consolidat\1\_memories()`, multiple related memories synthesize into higher-order insights
\end{enumerate}

This progression mimics human learning processes where repeated experiences transform into consolidated knowledge and eventually into skills and habits.

\subsection*{Architectural Implementation Details}

The system implements these cognitive principles through sophisticated database design and processing logic:

\subsubsection*{Circular References and Advanced SQL Techniques}

One unique aspect not fully explored in previous sections is the handling of circular references between memories and thoughts:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
-- Deferrable Circular Foreign Key Constraints for thoughts <-> memories
BEGIN IMMEDIATE TRANSACTION;
PRAGMA defer_foreign_keys = ON;

ALTER TABLE thoughts ADD CONSTRAINT fk_thoughts_memory
    FOREIGN KEY (relevant_memory_id) REFERENCES memories(memory_id)
    ON DELETE SET NULL DEFERRABLE INITIALLY DEFERRED;

ALTER TABLE memories ADD CONSTRAINT fk_memories_thought
    FOREIGN KEY (thought_id) REFERENCES thoughts(thought_id)
    ON DELETE SET NULL DEFERRABLE INITIALLY DEFERRED;

COMMIT;
\end{minted}
\end{pageablecode}
This implementation uses SQLite's deferred constraints to solve the chicken-and-egg problem of bidirectional references. This enables the creation of thoughts that reference memories, and memories that reference thoughts, without circular dependency issues during insertion.

\subsubsection*{Embedding Integration and Vector Search}

The vector embedding system represents a crucial advancement in semantic retrieval. The code implements:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Dimension-Aware Storage}: Embeddings include dimension metadata for compatibility checking
    \item \textbf{Binary BLOB Storage}: Vectors are efficiently stored as binary blobs
    \item \textbf{Model Tracking}: Embedding model information is preserved for future compatibility
    \item \textbf{Optimized Retrieval}: Candidate pre-filtering happens before similarity calculation
    \item \textbf{Hybrid Retrieval}: Combined vector and keyword search for robust memory access
\end{enumerate}

This sophisticated approach enables the "remembering-by-meaning" capability essential for human-like memory retrieval.

\subsection*{LLM Integration for Meta-Cognitive Functions}

A distinctive aspect of this architecture is its use of LLMs for meta-cognitive processes:

\subsubsection*{Prompt Engineering for Cognitive Functions}

The system includes carefully crafted prompts for various cognitive operations:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _generate_consolidation_prompt(memories: List[Dict], consolidation_type: str) -> str:
    # Format memory details...
    base_prompt = f"""You are an advanced cognitive system processing and consolidating
    memories for an AI agent. Below are {len(memories)} memory items containing
    information, observations, and insights relevant to a task. Your goal is to
    perform a specific type of consolidation: '{consolidation_type}'...
    """

    if consolidation_type == "summary":
        base_prompt += """TASK: Create a comprehensive and coherent summary that
        synthesizes the key information and context from ALL the provided memories...
        """
    # Additional consolidation types...
\end{minted}
\end{pageablecode}
These prompts implement different cognitive functions by leveraging the LLM's capabilities within structured contexts:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Summary}: Integration of information across memories
    \item \textbf{Insight}: Pattern recognition and implication detection
    \item \textbf{Procedural}: Extraction of generalizable procedures and methods
    \item \textbf{Question}: Identification of knowledge gaps and uncertainties
\end{enumerate}

Similarly, the reflection system analyzes agent behavior through targeted prompts:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _generate_reflection_prompt(workflow_name, workflow_desc, operations, memories, reflection_type):
    # Format operations with memory context...
    base_prompt = f"""You are an advanced meta-cognitive system analyzing an AI agent's
    workflow: "{workflow_name}"...
    """

    if reflection_type == "summary":
        base_prompt += """TASK: Create a reflective summary of this workflow's
        progress and current state...
        """
    # Additional reflection types...
\end{minted}
\end{pageablecode}
These meta-cognitive capabilities represent an emergent property when LLMs are used to analyze the agent's own memory and behavior.

\subsection*{Cognitive State Management}

An essential aspect of the system is its sophisticated cognitive state management:

\subsubsection*{Working Memory Optimization}

The working memory implements capacity-constrained optimization:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def optimize_working_memory(
    context_id: str,
    target_size: int = MAX_WORKING_MEMORY_SIZE,
    strategy: str = "balanced",  # balanced, importance, recency, diversity
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function implements multiple strategies for managing limited attentional capacity:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Balanced}: Considers all relevance factors
    \item \textbf{Importance}: Prioritizes important memories
    \item \textbf{Recency}: Prioritizes recent memories
    \item \textbf{Diversity}: Ensures varied memory types for broader context
\end{enumerate}

These strategies mirror different cognitive styles and attentional priorities in human cognition.

\subsubsection*{Focus Management and Attention}

The system implements attentional mechanisms through focus management:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def auto_update_focus(
    context_id: str,
    recent_actions_count: int = 3,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This function models automatic attention shifting through sophisticated heuristics:
\begin{itemize}
    \item Relevant to recent actions (recency bias)
    \item Memory type (questions and plans get priority)
    \item Memory level (semantic/procedural knowledge gets higher priority)
    \item Base relevance (importance, confidence)
\end{itemize}

This dynamic focus management creates an emergent attentional system resembling human cognitive focus.

\subsection*{Practical System Applications}

The unified memory system enables several practical capabilities for AI agents:

\subsubsection*{Persistent Context Across Sessions}

Through `sav\1\_cognitiv\1\_state()` and `loa\1\_cognitiv\1\_state()`, the system enables agents to maintain cognitive continuity across sessions. This allows for:

\begin{enumerate}[label=\arabic*.]
    \item Persistent user relationships that evolve over time
    \item Long-running projects with progress maintained between interactions
    \item Incremental knowledge accumulation and refinement
\end{enumerate}

\subsubsection*{Knowledge Evolution and Refinement}

The memory evolution pathways (episodic ‚Üí semantic ‚Üí procedural) enable knowledge maturation. Key applications include:

\begin{enumerate}[label=\arabic*.]
    \item Learning from repeated experiences
    \item Developing expertise through information refinement
    \item Converting learned patterns into reusable skills
    \item Building increasingly sophisticated domain understanding
\end{enumerate}

\subsubsection*{Meta-Cognitive Self-Improvement}

Through reflection and consolidation, the system enables emergent self-improvement capabilities:

\begin{enumerate}[label=\arabic*.]
    \item Identifying knowledge gaps through reflection
    \item Consolidating fragmented observations into coherent insights
    \item Recognizing patterns in its own problem-solving approaches
    \item Refining strategies based on past successes and failures
\end{enumerate}

These capabilities represent stepping stones toward more sophisticated cognitive agents with emergent meta-learning capabilities.

\subsection*{Performance Optimization and Scaling}

The system incorporates numerous optimizations for practical deployment:

\subsubsection*{Database Performance Tuning}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
SQLITE_PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA foreign_keys=ON",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA cache_size=-32000",
    "PRAGMA mmap_size=2147483647",
    "PRAGMA busy_timeout=30000"
]
\end{minted}
\end{pageablecode}
These pragmas optimize SQLite for:
\begin{enumerate}[label=\arabic*.]
    \item Write-Ahead Logging for concurrency
    \item Memory-based temporary storage
    \item Large cache size (32MB)
    \item Memory-mapped I/O for performance
    \item Extended busy timeout for reliability
\end{enumerate}

\subsubsection*{Query Optimization}

The schema includes comprehensive indexing:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{sql}
-- Workflow indices
CREATE INDEX IF NOT EXISTS idx_workflows_status ON workflows(status);
CREATE INDEX IF NOT EXISTS idx_workflows_parent ON workflows(parent_workflow_id);
CREATE INDEX IF NOT EXISTS idx_workflows_last_active ON workflows(last_active DESC);
-- Action indices
CREATE INDEX IF NOT EXISTS idx_actions_workflow_id ON actions(workflow_id);
-- Memory indices
CREATE INDEX IF NOT EXISTS idx_memories_workflow ON memories(workflow_id);
CREATE INDEX IF NOT EXISTS idx_memories_level ON memories(memory_level);
CREATE INDEX IF NOT EXISTS idx_memories_type ON memories(memory_type);
CREATE INDEX IF NOT EXISTS idx_memories_importance ON memories(importance DESC);
-- Many more indices...
\end{minted}
\end{pageablecode}
With over 30 carefully designed indices covering most query patterns, the system ensures efficient database access despite complex query patterns.

\subsubsection*{Memory Management}

The system implements sophisticated memory lifecycle management:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Time-To-Live (TTL)}: Different memory levels have appropriate default lifespans
    \item \textbf{Expiration Management}: `delet\1\_expire\1\_memories()` handles cleanup
    \item \textbf{Importance-Based Prioritization}: More important memories persist longer
    \item \textbf{Access Reinforcement}: Frequently used memories remain accessible
\end{enumerate}

For large-scale deployments, the system could be extended with:
\begin{itemize}
    \item Archival mechanisms for cold storage
    \item Distributed database backends for horizontal scaling
    \item Memory sharding across workflows
\end{itemize}

\subsection*{Visualization and Reporting Capabilities}

The system includes sophisticated visualization that wasn't fully explored in previous sections:

\subsubsection*{Interactive Mermaid Diagrams}

The `visualiz\1\_memor\1\_network()` and `visualiz\1\_reasonin\1\_chain()` functions generate interactive Mermaid diagrams that represent complex cognitive structures:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{mermaid}
graph TD
    M_abc123["Observation<br/>Column A is numerical<br/><i>(I: 6.0)</i>"]:::levelepisodic
    M_def456["Observation<br/>Column B is categorical<br/><i>(I: 6.0)</i>"]:::levelepisodic
    M_ghi789["Insight<br/>Data requires mixed analysis<br/><i>(I: 7.5)</i>"]:::levelsemantic

    M_ghi789 -- generalizes --> M_abc123
    M_ghi789 -- generalizes --> M_def456

    classDef levelepisodic fill:#e8f5e9,stroke:#4caf50,color:#388e3c,stroke-width:1px;
    classDef levelsemantic fill:#fffde7,stroke:#ffc107,color:#ffa000,stroke-width:1px;
\end{minted}
\end{pageablecode}
These visualizations enable:
\begin{enumerate}[label=\arabic*.]
    \item Understanding complex memory relationships
    \item Tracing reasoning pathways
    \item Identifying key knowledge structures
    \item Visualizing the agent's cognitive evolution
\end{enumerate}

\subsubsection*{Comprehensive Reports}

The `generat\1\_workflo\1\_report()` function creates detailed reports in multiple formats and styles:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Professional}: Formal business-style reporting
    \item \textbf{Concise}: Brief executive summaries
    \item \textbf{Narrative}: Story-based explanations
    \item \textbf{Technical}: Data-oriented technical documentation
\end{enumerate}

These reporting capabilities make the agent's internal processes transparent and understandable to human collaborators.

\subsection*{Integration Examples and Workflow}

Let's examine a complete workflow to understand how all components integrate:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Workflow Creation}: Agent creates a workflow container for a data analysis task with `creat\1\_workflow()`
    \item \textbf{Initial Goals}: Records initial goals as thoughts with `recor\1\_thought()`
    \item \textbf{Action Planning}: Plans data loading as an action with `recor\1\_actio\1\_start()`
    \item \textbf{Tool Execution}: Executes the data loading tool and records results with `recor\1\_actio\1\_completion()`
    \item \textbf{Artifact Creation}: Saves loaded data as an artifact with `recor\1\_artifact()`
    \item \textbf{Observation Creation}: Records observations about data as memories with `stor\1\_memory()`
    \item \textbf{Memory Linking}: Creates associations between related observations with `creat\1\_memor\1\_link()`
    \item \textbf{Insight Generation}: Consolidates observations into insights with `consolidat\1\_memories()`
    \item \textbf{Action Planning (Continued)}: Plans analysis methods based on insights
    \item \textbf{Execution and Recording}: Continues execution, recording results
    \item \textbf{Reflection}: Periodically reflects on progress with `generat\1\_reflection()`
    \item \textbf{Focus Management}: Shifts focus based on current priorities with `aut\1\_updat\1\_focus()`
    \item \textbf{Memory Evolution}: Frequently accessed observations evolve into semantic knowledge with `promot\1\_memor\1\_level()`
    \item \textbf{State Preservation}: Saves cognitive state with `sav\1\_cognitiv\1\_state()` for later continuation
\end{enumerate}

This integrated workflow demonstrates how the memory system supports sophisticated cognitive processes while maintaining continuity, evolving knowledge, and enabling metacognition.

\subsection*{Future Extensions and Research Directions}

The architecture lays groundwork for several advanced capabilities:

\subsubsection*{Multi-Agent Memory Sharing}

The system could be extended for knowledge sharing between agents through:
\begin{itemize}
    \item Standardized memory export/import
    \item Selective memory sharing protocols
    \item Cross-agent memory linking
    \item Collaborative knowledge building
\end{itemize}

\subsubsection*{Emotional and Motivational Components}

Cognitive architectures could incorporate:
\begin{itemize}
    \item Affective tagging of memories
    \item Motivation-based memory prioritization
    \item Emotional context for memory formation
    \item Value-aligned memory evolution
\end{itemize}

\subsubsection*{Neural-Symbolic Integration}

Future versions might incorporate:
\begin{itemize}
    \item Structured knowledge representations
    \item Logical reasoning over memory networks
    \item Constraint satisfaction for memory consistency
    \item Rule-based memory consolidation
\end{itemize}

\subsubsection*{Learning Optimizations}

The system could be enhanced with:
\begin{itemize}
    \item Adaptive memory promotion thresholds
    \item Personalized decay rates
    \item Learning rate parameters for different domains
    \item Automated memory organization optimization
\end{itemize}

\subsection*{Conclusion: Toward Emergent Cognitive Systems}

The Unified Agent Memory and Cognitive System represents a sophisticated architecture that bridges traditional database systems with cognitive science-inspired memory models. By implementing a structured yet flexible memory architecture with meta-cognitive capabilities, it creates a foundation for increasingly sophisticated AI agents that can:

\begin{enumerate}[label=\arabic*.]
    \item Learn from experiences through structured memory evolution
    \item Maintain cognitive continuity across sessions
    \item Develop increasingly refined understanding through consolidation
    \item Engage in self-reflection and improvement
    \item Organize and prioritize information effectively
\end{enumerate}

As LLM-based agents continue to evolve, sophisticated memory architectures like this one will become increasingly essential for overcoming the limitations of context windows and enabling truly persistent, learning agents with emergent cognitive capabilities.

The system ultimately aims to address a core challenge in AI development: creating agents that don't just simulate intelligence in the moment, but that accumulate, refine, and evolve knowledge over time - a crucial stepping stone toward more capable and general artificial intelligence.

% --- END OF UMS TECHNICAL ANALYSIS ---


\section{Appendix B: Agent Master Loop (AML) Technical Analysis}

% --- START OF AML TECHNICAL ANALYSIS ---

\subsection*{EideticEngine Agent Master Loop (AML) Technical Analysis}

\subsubsection*{Overview and Architecture}

The code implements the EideticEngine Agent Master Loop (AML), an AI agent orchestration system that manages a sophisticated cognitive agent with capabilities inspired by human memory and reasoning.

The system orchestrates a primary think-act cycle where the agent:
\begin{enumerate}[label=\arabic*.]
    \item Gathers comprehensive context from its memory systems
    \item Consults a Large Language Model (LLM) for decision-making
    \item Executes actions via tools
    \item Updates its plans and goals
    \item Performs periodic meta-cognitive operations
    \item Maintains persistent state for continuity
\end{enumerate}

\subsubsection*{Core Components}

\begin{enumerate}[label=\arabic*.]
    \item \textbf{AgentMasterLoop}: The main orchestrator class managing the entire agent lifecycle
    \item \textbf{AgentState}: Dataclass representing the runtime state of the agent
    \item \textbf{PlanStep}: Pydantic model representing individual steps in the agent's plan
    \item \textbf{External Dependencies}:
    \begin{itemize}
        \item MCPClient: Interface for the Unified Memory System (UMS)
        \item AsyncAnthropic: Client for the Anthropic Claude LLM
    \end{itemize}
\end{enumerate}

\subsubsection*{Key Features and Innovations}

\begin{itemize}
    \item \textbf{Goal Stack Management}: Hierarchical goal decomposition with explicit goal states
    \item \textbf{Mental Momentum Bias}: Preference for completing current plan steps when progress is stable
    \item \textbf{Adaptive Thresholds}: Dynamically adjusts reflection and consolidation based on performance metrics
    \item \textbf{Background Task Management}: Robust concurrent processing with semaphores and timeouts
    \item \textbf{Structure-Aware Context}: Multi-faceted context with freshness indicators and priority weighting
    \item \textbf{Plan Validation}: Detects dependency cycles and validates plan structure
    \item \textbf{Categorized Error Handling}: Sophisticated error recovery with typed errors and fallback strategies
\end{itemize}

\subsection*{Core Data Structures}

\subsubsection*{PlanStep (Pydantic BaseModel)}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
class PlanStep(BaseModel):
    id: str = Field(default_factory=lambda: f"step-{MemoryUtils.generate_id()[:8]}")
    description: str
    status: str = "planned"
    depends_on: List[str] = Field(default_factory=list)
    assigned_tool: Optional[str] = None
    tool_args: Optional[Dict[str, Any]] = None
    result_summary: Optional[str] = None
    is_parallel_group: Optional[str] = None
\end{minted}
\end{pageablecode}
This model represents a single step in the agent's plan, with fields for tracking:
\begin{itemize}
    \item A unique identifier
    \item Description of the action
    \item Current status (planned, in\_progress, completed, failed, skipped)
    \item Dependencies on other steps (for sequencing)
    \item Tool and arguments for execution
    \item Results after completion
    \item Optional parallel execution grouping
\end{itemize}

\subsubsection*{AgentState (Dataclass)}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
@dataclass
class AgentState:
    # Workflow management
    workflow_id: Optional[str] = None
    context_id: Optional[str] = None
    workflow_stack: List[str] = field(default_factory=list)

    # Goal management
    goal_stack: List[Dict[str, Any]] = field(default_factory=list)
    current_goal_id: Optional[str] = None

    # Planning & reasoning
    current_plan: List[PlanStep] = field(
        default_factory=lambda: [PlanStep(description=DEFAULT_PLAN_STEP)]
    )
    current_thought_chain_id: Optional[str] = None
    last_action_summary: str = "Loop initialized."
    current_loop: int = 0
    goal_achieved_flag: bool = False

    # Error tracking
    consecutive_error_count: int = 0
    needs_replan: bool = False
    last_error_details: Optional[Dict[str, Any]] = None

    # Meta-cognition metrics
    successful_actions_since_reflection: float = 0.0
    successful_actions_since_consolidation: float = 0.0
    loops_since_optimization: int = 0
    loops_since_promotion_check: int = 0
    loops_since_stats_adaptation: int = 0
    loops_since_maintenance: int = 0
    reflection_cycle_index: int = 0
    last_meta_feedback: Optional[str] = None

    # Adaptive thresholds
    current_reflection_threshold: int = BASE_REFLECTION_THRESHOLD
    current_consolidation_threshold: int = BASE_CONSOLIDATION_THRESHOLD

    # Tool statistics
    tool_usage_stats: Dict[str, Dict[str, Union[int, float]]] = field(
        default_factory=_default_tool_stats
    )

    # Background tasks (transient)
    background_tasks: Set[asyncio.Task] = field(
        default_factory=set, init=False, repr=False
    )
\end{minted}
\end{pageablecode}
This comprehensive state object maintains:
\begin{itemize}
    \item Workflow context and goal hierarchy
    \item Current execution plan and thought records
    \item Error tracking information
    \item Meta-cognitive metrics and thresholds
    \item Tool usage statistics
    \item Active background tasks
\end{itemize}

\subsection*{Key Constants and Configuration}

The system uses numerous constants for configuration, many of which can be overridden via environment variables:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# File and agent identification
AGENT_STATE_FILE = "agent_loop_state_v4.1.json"
AGENT_NAME = "EidenticEngine4.1"
MASTER_LEVEL_AGENT_LLM_MODEL_STRING = "claude-3-7-sonnet-20250219"

# Meta-cognition thresholds
BASE_REFLECTION_THRESHOLD = int(os.environ.get("BASE_REFLECTION_THRESHOLD", "7"))
BASE_CONSOLIDATION_THRESHOLD = int(os.environ.get("BASE_CONSOLIDATION_THRESHOLD", "12"))
MIN_REFLECTION_THRESHOLD = 3
MAX_REFLECTION_THRESHOLD = 15
MIN_CONSOLIDATION_THRESHOLD = 5
MAX_CONSOLIDATION_THRESHOLD = 25
THRESHOLD_ADAPTATION_DAMPENING = float(os.environ.get("THRESHOLD_DAMPENING", "0.75"))
MOMENTUM_THRESHOLD_BIAS_FACTOR = 1.2

# Interval constants
OPTIMIZATION_LOOP_INTERVAL = int(os.environ.get("OPTIMIZATION_INTERVAL", "8"))
MEMORY_PROMOTION_LOOP_INTERVAL = int(os.environ.get("PROMOTION_INTERVAL", "15"))
STATS_ADAPTATION_INTERVAL = int(os.environ.get("STATS_ADAPTATION_INTERVAL", "10"))
MAINTENANCE_INTERVAL = int(os.environ.get("MAINTENANCE_INTERVAL", "50"))

# Context limits
CONTEXT_RECENT_ACTIONS_FETCH_LIMIT = 10
CONTEXT_IMPORTANT_MEMORIES_FETCH_LIMIT = 7
CONTEXT_KEY_THOUGHTS_FETCH_LIMIT = 7
# ...and many more context limit constants

# Background task management
BACKGROUND_TASK_TIMEOUT_SECONDS = 60.0
MAX_CONCURRENT_BG_TASKS = 10
\end{minted}
\end{pageablecode}

\subsection*{Unified Memory System Tool Constants}

The system defines constants for UMS tool names to ensure consistency:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# Core memory tools
TOOL_STORE_MEMORY = "unified_memory:store_memory"
TOOL_UPDATE_MEMORY = "unified_memory:update_memory"
TOOL_GET_MEMORY_BY_ID = "unified_memory:get_memory_by_id"
TOOL_HYBRID_SEARCH = "unified_memory:hybrid_search_memories"
TOOL_SEMANTIC_SEARCH = "unified_memory:search_semantic_memories"
TOOL_QUERY_MEMORIES = "unified_memory:query_memories"

# Working memory tools
TOOL_GET_WORKING_MEMORY = "unified_memory:get_working_memory"
TOOL_OPTIMIZE_WM = "unified_memory:optimize_working_memory"
TOOL_AUTO_FOCUS = "unified_memory:auto_update_focus"

# Meta-cognitive tools
TOOL_REFLECTION = "unified_memory:generate_reflection"
TOOL_CONSOLIDATION = "unified_memory:consolidate_memories"
TOOL_PROMOTE_MEM = "unified_memory:promote_memory_level"

# Goal stack tools
TOOL_PUSH_SUB_GOAL = "unified_memory:push_sub_goal"
TOOL_MARK_GOAL_STATUS = "unified_memory:mark_goal_status"
TOOL_GET_GOAL_DETAILS = "unified_memory:get_goal_details"

# Workflow tools
TOOL_CREATE_WORKFLOW = "unified_memory:create_workflow"
TOOL_UPDATE_WORKFLOW_STATUS = "unified_memory:update_workflow_status"
TOOL_GET_WORKFLOW_DETAILS = "unified_memory:get_workflow_details"

# Internal agent tool
AGENT_TOOL_UPDATE_PLAN = "agent:update_plan"
\end{minted}
\end{pageablecode}

\subsection*{Core Utility Functions}

The system includes several helper functions:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _fmt_id(val: Any, length: int = 8) -> str:
    """Format an ID for readable logs, truncated to specified length."""

def _utf8_safe_slice(s: str, max_len: int) -> str:
    """Return a UTF-8 boundary-safe slice within max_len bytes."""

def _truncate_context(context: Dict[str, Any], max_len: int = 25_000) -> str:
    """Structure-aware context truncation with UTF-8 safe fallback."""

def _default_tool_stats() -> Dict[str, Dict[str, Union[int, float]]]:
    """Factory function for initializing tool usage statistics dictionary."""

def _detect_plan_cycle(self, plan: List[PlanStep]) -> bool:
    """Detects cyclic dependencies in the agent's plan using Depth First Search."""
\end{minted}
\end{pageablecode}

A distinctive feature of the context system is the explicit inclusion of temporal awareness through 'freshness' indicators:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
retrieval_timestamp = datetime.now(timezone.utc).isoformat()
\end{minted}
\end{pageablecode}
Throughout context gathering, each component is tagged with when it was retrieved:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
base_context['core_context']['retrieved_at'] = retrieval_timestamp
\end{minted}
\end{pageablecode}
These timestamp indicators serve several critical functions:
\begin{enumerate}[label=\arabic*.]
    \item They enable the LLM to reason about potentially stale information
    \item They help prioritize more recent information in decision-making
    \item They provide clear signals about the temporal relationship between different context components
\end{enumerate}

By tagging context components with retrieval timestamps, the system creates a time-aware context representation that helps the LLM make more temporally grounded decisions, mimicking human awareness of information recency.

\subsection*{AgentMasterLoop Core Implementation}

\subsubsection*{Initialization and Setup}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def __init__(self, mcp_client_instance: MCPClient, agent_state_file: str = AGENT_STATE_FILE):
\end{minted}
\end{pageablecode}
The constructor initializes the agent with a reference to the MCPClient (interface to the Unified Memory System) and state file path. It:
\begin{itemize}
    \item Validates the MCPClient dependency is available
    \item Stores a reference to the Anthropic client for LLM interaction
    \item Sets up logger configuration
    \item Initializes state and synchronization primitives
    \item Configures cognitive process parameters including thresholds and reflection types
\end{itemize}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def initialize(self) -> bool:
\end{minted}
\end{pageablecode}
This method performs crucial initialization steps:
\begin{enumerate}[label=\arabic*.]
    \item Loads prior agent state from the file system
    \item Fetches available tool schemas from MCPClient
    \item Filters schemas to only those relevant to this agent (UMS tools and internal agent tools)
    \item Verifies essential tools are available
    \item Validates the workflow ID loaded from state file
    \item Validates the loaded goal stack consistency
    \item Sets the default thought chain ID if needed
\end{enumerate}

The method uses these helper methods for specific tasks:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _load_agent_state(self) -> None:
\end{minted}
\end{pageablecode}
Loads and validates agent state from the JSON file, handling potential issues:
\begin{itemize}
    \item File not found (initializes with defaults)
    \item JSON decoding errors
    \item Structure mismatches between saved state and current AgentState dataclass
    \item Type validation and conversion for complex nested structures
    \item Consistency checks for loaded thresholds and goal stack
\end{itemize}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _validate_goal_stack_on_load(self):
\end{minted}
\end{pageablecode}
Verifies goal stack integrity:
\begin{itemize}
    \item Checks if goals still exist in the UMS
    \item Confirms goals belong to the correct workflow
    \item Removes invalid/missing goals
    \item Updates current\_goal\_id if needed
    \item Handles empty stack case
\end{itemize}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _set_default_thought_chain_id(self):
\end{minted}
\end{pageablecode}
Sets the active thought chain:
\begin{itemize}
    \item Retrieves thought chains associated with the current workflow
    \item Selects the primary (usually first created) chain
    \item Updates the agent state with the chain ID
\end{itemize}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _check_workflow_exists(self, workflow_id: str) -> bool:
\end{minted}
\end{pageablecode}
Efficiently verifies workflow existence by making a minimal UMS query

\subsubsection*{Main Agent Loop}

The primary execution loop is implemented in:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def run(self, goal: str, max_loops: int = 100) -> None:
\end{minted}
\end{pageablecode}
This method orchestrates the entire agent lifecycle:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Setup Phase}:
    \begin{itemize}
        \item Initializes workflow if none exists
        \item Creates initial thought chain if needed
        \item Ensures current\_goal\_id is set
    \end{itemize}

    \item \textbf{Main Loop Execution}: In each iteration:
    \begin{itemize}
        \item Run periodic cognitive tasks
        \item Gather comprehensive context
        \item Call LLM for decision
        \item Execute the decided action
        \item Apply plan updates (explicit or heuristic)
        \item Check error limits
        \item Save state
    \end{itemize}

    \item \textbf{Termination Handling}:
    \begin{itemize}
        \item Goal achieved signal
        \item Max loop reached
        \item Shutdown signal
        \item Error limit exceeded
        \item Final state save and cleanup
    \end{itemize}
\end{enumerate}

This loop continues until a termination condition is met: achieving the goal, reaching max iterations, receiving a shutdown signal, or hitting the error limit.

\subsubsection*{Context Gathering System}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _gather_context(self) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This sophisticated method assembles a multi-faceted context for the LLM:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Base Context Structure}: Creates initial context with basic state information
    \begin{itemize}
        \item Current loop count, workflow/context IDs, current plan
        \item Error details, replan flag, workflow stack
        \item Placeholders for fetched components
    \end{itemize}
    \item \textbf{Goal Stack Context}: Retrieves and structures goal hierarchy data
    \begin{itemize}
        \item Fetches details of current goal
        \item Includes summary of goal stack (limited by `CONTEX\1\_GOA\1\_STAC\1\_SHO\1\_LIMIT`)
        \item Handles cases where goal tools are unavailable
    \end{itemize}
    \item \textbf{Core Context}: Retrieves foundational workflow context
    \begin{itemize}
        \item Recent actions (limited by `CONTEX\1\_RECEN\1\_ACTION\1\_FETC\1\_LIMIT`)
        \item Important memories (limited by `CONTEX\1\_IMPORTAN\1\_MEMORIE\1\_FETC\1\_LIMIT`)
        \item Key thoughts (limited by `CONTEX\1\_KE\1\_THOUGHT\1\_FETC\1\_LIMIT`)
        \item Adds freshness timestamp
    \end{itemize}
    \item \textbf{Working Memory}: Retrieves active memories and focal point
    \begin{itemize}
        \item Gets current working memories and focal\_memory\_id
        \item Stores full result with freshness timestamp
        \item Extracts focal ID and memory list for later use
    \end{itemize}
    \item \textbf{Proactive Memories}: Performs goal-directed memory retrieval
    \begin{itemize}
        \item Formulates query based on current step/goal
        \item Uses hybrid search with semantic/keyword balance
        \item Formats results with scores and freshness
    \end{itemize}
    \item \textbf{Procedural Memories}: Finds relevant how-to knowledge
    \begin{itemize}
        \item Focuses query on accomplishing current step/goal
        \item Explicitly filters for procedural memory level
        \item Formats with scores and freshness
    \end{itemize}
    \item \textbf{Contextual Link Traversal}: Explores memory relationships
    \begin{itemize}
        \item Prioritizes focal memory, falls back to working or important memories
        \item Retrieves incoming and outgoing links
        \item Creates concise summary with limited link details
    \end{itemize}
    \item \textbf{Context Compression}: Checks if context exceeds token threshold
    \begin{itemize}
        \item Estimates token count using Anthropic API
        \item Compresses verbose parts if needed
        \item Generates summary using UMS summarization tool
    \end{itemize}
\end{enumerate}

The gathered context includes freshness indicators for each component and detailed error tracking throughout the process.

\subsubsection*{Contextual Link Traversal Strategy}

The system implements a sophisticated prioritization strategy for memory graph exploration during contextual link traversal:

\begin{enumerate}[label=\arabic*.]
    \item First prioritizes the focal memory from working memory (mimicking conscious attention)
    \item Falls back to the first memory in working memory if no focal exists (using working memory prominence)
    \item Falls back further to important memories from core context (leveraging episodic salience)
\end{enumerate}

This multi-level fallback approach models human associative memory traversal, where we naturally follow connections from whatever is most present in our awareness at the moment. By prioritizing the focal memory, the system mirrors how human attention guides associative thinking.

The system implements a sophisticated prioritization strategy during contextual link traversal that models human associative memory. It follows a three-tier fallback approach:

\begin{enumerate}[label=\arabic*.]
    \item First prioritizes the focal memory from working memory (mimicking conscious attention):
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   if focal_mem_id_from_wm:
       mem_id_to_traverse = focal_mem_id_from_wm
    \end{minted}
    \end{pageablecode}

    \item Falls back to the first memory in working memory if no focal exists:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   if not mem_id_to_traverse and working_mem_list_from_wm:
       first_wm_item = working_mem_list_from_wm[0]
       mem_id_to_traverse = first_wm_item.get('memory_id')
    \end{minted}
    \end{pageablecode}

    \item Falls back further to important memories from core context:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   if not mem_id_to_traverse:
       important_mem_list = core_ctx_data.get('important_memories', [])
       if important_mem_list:
           first_mem = important_mem_list[0]
           mem_id_to_traverse = first_mem.get('memory_id')
    \end{minted}
    \end{pageablecode}
\end{enumerate}

This multi-level fallback approach models how human attention guides associative thinking, always traversing from whatever is most present in our awareness at the moment.

\subsubsection*{LLM Interaction}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _call_agent_llm(self, goal: str, context: Dict[str, Any]) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This method handles the LLM interaction:

\begin{enumerate}[label=\arabic*.]
    \item Constructs the LLM prompt using \1\_construc\1\_agen\1\_prompt`
    \item Defines tools for the Anthropic API
    \item Makes the API call with retry logic for transient errors
    \item Parses the LLM response to extract the agent's decision:
    \begin{itemize}
        \item Tool call (`"decision": "cal\1\_tool", "too\1\_name": str, "arguments": dict`)
        \item Thought process (`"decision": "though\1\_process", "content": str`)
        \item Goal completion (`"decision": "complete", "summary": str`)
        \item Error (`"decision": "error", "message": str`)
        \item Plan update (`"decision": "pla\1\_update", "update\1\_pla\1\_steps": List[PlanStep]`)
    \end{itemize}
\end{enumerate}

The prompt construction is particularly sophisticated:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _construct_agent_prompt(self, goal: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
\end{minted}
\end{pageablecode}
This method creates a detailed prompt structure:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{System Instructions}:
    \begin{itemize}
        \item Agent identity and capabilities
        \item Available tools with schemas (highlighting essential cognitive tools)
        \item Current goal context from goal stack
        \item Detailed process instructions (analysis, error handling, reasoning, planning, action)
        \item Key considerations (goal focus, mental momentum, dependencies)
        \item Recovery strategies for different error types
    \end{itemize}
    \item \textbf{User Message}:
    \begin{itemize}
        \item Current context (JSON, robustly truncated)
        \item Current plan (JSON)
        \item Last action summary
        \item Error details (prominently highlighted if present)
        \item Meta-cognitive feedback
        \item Current goal reminder
        \item Final instruction
    \end{itemize}
\end{enumerate}

The prompt emphasizes goals, error recovery, plan repair, and mental momentum bias.

\subsubsection*{Tool Execution System}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _execute_tool_call_internal(self, tool_name: str, arguments: Dict[str, Any], record_action: bool = True, planned_dependencies: Optional[List[str]] = None) -> Dict[str, Any]:
\end{minted}
\end{pageablecode}
This central method handles all tool execution with comprehensive functionality:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Server Lookup}: Finds the appropriate server for the tool
    \item \textbf{Context Injection}: Adds workflow/context IDs if missing but relevant
    \item \textbf{Dependency Check}: Verifies prerequisites before execution
    \item \textbf{Internal Tool Handling}: Processes the `AGEN\1\_TOO\1\_UPDAT\1\_PLAN` tool internally
    \item \textbf{Plan Validation}: Checks for dependency cycles in plan updates
    \item \textbf{Action Recording}: Optionally records action start/dependencies in UMS
    \item \textbf{Tool Execution}: Calls the tool via MCPClient with retry logic
    \item \textbf{Result Processing}: Standardizes result format and categorizes errors
    \item \textbf{Background Triggers}: Initiates auto-linking and promotion checks when appropriate
    \item \textbf{State Updates}: Updates statistics, error details, and last action summary
    \item \textbf{Action Completion}: Records the outcome in UMS
    \item \textbf{Side Effects}: Handles workflow and goal stack implications
\end{enumerate}

The method uses several helper methods for specific subtasks:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _find_tool_server(self, tool_name: str) -> Optional[str]:
\end{minted}
\end{pageablecode}
Locates the server providing the specified tool, handling internal tools and server availability.

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _check_prerequisites(self, ids: List[str]) -> Tuple[bool, str]:
\end{minted}
\end{pageablecode}
Verifies that all specified prerequisite action IDs have status 'completed'.

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _record_action_start_internal(self, tool_name: str, tool_args: Dict[str, Any], planned_dependencies: Optional[List[str]] = None) -> Optional[str]:
\end{minted}
\end{pageablecode}
Records the start of an action in UMS, handling dependencies.

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _record_action_dependencies_internal(self, source_id: str, target_ids: List[str]) -> None:
\end{minted}
\end{pageablecode}
Records dependency relationships between actions in UMS.

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _record_action_completion_internal(self, action_id: str, result: Dict[str, Any]) -> None:
\end{minted}
\end{pageablecode}
Records the completion status and result for a given action.

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _handle_workflow_and_goal_side_effects(self, tool_name: str, arguments: Dict, result_content: Dict):
\end{minted}
\end{pageablecode}
Manages state changes triggered by specific tool outcomes, including:
\begin{itemize}
    \item Workflow creation/termination
    \item Goal stack updates (push/pop)
    \item Goal status changes
    \item Sub-workflow management
\end{itemize}

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _with_retries(self, coro_fun, *args, max_retries: int = 3, retry_exceptions: Tuple[type[BaseException], ...] = (...), retry_backoff: float = 2.0, jitter: Tuple[float, float] = (0.1, 0.5), **kwargs):
\end{minted}
\end{pageablecode}
Generic retry wrapper with exponential backoff and jitter, handling various exception types.

\subsubsection*{Plan Management}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _apply_heuristic_plan_update(self, last_decision: Dict[str, Any], last_tool_result_content: Optional[Dict[str, Any]] = None):
\end{minted}
\end{pageablecode}
This method updates the plan based on action outcomes when the LLM doesn't explicitly call `agent:updat\1\_plan`:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Success Case}:
    \begin{itemize}
        \item Marks step as completed
        \item Removes completed step from plan
        \item Generates a summary of the result
        \item Adds a final analysis step if plan becomes empty
        \item Resets error counter
    \end{itemize}

    \item \textbf{Failure Case}:
    \begin{itemize}
        \item Marks step as failed
        \item Keeps failed step in plan for context
        \item Inserts an analysis step for recovery
        \item Sets needs\_replan flag to true
        \item Updates consecutive error count
    \end{itemize}

    \item \textbf{Thought Case}:
    \begin{itemize}
        \item Marks step as completed
        \item Creates summary from thought content
        \item Adds next action step if plan becomes empty
        \item Counts as partial progress for metacognitive metrics
    \end{itemize}

    \item \textbf{Completion Case}:
    \begin{itemize}
        \item Marks as success
        \item Updates plan with a finalization step
        \item Status handled in main loop
    \end{itemize}
\end{enumerate}

The method also updates meta-cognitive counters based on success/failure.

The heuristic plan update system implements nuanced strategies for different decision types:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Success Case (Tool Call)}:
    \begin{itemize}
        \item Marks step as completed with detailed result summary
        \item Removes step from plan and handles empty plan
        \item Resets error counter and needs\_replan flag
        \item Increments meta-cognitive success counters
    \end{itemize}

    \item \textbf{Failure Case (Tool Call)}:
    \begin{itemize}
        \item Marks step as failed but preserves it in the plan
        \item Inserts an analysis step after the failed step
        \item Sets needs\_replan flag
        \item Increments consecutive\_error\_count
        \item Resets reflection counter to trigger faster reflection
    \end{itemize}

    \item \textbf{Thought Process Case}:
    \begin{itemize}
        \item Marks step as completed with thought content summary
        \item Removes step from plan and handles empty plan
        \item Increments meta-cognitive counters at reduced weight (0.5)
        \item Preserves needs\_replan state
    \end{itemize}

    \item \textbf{Completion Signal Case}:
    \begin{itemize}
        \item Creates finalization step
        \item Sets goal\_achieved\_flag for main loop termination
    \end{itemize}

    \item \textbf{Error or Unknown Decision Case}:
    \begin{itemize}
        \item Differentiates between plan update tool failures and other errors
        \item Inserts re-evaluation step
        \item Forces needs\_replan flag
        \item Updates error counters appropriately
    \end{itemize}
\end{enumerate}

These sophisticated heuristics enable coherent plan progression even when the LLM doesn't explicitly update the plan, creating a robust fallback that maintains execution consistency.

\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _detect_plan_cycle(self, plan: List[PlanStep]) -> bool:
\end{minted}
\end{pageablecode}
This method uses depth-first search to detect cyclic dependencies in the agent's plan:
\begin{enumerate}[label=\arabic*.]
    \item Builds an adjacency list from dependency relationships
    \item Implements DFS with path tracking for cycle detection
    \item Returns true if any cycle is found, false otherwise
\end{enumerate}

\subsubsection*{Goal Stack Management}

The core of the goal stack feature is distributed across several methods:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Goal Context Gathering}:
    \begin{itemize}
        \item Fetches current goal details from UMS
        \item Provides a summary of the goal stack
        \item Includes in LLM context
    \end{itemize}
    \item \textbf{Goal Stack Side Effects}:
    \begin{itemize}
        \item Creates root goal when creating new workflow
        \item Updates goal status when marked by LLM
        \item Pops completed/failed goals from stack
        \item Sets goal\_achieved\_flag when root goal completes
        \item Manages workflow status when goal stack empties
    \end{itemize}
    \item \textbf{Sub-Workflow Integration}:
    \begin{itemize}
        \item Associates sub-workflows with goals
        \item Updates goal status when sub-workflow completes
        \item Returns to parent workflow context when sub-workflow finishes
    \end{itemize}
\end{enumerate}

The goal stack is stored in the AgentState:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
goal_stack: List[Dict[str, Any]] = field(default_factory=list)
current_goal_id: Optional[str] = None
\end{minted}
\end{pageablecode}
Where each goal dictionary contains:
\begin{verbatim}
{
  "goa\1\_id": str,          # Unique identifier
  "description": str,      # Text description of the goal
  "status": str,           # "active", "completed", "failed"
  "paren\1\_goa\1\_id": str,   # Optional reference to parent goal
  # Other potential fields from UMS
}
\end{verbatim}

The system maintains a clear separation between two hierarchical stacks that serve distinct purposes:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Workflow Stack}: Manages execution contexts across potentially different environments or domains. A sub-workflow might represent an entirely separate task context with its own memory space and thought chains, but still connected to the parent workflow. This enables modularity and compartmentalization of execution environments.

    \item \textbf{Goal Stack}: Manages hierarchical decomposition of objectives within a single workflow context. Goals in the stack share the same memory space and thought chain, representing progressive refinement of intentions rather than context switching.
\end{enumerate}

This dual-stack approach enables sophisticated task management where the agent can both decompose goals hierarchically (via goal stack) and switch entire working contexts (via workflow stack). When a sub-workflow completes, the system automatically marks the corresponding goal in the parent workflow, creating a seamless bridge between these two hierarchical mechanisms.

\subsubsection*{Adaptive Threshold System}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _adapt_thresholds(self, stats: Dict[str, Any]) -> None:
\end{minted}
\end{pageablecode}
This method dynamically adjusts reflection and consolidation thresholds:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Consolidation Threshold Adaptation}:
    \begin{itemize}
        \item Analyzes episodic memory ratio in total memories
        \item Computes deviation from target ratio range
        \item Adjusts threshold to maintain optimal memory balance
        \item Applies dampening to prevent large swings
    \end{itemize}
    \item \textbf{Reflection Threshold Adaptation}:
    \begin{itemize}
        \item Analyzes tool failure rate from usage statistics
        \item Computes deviation from target failure rate
        \item Adjusts threshold based on performance
        \item Applies dampening factor to stabilize changes
    \end{itemize}
    \item \textbf{Mental Momentum Bias}:
    \begin{itemize}
        \item Detects stable progress periods (low error rate)
        \item Applies positive bias to reflection threshold when stable
        \item Makes agent less likely to reflect during productive periods
        \item Multiplies adjustments by MOMENTUM\_THRESHOLD\_BIAS\_FACTOR
    \end{itemize}
\end{enumerate}

The system ensures all thresholds stay within configured MIN/MAX bounds. The Mental Momentum Bias is implemented as a key cognitive feature in this function. When the agent detects stable progress periods (failure rate < 50% of target and zero consecutive errors), it applies the MOMENTUM\_THRESHOLD\_BIAS\_FACTOR multiplier (1.2) specifically to positive threshold adjustments. This creates a measurable 'flow state' effect where reflection becomes less frequent during productive periods, allowing the agent to maintain momentum. The system only applies this bias to increasing threshold adjustments, not to decreasing ones, ensuring quick response to errors while extending productive periods. This momentum effect is then dampened through the THRESHOLD\_ADAPTATION\_DAMPENING factor (0.75) to prevent overreaction to temporary improvements.

The Mental Momentum Bias represents a key cognitive feature of the system. When the agent is making stable progress (low error rates), it applies a multiplier (MOMENTUM\_THRESHOLD\_BIAS\_FACTOR) to increase the reflection threshold adjustment. This creates a 'flow state' where the agent is less likely to interrupt its productive work with reflection, mirroring how humans maintain momentum when things are going well. This bias dynamically balances between steady execution and necessary adaptation, creating more human-like task progression patterns.

\subsubsection*{Background Task Management}

The system implements sophisticated background task handling:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _start_background_task(self, coro_fn, *args, **kwargs) -> asyncio.Task:
\end{minted}
\end{pageablecode}
This method creates and manages background tasks with five critical reliability features:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{State snapshotting}: Captures workflow/context IDs at task creation time, ensuring tasks operate with consistent state even if the main agent state changes before execution.
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   # Snapshot critical state needed by the background task
   snapshot_wf_id = self.state.workflow_id
   snapshot_ctx_id = self.state.context_id
    \end{minted}
    \end{pageablecode}

    \item \textbf{Semaphore-based concurrency limiting}: Prevents resource exhaustion by limiting concurrent background tasks to MAX\_CONCURRENT\_BG\_TASKS.
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   await self._bg_task_semaphore.acquire()
    \end{minted}
    \end{pageablecode}

    \item \textbf{Timeout handling}: All background tasks have built-in timeouts to prevent hanging.
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   await asyncio.wait_for(
       coro_fn(...),
       timeout=BACKGROUND_TASK_TIMEOUT_SECONDS
   )
    \end{minted}
    \end{pageablecode}

    \item \textbf{Thread-safe tracking}: Uses asyncio locks to safely manage the background task set.
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   async with self._bg_tasks_lock:
       self.state.background_tasks.add(task)
    \end{minted}
    \end{pageablecode}

    \item \textbf{Guaranteed resource release}: Completion callbacks ensure semaphores are released even on failure.
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   task.add_done_callback(self._background_task_done)
    \end{minted}
    \end{pageablecode}
\end{enumerate}

The system provides robust shutdown cleanup through the \1\_cleanu\1\_backgroun\1\_tasks` method, which safely cancels all pending tasks, awaits their completion, and verifies semaphore state integrity. This comprehensive approach ensures cognitive background processes operate reliably without interrupting the main agent loop, mimicking human parallel thought processes.

\subsubsection*{Periodic Meta-Cognitive Tasks}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _run_periodic_tasks(self):
\end{minted}
\end{pageablecode}
This method orchestrates scheduled cognitive maintenance tasks:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Stats Computation \& Threshold Adaptation}:
    \begin{itemize}
        \item Triggered by STATS\_ADAPTATION\_INTERVAL
        \item Computes memory statistics
        \item Adapts thresholds based on performance
        \item Potentially triggers immediate consolidation if needed
    \end{itemize}
    \item \textbf{Reflection}:
    \begin{itemize}
        \item Triggered by successful actions exceeding threshold or replan flag
        \item Cycles through different reflection types
        \item Provides feedback for next prompt
        \item Forces replanning after significant insights
    \end{itemize}
    \item \textbf{Consolidation}:
    \begin{itemize}
        \item Triggered by successful actions exceeding threshold
        \item Summarizes episodic memories into semantic knowledge
        \item Integrates information across sources
        \item Provides feedback for next prompt
    \end{itemize}
    \item \textbf{Working Memory Optimization}:
    \begin{itemize}
        \item Triggered by OPTIMIZATION\_LOOP\_INTERVAL
        \item Improves relevance/diversity of working memory
        \item Updates focus to most important memory
        \item Maintains cognitive efficiency
    \end{itemize}
    \item \textbf{Memory Promotion Check}:
    \begin{itemize}
        \item Triggered by MEMORY\_PROMOTION\_LOOP\_INTERVAL
        \item Finds recently accessed memories
        \item Checks promotion criteria for each
        \item Elevates memory levels when appropriate
    \end{itemize}
    \item \textbf{Maintenance}:
    \begin{itemize}
        \item Triggered by MAINTENANCE\_INTERVAL
        \item Deletes expired memories
        \item Maintains memory system health
    \end{itemize}
\end{enumerate}

The method prioritizes tasks, handles exceptions, and ensures graceful shutdown if requested.
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _trigger_promotion_checks(self):
\end{minted}
\end{pageablecode}
Helper method that:
\begin{itemize}
    \item Queries for recently accessed memories
    \item Identifies candidates for promotion
    \item Schedules background checks for each
    \item Handles both Episodic‚ÜíSemantic and Semantic‚ÜíProcedural candidates
\end{itemize}

\subsection*{Comprehensive Error Handling System}

The AML implements a sophisticated error handling framework that categorizes errors, provides detailed context for recovery, and implements graceful degradation strategies.

\subsubsection*{Error Categorization and Recording}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# Error state tracking in AgentState
consecutive_error_count: int = 0
needs_replan: bool = False
last_error_details: Optional[Dict[str, Any]] = None

# Error categorization in _execute_tool_call_internal
error_type = "ToolExecutionError"  # Default category
status_code = res.get("status_code")
error_message = res.get("error", "Unknown failure")

if status_code == 412: error_type = "DependencyNotMetError"
elif status_code == 503: error_type = "ServerUnavailable"
elif "input" in str(error_message).lower() or "validation" in str(error_message).lower(): error_type = "InvalidInputError"
elif "timeout" in str(error_message).lower(): error_type = "NetworkError"
elif tool_name in [TOOL_PUSH_SUB_GOAL, TOOL_MARK_GOAL_STATUS] and ("not found" in str(error_message).lower() or "invalid" in str(error_message).lower()):
     error_type = "GoalManagementError"
\end{minted}
\end{pageablecode}
The system maintains a structured error record with:
\begin{itemize}
    \item Tool name and arguments that caused the error
    \item Error message and status code
    \item Categorized error type
    \item Additional context-specific fields
\end{itemize}

This structured approach allows for targeted recovery strategies and detailed feedback to the LLM.

\subsubsection*{Error Types and Recovery Strategies}

The system implements a sophisticated error classification system with at least ten distinct categories, each triggering specific recovery behaviors:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{InvalidInputError}: Occurs when tool arguments fail validation. Recovery involves reviewing schemas, correcting arguments, or selecting alternative tools.
    \item \textbf{DependencyNotMetError}: Triggered when prerequisite actions aren't completed. The system checks dependency status, waits for completion, or adjusts plan order.
    \item \textbf{ServerUnavailable/NetworkError}: Indicates tool servers are unreachable. The agent attempts alternative tools, implements waiting periods, or adjusts plans to account for unavailable services.
    \item \textbf{APILimitError/RateLimitError}: Occurs when external API limits are reached. Recovery includes implementing wait periods and reducing request frequency.
    \item \textbf{ToolExecutionError/ToolInternalError}: Represents failures during tool execution. The agent analyzes error messages to determine if different arguments or alternative tools might succeed.
    \item \textbf{PlanUpdateError}: Indicates invalid plan structure. The system re-examines steps and dependencies to correct structural issues.
    \item \textbf{PlanValidationError}: Triggered when logical issues like cycles are detected. The agent debugs dependencies and proposes corrected structures.
    \item \textbf{CancelledError}: Occurs when actions are cancelled, often during shutdown. The agent re-evaluates the current step upon resumption.
    \item \textbf{GoalManagementError}: Indicates failures in goal stack operations. Recovery involves reviewing the goal context and stack logic.
    \item \textbf{UnknownError/UnexpectedExecutionError}: Catchall for unclassified errors. The agent analyzes messages, simplifies steps, or seeks clarification.
\end{enumerate}

The system differentiates between transient errors (appropriate for retry) and permanent ones, with dedicated handling strategies for each category. This explicit categorization is communicated to the LLM for targeted recovery actions.

\subsubsection*{Error Handling Implementation}

The error handling is distributed across several layers:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Tool Call Level} (\1\_execut\1\_too\1\_cal\1\_internal`):
    \begin{itemize}
        \item Categorizes errors based on messages/codes
        \item Updates state.last\_error\_details with structured info
        \item Increments consecutive\_error\_count
        \item Sets needs\_replan flag when appropriate
        \item Updates last\_action\_summary with error context
    \end{itemize}
    \item \textbf{LLM Decision Level} (\1\_cal\1\_agen\1\_llm`):
    \begin{itemize}
        \item Handles API errors with specific categorization
        \item Retries transient errors with exponential backoff
        \item Returns structured error decision when needed
    \end{itemize}
    \item \textbf{Plan Update Level} (\1\_appl\1\_heuristi\1\_pla\1\_update`):
    \begin{itemize}
        \item Handles errors by marking steps as failed
        \item Inserts analysis steps for error recovery
        \item Adjusts counters based on failure type
    \end{itemize}
    \item \textbf{Main Loop Level} (`run`):
    \begin{itemize}
        \item Checks consecutive\_error\_count against MAX\_CONSECUTIVE\_ERRORS
        \item Terminates execution if error threshold exceeded
        \item Updates workflow status to FAILED if appropriate
    \end{itemize}
    \item \textbf{Background Task Level}:
    \begin{itemize}
        \item Implements timeout handling for all background tasks
        \item Manages exceptions without disrupting main loop
        \item Ensures semaphore release even on failure
    \end{itemize}
\end{enumerate}

The system also exposes error details prominently in the LLM prompt:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# From _construct_agent_prompt
if self.state.last_error_details:
    user_blocks += [
        "**CRITICAL: Address Last Error Details**:",
        "```json",
        json.dumps(self.state.last_error_details, indent=2, default=str),
        "```",
        "",
    ]
\end{minted}
\end{pageablecode}

\subsubsection*{Retry Mechanism}

The system implements sophisticated retry logic with:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _with_retries(
    self,
    coro_fun,
    *args,
    max_retries: int = 3,
    retry_exceptions: Tuple[type[BaseException], ...] = (...),
    retry_backoff: float = 2.0,
    jitter: Tuple[float, float] = (0.1, 0.5),
    **kwargs,
):
\end{minted}
\end{pageablecode}
This wrapper provides:
\begin{itemize}
    \item Configurable max retry attempts
    \item Exponential backoff (each delay = previous * backoff\_factor)
    \item Random jitter to prevent thundering herd problems
    \item Selective retry based on exception types
    \item Cancellation detection during retry waits
    \item Detailed logging of retry attempts
\end{itemize}

It's selectively applied based on operation idempotency:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# In _execute_tool_call_internal
idempotent = tool_name in {
    # Read-only operations are generally safe to retry
    TOOL_GET_CONTEXT, TOOL_GET_MEMORY_BY_ID, TOOL_SEMANTIC_SEARCH,
    TOOL_HYBRID_SEARCH, TOOL_GET_ACTION_DETAILS, TOOL_LIST_WORKFLOWS,
    # ...many more tools
}

# Execute with appropriate retry count
raw = await self._with_retries(
    _do_call,
    max_retries=3 if idempotent else 1,  # Retry only idempotent tools
    # Specify exceptions that should trigger a retry attempt
    retry_exceptions=(
        ToolError, ToolInputError,  # Specific MCP errors
        asyncio.TimeoutError, ConnectionError,  # Common network issues
        APIConnectionError, RateLimitError, APIStatusError,  # Anthropic/API issues
    ),
)
\end{minted}
\end{pageablecode}

Beyond retry logic, the system implements comprehensive dynamic tool availability management:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _find_tool_server(self, tool_name: str) -> Optional[str]:
\end{minted}
\end{pageablecode}
This method handles tool availability by:
\begin{enumerate}[label=\arabic*.]
    \item Checking if the tool's server is currently active in the MCPClient's server manager
    \item Providing special handling for the internal `AGEN\1\_TOO\1\_UPDAT\1\_PLAN` tool
    \item Attempting to route core tools to the 'CORE' server if available
\end{enumerate}

Throughout the codebase, tool calls are guarded with availability checks:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
if self._find_tool_server(tool_name):
    # Execute tool with proper handling
else:
    # Log unavailability and implement fallback behavior
\end{minted}
\end{pageablecode}

The system implements sophisticated fallback mechanisms when preferred tools are unavailable:
\begin{itemize}
    \item For search operations, falling back from hybrid search to pure semantic search
    \item For context gathering, continuing with partial context when certain components can't be fetched
    \item For meta-cognitive operations, skipping non-critical tasks while preserving core functionality
\end{itemize}

This robust handling of tool availability ensures the agent degrades gracefully in distributed environments where services may be temporarily unavailable, mimicking human adaptability to missing resources.

\subsection*{Token Estimation and Context Management}

\subsubsection*{Token Estimation}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _estimate_tokens_anthropic(self, data: Any) -> int:
\end{minted}
\end{pageablecode}
This method provides accurate token counting:
\begin{enumerate}[label=\arabic*.]
    \item Uses the Anthropic API's `coun\1\_tokens` method for precise estimation
    \item Handles both string and structured data by serializing if needed
    \item Provides a fallback heuristic (chars/4) if the API call fails
    \item Returns consistent integer results for all cases
\end{enumerate}
This token counting is crucial for:
\begin{itemize}
    \item Determining when context compression is needed
    \item Ensuring LLM inputs stay within model context limits
    \item Optimizing token usage for cost efficiency
\end{itemize}

\subsubsection*{Context Truncation and Compression}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _truncate_context(context: Dict[str, Any], max_len: int = 25_000) -> str:
\end{minted}
\end{pageablecode}
This sophisticated utility implements a cognitively-informed, multi-stage context truncation strategy:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Initial JSON Serialization}: Attempts standard serialization
    \item \textbf{Structure-Aware Prioritized Truncation}: If size exceeds limit, applies intelligent reductions:
    \begin{itemize}
        \item Truncates lists based on priority and SHOW\_LIMIT constants
        \item Applies different limits to different context types (working memory, recent actions, goal stack, etc.)
        \item Adds explicit notes about omissions to maintain context coherence
        \item Preserves original structure and semantics
    \end{itemize}
    \item \textbf{Prioritized Component Removal}: If still too large, removes entire low-priority components in this specific order:
    \begin{itemize}
        \item relevant\_procedures (lowest priority)
        \item proactive\_memories
        \item contextual\_links
        \item core context components (in priority order)
        \item current\_working\_memory (higher priority)
        \item current\_goal\_context (highest priority)
    \end{itemize}
    \item \textbf{UTF-8 Safe Byte Slice}: As last resort, applies direct byte slicing
    \begin{itemize}
        \item Attempts to find valid JSON boundaries
        \item Adds explicit truncation markers
        \item Ensures resulting string is valid UTF-8
    \end{itemize}
\end{enumerate}

This approach not only preserves size constraints but maintains the most critical information for decision-making.

For context exceeding token limits, the system implements LLM-based compression:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# In _gather_context
if estimated_tokens > CONTEXT_MAX_TOKENS_COMPRESS_THRESHOLD:
    self.logger.warning(f"Context ({estimated_tokens} tokens) exceeds threshold {CONTEXT_MAX_TOKENS_COMPRESS_THRESHOLD}. Attempting summary compression.")
    # Check if summarization tool is available
    if self._find_tool_server(TOOL_SUMMARIZE_TEXT):
        # Strategy: Summarize the potentially longest/most verbose part first
        # Example: Summarize 'core_context' -> 'recent_actions' if it exists and is large
        core_ctx = base_context.get("core_context")
        actions_to_summarize = None
        # ... summarization logic ...
        summary_result = await self._execute_tool_call_internal(
            TOOL_SUMMARIZE_TEXT,
            {
                "text_to_summarize": actions_text,
                "target_tokens": CONTEXT_COMPRESSION_TARGET_TOKENS,
                "prompt_template": "summarize_context_block",
                "context_type": "actions",
                "workflow_id": current_wf_id,
                "record_summary": False
            },
            record_action=False
        )
\end{minted}
\end{pageablecode}
This compression targets the most verbose parts first, generating concise summaries while preserving critical details.

A distinctive feature of the context system is the explicit inclusion of temporal awareness through 'freshness' indicators:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
retrieval_timestamp = datetime.now(timezone.utc).isoformat()
\end{minted}
\end{pageablecode}
Throughout context gathering, each component is tagged with when it was retrieved:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
base_context['core_context']['retrieved_at'] = retrieval_timestamp
\end{minted}
\end{pageablecode}
These timestamp indicators serve several critical functions:
\begin{enumerate}[label=\arabic*.]
    \item They enable the LLM to reason about potentially stale information
    \item They help prioritize more recent information in decision-making
    \item They provide clear signals about the temporal relationship between different context components
\end{enumerate}

By tagging context components with retrieval timestamps, the system creates a time-aware context representation that helps the LLM make more temporally grounded decisions, mimicking human awareness of information recency.

\subsection*{State Persistence System}

\subsubsection*{State Saving}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _save_agent_state(self) -> None:
\end{minted}
\end{pageablecode}
This method implements atomically reliable state persistence designed to survive system crashes:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{State Serialization}:
    \begin{itemize}
        \item Converts dataclass to dictionary with `dataclasses.asdict`
        \item Adds timestamp for when state was saved
        \item Removes non-serializable fields (background\_tasks)
        \item Converts complex types (PlanStep, defaultdict) to serializable forms
    \end{itemize}
    \item \textbf{Atomic File Write}:
    \begin{itemize}
        \item Creates a process-specific temporary file to prevent collisions
        \begin{pageablecode}
        \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   tmp_file = self.agent_state_file.with_suffix(f'.tmp_{os.getpid()}')
        \end{minted}
        \end{pageablecode}
        \item Uses `os.fsync` to ensure physical disk write
        \begin{pageablecode}
        \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   os.fsync(f.fileno())
        \end{minted}
        \end{pageablecode}
        \item Atomically replaces old file with `os.replace`
        \begin{pageablecode}
        \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   os.replace(tmp_file, self.agent_state_file)
        \end{minted}
        \end{pageablecode}
    \end{itemize}
    \item \textbf{Error Handling}:
    \begin{itemize}
        \item Ensures directory exists before writing
        \item Handles serialization errors with fallbacks
        \item Cleans up temporary file on write failure
        \item Preserves original file if any step fails
    \end{itemize}
\end{enumerate}

This approach ensures state integrity even with process crashes or power failures, providing guaranteed persistence for long-running agents.

\subsubsection*{State Loading}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def _load_agent_state(self) -> None:
\end{minted}
\end{pageablecode}
This method handles robust state restoration:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{File Reading}:
    \begin{itemize}
        \item Checks if state file exists
        \item Reads and parses JSON asynchronously
    \end{itemize}
    \item \textbf{Field Processing}:
    \begin{itemize}
        \item Iterates through AgentState dataclass fields
        \item Handles special fields requiring conversion:
        \begin{itemize}
            \item \code{curren\1\_plan}: Validates and converts to PlanStep objects
            \item \code{too\1\_usag\1\_stats}: Reconstructs defaultdict structure
            \item \code{goa\1\_stack}: Validates structure and content
        \end{itemize}
    \end{itemize}
    \item \textbf{Validation and Correction}:
    \begin{itemize}
        \item Ensures thresholds are within MIN/MAX bounds
        \item Verifies goal stack consistency
        \item Ensures current\_goal\_id points to a goal in the stack
        \item Checks for unknown fields in saved state
    \end{itemize}
    \item \textbf{Error Recovery}:
    \begin{itemize}
        \item Handles file not found gracefully
        \item Recovers from JSON decoding errors
        \item Falls back to defaults on structure mismatches
        \item Ensures critical fields are always initialized
    \end{itemize}
\end{enumerate}

This implementation balances flexibility with safety, allowing for schema evolution while maintaining stability.

\subsection*{Shutdown Mechanisms}

The system implements comprehensive shutdown handling:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def shutdown(self) -> None:
\end{minted}
\end{pageablecode}
This method provides graceful termination:
\begin{enumerate}[label=\arabic*.]
    \item Sets the shutdown event to signal loops and tasks
    \item Waits for background tasks to complete or cancel
    \item Saves the final agent state
    \item Logs completion of shutdown process
\end{enumerate}

The shutdown signal propagates throughout the system:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Main Loop Detection}:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   # In run method
   if self._shutdown_event.is_set():
       break
    \end{minted}
    \end{pageablecode}
    \item \textbf{Background Task Cancellation}:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   async def _cleanup_background_tasks(self) -> None:
       # ... task gathering ...
       for t in tasks_to_cleanup:
           if not t.done():
               t.cancel()
       # ... wait for completion ...
    \end{minted}
    \end{pageablecode}
    \item \textbf{Retry Abortion}:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   # In _with_retries
   if self._shutdown_event.is_set():
       self.logger.warning(f"Shutdown signaled during retry wait for {coro_fun.__name__}. Aborting retry.")
       raise asyncio.CancelledError(f"Shutdown during retry for {coro_fun.__name__}") from last_exception
    \end{minted}
    \end{pageablecode}
    \item \textbf{Periodic Task Termination}:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
   # In _run_periodic_tasks
   if self._shutdown_event.is_set():
       self.logger.info("Shutdown detected during periodic tasks, aborting remaining.")
       break
    \end{minted}
    \end{pageablecode}
\end{enumerate}

The design ensures all operations check for shutdown signals regularly, maintaining responsiveness while allowing for clean termination.

\subsection*{Signal Handling Integration}

The system integrates with OS signals via asyncio:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# In run_agent_process

# Define the signal handler function
def signal_handler_wrapper(signum):
    signal_name = signal.Signals(signum).name
    log.warning(f"Signal {signal_name} received. Initiating graceful shutdown.")
    # Set the event to signal other tasks
    stop_event.set()
    # Trigger the agent's internal shutdown method asynchronously
    if agent_loop_instance:
         asyncio.create_task(agent_loop_instance.shutdown())

# Register the handler for SIGINT (Ctrl+C) and SIGTERM
for sig in [signal.SIGINT, signal.SIGTERM]:
    try:
        loop.add_signal_handler(sig, signal_handler_wrapper, sig)
        log.debug(f"Registered signal handler for {sig.name}")
    except ValueError:
        log.debug(f"Signal handler for {sig.name} may already be registered.")
    except NotImplementedError:
        log.warning(f"Signal handling for {sig.name} not supported on this platform.")
\end{minted}
\end{pageablecode}
This implementation:
\begin{enumerate}[label=\arabic*.]
    \item Registers handlers for OS termination signals
    \item Converts signals to shutdown events
    \item Triggers the agent's graceful shutdown sequence
    \item Handles platform-specific limitations
    \item Prevents double registration errors
\end{enumerate}

During execution, the system uses a race mechanism to handle shutdown:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
# Create tasks for the main agent run and for waiting on the stop signal
run_task = asyncio.create_task(agent_loop_instance.run(goal=goal, max_loops=max_loops))
stop_task = asyncio.create_task(stop_event.wait())

# Wait for either the agent run to complete OR the stop signal to be received
done, pending = await asyncio.wait(
    {run_task, stop_task},
    return_when=asyncio.FIRST_COMPLETED
)
\end{minted}
\end{pageablecode}
This approach ensures the agent responds promptly to shutdown signals without polling.

\subsection*{Driver and Entry Point Functionality}

\subsubsection*{Main Driver Function}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
async def run_agent_process(
    mcp_server_url: str,
    anthropic_key: str,
    goal: str,
    max_loops: int,
    state_file: str,
    config_file: Optional[str],
) -> None:
\end{minted}
\end{pageablecode}
This function manages the complete agent lifecycle:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Setup Phase}:
    \begin{itemize}
        \item Instantiates MCPClient with server URL and config
        \item Configures Anthropic API key
        \item Sets up MCP connections and interactions
        \item Creates AgentMasterLoop instance
        \item Registers signal handlers for clean termination
    \end{itemize}
    \item \textbf{Agent Initialization}:
    \begin{itemize}
        \item Calls agent.initialize() to load state and prepare tools
        \item Exits early if initialization fails
    \end{itemize}
    \item \textbf{Execution Phase}:
    \begin{itemize}
        \item Creates concurrent tasks for agent.run() and stop\_event.wait()
        \item Races them with asyncio.wait()
        \item Handles both normal completion and signal interruption
    \end{itemize}
    \item \textbf{Termination Phase}:
    \begin{itemize}
        \item Ensures agent shutdown method is called
        \item Closes MCP client connections
        \item Sets appropriate exit code based on outcome
        \item Cleans up resources before exiting
    \end{itemize}
\end{enumerate}

The function includes comprehensive error handling at each stage, with proper error propagation and logging.

\subsubsection*{Entry Point}
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
if __name__ == "__main__":
    # Load configuration from environment variables or defaults
    MCP_SERVER_URL = os.environ.get("MCP_SERVER_URL", "http://localhost:8013")
    ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
    AGENT_GOAL = os.environ.get(
        "AGENT_GOAL",
        "Create workflow 'Tier 3 Test': Research Quantum Computing impact on Cryptography.",
    )
    MAX_ITER = int(os.environ.get("MAX_ITERATIONS", "30"))
    STATE_FILE = os.environ.get("AGENT_STATE_FILE", AGENT_STATE_FILE)
    CONFIG_PATH = os.environ.get("MCP_CLIENT_CONFIG")

    # Validate essential configuration
    if not ANTHROPIC_API_KEY:
        print("‚ùå ERROR: ANTHROPIC_API_KEY missing in environment variables.")
        sys.exit(1)
    if not MCP_CLIENT_AVAILABLE:
        print("‚ùå ERROR: MCPClient dependency missing.")
        sys.exit(1)

    # Display configuration being used before starting
    print(f"--- {AGENT_NAME} ---")
    print(f"Memory System URL: {MCP_SERVER_URL}")
    print(f"Agent Goal: {AGENT_GOAL}")
    print(f"Max Iterations: {MAX_ITER}")
    print(f"State File: {STATE_FILE}")
    print(f"Client Config: {CONFIG_PATH or 'Default internal config'}")
    print(f"Log Level: {logging.getLevelName(log.level)}")
    print("Anthropic API Key: Found")
    print("-----------------------------------------")

    # Define and run the main async function
    async def _main() -> None:
        await run_agent_process(
            MCP_SERVER_URL,
            ANTHROPIC_API_KEY,
            AGENT_GOAL,
            MAX_ITER,
            STATE_FILE,
            CONFIG_PATH,
        )

    # Run with asyncio.run() and handle initialization interrupts
    try:
        asyncio.run(_main())
    except KeyboardInterrupt:
        print("\n[yellow]Initial KeyboardInterrupt detected. Exiting.[/yellow]")
        sys.exit(130)
\end{minted}
\end{pageablecode}
This entry point provides:
\begin{enumerate}[label=\arabic*.]
    \item Configuration via environment variables with sensible defaults
    \item Validation of critical requirements
    \item Transparent display of runtime configuration
    \item Clean asyncio execution pattern
    \item Initial interrupt handling before signal handlers are registered
\end{enumerate}

\subsection*{Integration Architecture and Workflow}

To fully understand how this agent operates in practice, let's examine the complete workflow:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Startup Sequence}:
    \begin{itemize}
        \item User calls script with goal (CLI or environment variable)
        \item System creates MCPClient connection to UMS
        \item AgentMasterLoop is initialized with configuration
        \item Prior state is loaded if available
        \item Signal handlers established for graceful termination
    \end{itemize}
    \item \textbf{Initial Workflow Creation}:
    \begin{itemize}
        \item If no active workflow, create one with the specified goal
        \item Create initial thought chain for reasoning
        \item Create root goal in goal stack
        \item Initialize plan with default first step
    \end{itemize}
    \item \textbf{Think-Act Cycle}:
    \begin{itemize}
        \item Run periodic cognitive tasks (reflection, consolidation, etc.)
        \item Gather comprehensive context (goals, memories, plans, errors)
        \item Call LLM for decision (with detailed prompt)
        \item Execute decided action (tool call, thought, or completion)
        \item Apply plan updates (explicit or heuristic)
        \item Save state for persistence
        \item Check termination conditions
    \end{itemize}
    \item \textbf{Goal Management Flow}:
    \begin{itemize}
        \item LLM can push new sub-goals (decomposing complex tasks)
        \item Focus shifts to sub-goal at top of stack
        \item When goal is marked complete/failed, it's popped from stack
        \item Focus returns to parent goal (or completes if root)
        \item Root goal completion signals the agent to finish
    \end{itemize}
    \item \textbf{Sub-Workflow Management}:
    \begin{itemize}
        \item Complex tasks may create sub-workflows
        \item Each with their own goal stack, thought chains, etc.
        \item Completion of sub-workflow returns to parent
        \item Links sub-workflow status to corresponding goal
    \end{itemize}
    \item \textbf{Error Recovery Path}:
    \begin{itemize}
        \item Tool errors are categorized and captured
        \item Error details fed to LLM with recovery strategies
        \item Plan updated to handle error condition
        \item Consecutive errors tracked with threshold limit
    \end{itemize}
    \item \textbf{Termination Sequence}:
    \begin{itemize}
        \item Goal achieved OR max loops reached OR error limit OR signal
        \item Cleanup background tasks
        \item Save final state
        \item Close connections
        \item Exit with appropriate code
    \end{itemize}
\end{enumerate}

\subsection*{Prompt Engineering as Cognitive Scaffolding}

The system's prompt construction approach represents a sophisticated cognitive scaffolding technique rather than simple context provision:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _construct_agent_prompt(self, goal: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
\end{minted}
\end{pageablecode}
The prompting approach functions as cognitive scaffolding rather than simple information provision. Beyond presenting factual context, the prompt:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Guides Analytical Process}: Provides a structured framework for problem analysis:
    \begin{verbatim}
   '1. Context Analysis: Deeply analyze 'Current Context'...'
   '2. Error Handling: If `las\1\_erro\1\_details` exists, **FIRST** reason about...'
    \end{verbatim}
    \item \textbf{Identifies Cognitive Integration Points}: Explicitly connects context elements:
    \begin{verbatim}
   'Note workflow status, errors (`las\1\_erro\1\_details` - *pay attention to error `type`*),
   **goal stack (`curren\1\_goa\1\_context` -> `goa\1\_stac\1\_summary`) and the `curren\1\_goal`**...'
    \end{verbatim}
    \item \textbf{Provides Recovery Frameworks}: Offers explicit recovery strategies:
    \begin{verbatim}
   'Recovery Strategies based on `las\1\_erro\1\_details.type`:'
   '*   `InvalidInputError`: Review tool schema, arguments, and context...'
    \end{verbatim}
    \item \textbf{Creates Decision Frameworks}: Structures the decision process:
    \begin{verbatim}
   '4. Action Decision: Choose **ONE** action based on the *first planned step* in your current plan:'
    \end{verbatim}
\end{enumerate}

This approach creates a 'cognitive partnership' with the LLM, using the prompt to guide reasoning rather than simply providing information. By teaching the LLM how to analyze, prioritize, and decide, the system creates more consistent and effective agent behavior.

\subsection*{Complete Integration Example}

Here's how you'd deploy this agent in a real-world scenario:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Setup Environment}:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{bash}
   export MCP_SERVER_URL="http://your-memory-server:8013"
   export ANTHROPIC_API_KEY="sk-ant-your-key-here"
   export AGENT_GOAL="Research and summarize recent developments in quantum computing and their potential impact on cryptography"
   export MAX_ITERATIONS=50
   export AGENT_LOOP_LOG_LEVEL=INFO
    \end{minted}
    \end{pageablecode}
    \item \textbf{Run Script}:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{bash}
   python agent_master_loop.py
    \end{minted}
    \end{pageablecode}
    \item \textbf{Monitor Progress}:
    \begin{itemize}
        \item Console logs show loop iterations, tool calls, errors
        \item State file updated regularly with persistence
        \item UMS records workflow, memories, actions, artifacts
    \end{itemize}
    \item \textbf{Integration with External Systems}:
    \begin{itemize}
        \item Agent can create artifacts via UMS tools
        \item Can incorporate external data sources via appropriate tools
        \item Can trigger downstream processes via workflow status changes
    \end{itemize}
    \item \textbf{Graceful Termination}:
    \begin{itemize}
        \item Press Ctrl+C to send SIGINT
        \item Agent completes current operation
        \item Saves state for later resumption
        \item Cleanly disconnects from services
    \end{itemize}
    \item \textbf{Resume from Previous State}:
    \begin{pageablecode}
    \begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{bash}
   # Same environment but potentially different goal
   export AGENT_GOAL="Continue previous research and focus on post-quantum cryptography standards"
   python agent_master_loop.py
    \end{minted}
    \end{pageablecode}
\end{enumerate}

\subsection*{Advanced Integration Capabilities}

This agent design supports sophisticated integration patterns:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Hierarchical Agent Collaboration}:
    \begin{itemize}
        \item Multiple agent instances can create sub-workflows for each other
        \item Parent agents can monitor and coordinate child agents
        \item Complex task decomposition across specialized agents
    \end{itemize}
    \item \textbf{Long-Running/Persistent Agents}:
    \begin{itemize}
        \item State persistence allows resuming after shutdown
        \item Goal stack preserves hierarchical task context
        \item Meta-cognitive processes consolidate knowledge over time
    \end{itemize}
    \item \textbf{Cognitive Framework Integration}:
    \begin{itemize}
        \item Memory levels model human-like episodic/semantic/procedural memory
        \item Working memory with focus mimics human attention
        \item Reflection/consolidation creates higher-level knowledge
    \end{itemize}
    \item \textbf{Adaptive Performance Tuning}:
    \begin{itemize}
        \item Mental momentum bias favors productive periods
        \item Threshold adaptation responds to memory balance
        \item Error rate monitoring triggers course corrections
    \end{itemize}
    \item \textbf{Process Monitoring and Observability}:
    \begin{itemize}
        \item Detailed logging of all operations
        \item State snapshots for debugging/analysis
        \item Tool usage statistics and performance metrics
    \end{itemize}
\end{enumerate}

This comprehensive architecture provides a solid foundation for reliable, sophisticated AI agents that can perform complex cognitive tasks with minimal supervision.

\subsection*{Unified Architectural Overview and Design Philosophy}

The EideticEngine Agent Master Loop represents a sophisticated cognitive architecture for autonomous AI agents, drawing inspiration from both human cognitive psychology and advanced AI systems design. This final section synthesizes our technical analysis while providing deeper context for the system's design philosophy and implementation choices.

\subsubsection*{Cognitive Science Foundations}

At its core, the EideticEngine employs a cognitive architecture inspired by human memory and reasoning processes:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Multi-Level Memory System}: The architecture implements three primary memory levels that mirror human cognition:
    \begin{itemize}
        \item \textbf{Episodic Memory}: Stores specific experiences and observations (akin to autobiographical memory)
        \item \textbf{Semantic Memory}: Contains generalized knowledge abstracted from episodes
        \item \textbf{Procedural Memory}: Encodes how-to knowledge and skills
    \end{itemize}
    \item \textbf{Working Memory and Attention}: The system maintains a limited working memory with a focal point, simulating human attention limitations and focus mechanisms. This is evident in the `CONTEX\1\_WORKIN\1\_MEMOR\1\_SHO\1\_LIMIT` parameter and the focal memory system.
    \item \textbf{Goal-Directed Cognition}: The goal stack implementation models how humans decompose complex tasks into manageable sub-goals, maintaining focus while preserving the broader context.
    \item \textbf{Mental Momentum}: The momentum bias system mirrors human cognitive preferences for staying on productive tracks rather than constantly re-evaluating when progress is steady.
    \item \textbf{Metacognition}: Reflection and consolidation processes simulate human introspection and knowledge organization capabilities.
\end{enumerate}

This cognitive foundation isn't merely metaphorical‚Äîit shapes the core data structures and algorithms throughout the system. The `AgentState` class acts as the agent's "mind," while the various memory tools and goal management functions serve as cognitive processes.

It's important to clarify the distinct purposes of the two hierarchical systems in the agent. The workflow stack manages execution contexts across potentially different environments or domains, where each sub-workflow represents a separate task context with its own memory space and thought chains. In contrast, the goal stack manages hierarchical decomposition of objectives within a single workflow context, where goals share the same memory space and thought chain. This dual-stack approach enables the agent to both decompose goals hierarchically and switch entire working contexts when needed.

\subsubsection*{LLM Integration and Prompting Strategy}

The system's interaction with the LLM (Claude from Anthropic) represents a particularly sophisticated approach to large language model prompting:
\begin{pageablecode}
\begin{minted}[fontsize=\scriptsize, breaklines=true, breakanywhere=true, breaksymbolleft={}, breaksymbolright={}, tabsize=2, autogobble]{python}
def _construct_agent_prompt(self, goal: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
\end{minted}
\end{pageablecode}
This method exemplifies advanced prompt engineering techniques:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Rich Contextual Grounding}: The prompt provides comprehensive context organized into semantically meaningful sections, helping the LLM understand the current state.
    \item \textbf{Process Guidance}: Rather than asking open-ended questions, the prompt outlines a specific analytical process:
    \begin{verbatim}
   "1. Context Analysis: Deeply analyze 'Current Context'..."
   "2. Error Handling: If `las\1\_erro\1\_details` exists, **FIRST** reason about..."
   "3. Reasoning & Planning:..."
    \end{verbatim}
    \item \textbf{Error Recovery Framework}: The system provides explicit recovery strategies based on error types, creating a structured approach to problem-solving:
    \begin{verbatim}
   "Recovery Strategies based on `las\1\_erro\1\_details.type`:"
   "*   `InvalidInputError`: Review tool schema, arguments, and context..."
    \end{verbatim}
    \item \textbf{Tool Rationalization}: The prompt highlights essential cognitive tools and provides their schemas, enabling informed tool selection.
    \item \textbf{Balance of Autonomy and Guidance}: The prompt provides structure without being prescriptive about specific decisions, maintaining the LLM's reasoning capability.
\end{enumerate}

This approach contrasts with simpler prompting strategies that either provide minimal context or overly constrain the model's reasoning. The AML creates a "cognitive partnership" with the LLM, using structured prompts to provide scaffolding for effective reasoning.

\subsubsection*{Architectural Integration and Information Flow}

When we synthesize the various components, an elegant information flow emerges:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Memory ‚Üí Context ‚Üí LLM ‚Üí Decision ‚Üí Action ‚Üí Memory} represents the primary cognitive loop
    \item \textbf{Goal Stack ‚Üî Workflow Stack}: Bidirectional flow between goal and workflow hierarchies maintains task coherence
    \item \textbf{Background Tasks ‚Üí Memory}: Asynchronous processes enrich the memory system without blocking the main loop
    \item \textbf{Meta-Cognition ‚Üí Feedback ‚Üí LLM}: Reflection and consolidation outputs feed back into future prompts
    \item \textbf{Error ‚Üí Categorization ‚Üí Recovery Strategy ‚Üí LLM ‚Üí Plan Update}: Structured error handling enables resilient execution
\end{enumerate}

These flows create multiple feedback loops that enable sophisticated adaptive behavior:
\begin{verbatim}
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Context Gathering ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ LLM Reasoning ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Tool Action ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñ≤                         ‚îÇ                        ‚îÇ
         ‚îÇ                         ‚îÇ                        ‚îÇ
         ‚îÇ                         ‚ñº                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Memory System  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Plan Updates ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Side Effects‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñ≤                                                  ‚îÇ
         ‚îÇ                                                  ‚îÇ
         ‚îÇ                                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Meta-Cognition  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Error System‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\end{verbatim}
This architecture balances synchronous and asynchronous processes, enabling the agent to maintain focus while still performing background cognitive maintenance.

\subsubsection*{Technical Implementation Excellence}

Several aspects of the implementation demonstrate exceptional software engineering practices:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Fault Tolerance and Resilience}:
    \begin{itemize}
        \item Atomic state persistence with `os.replace` and `os.fsync`
        \item Comprehensive error categorization and recovery
        \item Graceful degradation when services are unavailable
        \item Retry logic with exponential backoff and jitter
    \end{itemize}
    \item \textbf{Asynchronous Processing}:
    \begin{itemize}
        \item Background task management with semaphores
        \item Timeout handling to prevent stuck processes
        \item Efficient concurrency with asyncio primitives
        \item Thread-safe operations for shared state
    \end{itemize}
    \item \textbf{Resource Management}:
    \begin{itemize}
        \item Token usage optimization through estimation and compression
        \item Memory system maintenance to prevent unbounded growth
        \item Adaptive throttling of meta-cognitive processes
        \item Efficient context retrieval with fetch/show limits
    \end{itemize}
    \item \textbf{Extensibility and Modularity}:
    \begin{itemize}
        \item Clear separation of concerns across methods
        \item Consistent error handling patterns
        \item Pluggable tool architecture via MCPClient
        \item Environment variable configuration for deployment flexibility
    \end{itemize}
\end{enumerate}

These technical qualities enable the system to run reliably in production environments while maintaining adaptability.

\subsubsection*{The "Cognitive Engine" Metaphor}

The EideticEngine name refers to eidetic memory (exceptional recall ability), but the system functions more broadly as a cognitive engine with distinct functional components:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Memory System}: The UMS provides episodic, semantic, and procedural memory storage and retrieval
    \item \textbf{Attention System}: Working memory optimization and focal point management
    \item \textbf{Executive Function}: Goal stack and plan management
    \item \textbf{Metacognitive System}: Reflection and consolidation processes
    \item \textbf{Reasoning Engine}: LLM integration for decision-making
    \item \textbf{Action System}: Tool execution framework
    \item \textbf{Learning System}: Memory consolidation, promotion, and linking
    \item \textbf{Emotional System}: Mental momentum bias and adaptation thresholds
\end{enumerate}

This metaphor isn't merely aesthetic‚Äîit provides a unifying framework for understanding how the components interact. Just as human cognition emerges from the interaction of specialized brain systems, agent intelligence emerges from the interaction of these specialized cognitive components.

\subsubsection*{Practical Applications and Use Cases}

The EideticEngine architecture enables sophisticated applications beyond simple task automation:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Long-Running Research Agents}: The persistence system and goal stack enable extended research projects with complex sub-tasks.
    \item \textbf{Autonomous Knowledge Workers}: The memory system and metacognitive capabilities support knowledge acquisition, organization, and application.
    \item \textbf{Adaptive Personal Assistants}: The goal management system enables assistants that maintain context across multiple sessions and adapt to user patterns.
    \item \textbf{Exploratory Problem Solvers}: The plan-execution cycle with error recovery enables structured exploration of solution spaces.
    \item \textbf{Multi-Agent Systems}: The sub-workflow capability enables hierarchical collaboration between specialized agents.
\end{enumerate}

These applications leverage the system's distinctive ability to maintain context, learn from experience, and adapt its cognitive processes based on feedback.

\subsubsection*{Current Limitations and Future Directions}

Despite its sophistication, the system has several limitations that suggest future development directions:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{LLM Dependence}: The system relies heavily on LLM reasoning quality, inheriting potential biases and limitations.
    \item \textbf{Tool-Based Action Space}: Actions are limited to available tools, constraining the agent's capabilities.
    \item \textbf{Single-Agent Focus}: While sub-workflows exist, true multi-agent collaboration isn't fully supported.
    \item \textbf{Limited Self-Modification}: The agent can't modify its own code or cognitive architecture.
    \item \textbf{Static Prompt Strategy}: The LLM prompting approach is sophisticated but relatively static.
\end{enumerate}

Future versions might address these limitations through:
\begin{itemize}
    \item More dynamic prompt engineering based on context
    \item Enhanced multi-agent coordination protocols
    \item Greater architectural self-modification capabilities
    \item Improved hybridization with other AI techniques beyond LLMs
\end{itemize}

\subsubsection*{Conclusion: The Agent as a Cognitive System}

When we integrate all aspects of our analysis, the EideticEngine Agent Master Loop emerges not just as a technical implementation but as a comprehensive cognitive system. It embodies principles from cognitive science, AI theory, and software engineering to create an agent architecture capable of:

\begin{enumerate}[label=\arabic*.]
    \item \textbf{Maintaining Extended Context}: Through its memory systems and state persistence
    \item \textbf{Learning from Experience}: Via metacognitive feedback loops and memory consolidation
    \item \textbf{Adaptive Problem Solving}: Through goal decomposition and flexible planning
    \item \textbf{Resilient Execution}: Via sophisticated error handling and recovery
    \item \textbf{Self-Reflection}: Through periodic meta-cognitive processes
\end{enumerate}

This cognitive systems approach represents a significant advancement over simpler agent architectures that lack memory, meta-cognition, or goal hierarchies. While still fundamentally powered by an LLM, the EideticEngine creates an execution context that dramatically enhances the LLM's capabilities, enabling more reliable, contextual, and goal-directed behavior.

The system demonstrates how architectural design can complement foundational model capabilities, creating an integrated system greater than the sum of its parts‚Äîa true cognitive engine rather than simply an interface to an LLM.

% --- END OF AML TECHNICAL ANALYSIS ---


\section{Appendix C: Unified Memory System (UMS) Code Listing}


\captionof{listing}{Complete Code for Unified Memory System (cognitive\_and\_agent\_memory.py)}
\label{lst:ums-code}

\begin{pageablecode}
\begin{minted}[
  escapeinside=||,
  fontsize=\scriptsize,
  breaklines=true,
  breakanywhere=true,
  breaksymbolleft={},
  breaksymbolright={},
  tabsize=2,
  autogobble
]{python}
"""Unified Agent Memory and Cognitive System.

This module provides a comprehensive memory, reasoning, and workflow tracking system
designed for LLM agents, merging sophisticated cognitive modeling with structured
process tracking.

Based on the integration plan combining 'cognitive_memory.py' and 'agent_memory.py'.

Key Features:
- Multi-level memory hierarchy (working, episodic, semantic, procedural) with rich metadata.
- Structured workflow, action, artifact, and thought chain tracking.
- Associative memory graph with automatic linking capabilities.
- Vector embeddings for semantic similarity and clustering.
- Foundational tools for recording agent activity and knowledge.
- Integrated episodic memory creation linked to actions and artifacts.
- Basic cognitive state saving (structure defined, loading/saving tools ported).
- SQLite backend using aiosqlite with performance optimizations.
"""

import asyncio
import contextlib
import json
import os
import re
import time
import uuid
from collections import defaultdict
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, AsyncIterator, Dict, List, Optional, Tuple

import aiosqlite
import markdown
import numpy as np
from pygments.formatters import HtmlFormatter
from sklearn.metrics.pairwise import cosine_similarity as sk_cosine_similarity

from ultimate_mcp_server.constants import (
    Provider as LLMGatewayProvider,  # To use provider constants
)
from ultimate_mcp_server.core.providers.base import (
    get_provider,  # For consolidation/reflection LLM calls
)

# Import error handling and decorators from agent_memory concepts
from ultimate_mcp_server.exceptions import ToolError, ToolInputError
from ultimate_mcp_server.services.vector.embeddings import get_embedding_service
from ultimate_mcp_server.tools.base import with_error_handling, with_tool_metrics
from ultimate_mcp_server.utils import get_logger

logger = get_logger("ultimate.tools.unified_memory")

# ======================================================
# Configuration Settings
# ======================================================

DEFAULT_DB_PATH = os.environ.get("AGENT_MEMORY_DB_PATH", "unified_agent_memory.db")
MAX_TEXT_LENGTH = 64000  # Maximum length for text fields (from agent_memory)
CONNECTION_TIMEOUT = 10.0  # seconds (from cognitive_memory)
ISOLATION_LEVEL = None  # autocommit mode (from cognitive_memory)

# Memory management parameters (from cognitive_memory)
MAX_WORKING_MEMORY_SIZE = int(os.environ.get("MAX_WORKING_MEMORY_SIZE", "20"))
DEFAULT_TTL = {
    "working": 60 * 30,       # 30 minutes
    "episodic": 60 * 60 * 24 * 7, # 7 days (Increased default)
    "semantic": 60 * 60 * 24 * 30, # 30 days
    "procedural": 60 * 60 * 24 * 90 # 90 days
}
MEMORY_DECAY_RATE = float(os.environ.get("MEMORY_DECAY_RATE", "0.01"))  # Per hour
IMPORTANCE_BOOST_FACTOR = float(os.environ.get("IMPORTANCE_BOOST_FACTOR", "1.5"))

# Embedding model configuration (from cognitive_memory)
DEFAULT_EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSION = 1536  # For the default model
SIMILARITY_THRESHOLD = 0.75

# SQLite optimization pragmas (from cognitive_memory)
SQLITE_PRAGMAS = [
    "PRAGMA journal_mode=WAL",
    "PRAGMA synchronous=NORMAL",
    "PRAGMA foreign_keys=ON",
    "PRAGMA temp_store=MEMORY",
    "PRAGMA cache_size=-32000",
    "PRAGMA mmap_size=2147483647",
    "PRAGMA busy_timeout=30000"
]

MAX_SEMANTIC_CANDIDATES = int(os.environ.get("MAX_SEMANTIC_CANDIDATES", "500")) # Hard cap for semantic search candidates

# ======================================================
# Enums (Combined & Standardized)
# ======================================================

# --- Workflow & Action Status ---
class WorkflowStatus(str, Enum):
    ACTIVE = "active"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    ABANDONED = "abandoned"

class ActionStatus(str, Enum):
    PLANNED = "planned"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"

# --- Content Types ---
class ActionType(str, Enum):
    TOOL_USE = "tool_use"
    REASONING = "reasoning"
    PLANNING = "planning"
    RESEARCH = "research"
    ANALYSIS = "analysis"
    DECISION = "decision"
    OBSERVATION = "observation"
    REFLECTION = "reflection"
    SUMMARY = "summary"
    CONSOLIDATION = "consolidation"
    MEMORY_OPERATION = "memory_operation" 

class ArtifactType(str, Enum):
    FILE = "file"
    TEXT = "text"
    IMAGE = "image"
    TABLE = "table"
    CHART = "chart"
    CODE = "code"
    DATA = "data"
    JSON = "json" 
    URL = "url"   

class ThoughtType(str, Enum):
    GOAL = "goal"
    QUESTION = "question"
    HYPOTHESIS = "hypothesis"
    INFERENCE = "inference"
    EVIDENCE = "evidence"
    CONSTRAINT = "constraint"
    PLAN = "plan"
    DECISION = "decision"
    REFLECTION = "reflection"
    CRITIQUE = "critique"
    SUMMARY = "summary"

# --- Memory System Types ---
class MemoryLevel(str, Enum):
    WORKING = "working"
    EPISODIC = "episodic"
    SEMANTIC = "semantic"
    PROCEDURAL = "procedural"

class MemoryType(str, Enum):
    """Content type classifications for memories. Combines concepts."""
    OBSERVATION = "observation"    # Raw data or sensory input (like text)
    ACTION_LOG = "action_log"      # Record of an agent action
    TOOL_OUTPUT = "tool_output"    # Result from a tool
    ARTIFACT_CREATION = "artifact_creation" # Record of artifact generation
    REASONING_STEP = "reasoning_step" # Corresponds to a thought
    FACT = "fact"                  # Verifiable piece of information
    INSIGHT = "insight"            # Derived understanding or pattern
    PLAN = "plan"                  # Future intention or strategy
    QUESTION = "question"          # Posed question or uncertainty
    SUMMARY = "summary"            # Condensed information
    REFLECTION = "reflection"      # Meta-cognitive analysis (distinct from thought type)
    SKILL = "skill"                # Learned capability (like procedural)
    PROCEDURE = "procedure"        # Step-by-step method
    PATTERN = "pattern"            # Recognized recurring structure
    CODE = "code"                  # Code snippet
    JSON = "json"                  # Structured JSON data
    URL = "url"                    # A web URL
    TEXT = "text"                  # Generic text block (fallback)
    # Retain IMAGE? Needs blob storage/linking capability. Deferred.

class LinkType(str, Enum):
    """Types of associations between memories (from cognitive_memory)."""
    RELATED = "related"
    CAUSAL = "causal"
    SEQUENTIAL = "sequential"
    HIERARCHICAL = "hierarchical"
    CONTRADICTS = "contradicts"
    SUPPORTS = "supports"
    GENERALIZES = "generalizes"
    SPECIALIZES = "specializes"
    FOLLOWS = "follows"
    PRECEDES = "precedes"
    TASK = "task"
    REFERENCES = "references" # Added for linking thoughts/actions to memories


# ======================================================
# Database Schema
# ======================================================

# Note: Using TEXT for IDs (UUIDs) and INTEGER for datetimes (Unix timestamp seconds)

SCHEMA_SQL = """
-- Base Pragmas (Combined)
PRAGMA foreign_keys = ON;
PRAGMA journal_mode=WAL;
PRAGMA synchronous=NORMAL;
PRAGMA temp_store=MEMORY;
PRAGMA cache_size=-32000;
PRAGMA mmap_size=2147483647;
PRAGMA busy_timeout=30000;

-- Workflows table ---
CREATE TABLE IF NOT EXISTS workflows (
    workflow_id TEXT PRIMARY KEY,
    title TEXT NOT NULL,
    description TEXT,
    goal TEXT,
    status TEXT NOT NULL,
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    completed_at INTEGER,
    parent_workflow_id TEXT,
    metadata TEXT,
    last_active INTEGER
);

-- Actions table ---
CREATE TABLE IF NOT EXISTS actions (
    action_id TEXT PRIMARY KEY,
    workflow_id TEXT NOT NULL,
    parent_action_id TEXT,
    action_type TEXT NOT NULL,        -- Uses ActionType enum
    title TEXT,
    reasoning TEXT,
    tool_name TEXT,
    tool_args TEXT,                   -- JSON serialized
    tool_result TEXT,                 -- JSON serialized
    status TEXT NOT NULL,             -- Uses ActionStatus enum
    started_at INTEGER NOT NULL,
    completed_at INTEGER,
    sequence_number INTEGER,
    FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
    FOREIGN KEY (parent_action_id) REFERENCES actions(action_id) ON DELETE SET NULL
);

-- Artifacts table ---
CREATE TABLE IF NOT EXISTS artifacts (
    artifact_id TEXT PRIMARY KEY,
    workflow_id TEXT NOT NULL,
    action_id TEXT,                   -- Action that created this
    artifact_type TEXT NOT NULL,      -- Uses ArtifactType enum
    name TEXT NOT NULL,
    description TEXT,
    path TEXT,                        -- Filesystem path
    content TEXT,                     -- For text-based artifacts
    metadata TEXT,                    -- JSON serialized
    created_at INTEGER NOT NULL,
    is_output BOOLEAN DEFAULT FALSE,
    FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
    FOREIGN KEY (action_id) REFERENCES actions(action_id) ON DELETE SET NULL
);

-- Thought chains table (From agent_memory)
CREATE TABLE IF NOT EXISTS thought_chains (
    thought_chain_id TEXT PRIMARY KEY,
    workflow_id TEXT NOT NULL,
    action_id TEXT,                   -- Optional action context
    title TEXT NOT NULL,
    created_at INTEGER NOT NULL,
    FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
    FOREIGN KEY (action_id) REFERENCES actions(action_id) ON DELETE SET NULL
);

-- Embeddings table (Create before memories which references it) ---
CREATE TABLE IF NOT EXISTS embeddings (
    id TEXT PRIMARY KEY,               -- Embedding hash ID
    memory_id TEXT UNIQUE,             -- Link back to the memory
    model TEXT NOT NULL,               -- Embedding model used
    embedding BLOB NOT NULL,           -- Serialized vector
    dimension INTEGER NOT NULL,        --  Dimension of the embedding vector
    created_at INTEGER NOT NULL
    -- Cannot add FK to memories yet, as it doesn't exist. Will be added via memories FK.
);

-- Memories table (Create before thoughts which references it) ---
CREATE TABLE IF NOT EXISTS memories (
    memory_id TEXT PRIMARY KEY,        -- Renamed from 'id' for clarity
    workflow_id TEXT NOT NULL,
    content TEXT NOT NULL,             -- The core memory content
    memory_level TEXT NOT NULL,        -- Uses MemoryLevel enum
    memory_type TEXT NOT NULL,         -- Uses MemoryType enum
    importance REAL DEFAULT 5.0,       -- Relevance score (1.0-10.0)
    confidence REAL DEFAULT 1.0,       -- Confidence score (0.0-1.0)
    description TEXT,                  -- Optional short description
    reasoning TEXT,                    -- Optional reasoning for the memory
    source TEXT,                       -- Origin (tool name, file, user, etc.)
    context TEXT,                      -- JSON context of memory creation
    tags TEXT,                         -- JSON array of tags
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    last_accessed INTEGER,
    access_count INTEGER DEFAULT 0,
    ttl INTEGER DEFAULT 0,             -- TTL in seconds (0 = permanent)
    embedding_id TEXT,                 -- FK to embeddings table
    action_id TEXT,                    -- *** FK: Action associated with this memory ***
    thought_id TEXT,                   -- *** FK: Thought associated with this memory - REMOVED INLINE, ADDED VIA ALTER LATER ***
    artifact_id TEXT,                  -- *** FK: Artifact associated with this memory ***
    FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
    FOREIGN KEY (embedding_id) REFERENCES embeddings(id) ON DELETE SET NULL,
    FOREIGN KEY (action_id) REFERENCES actions(action_id) ON DELETE SET NULL,
    FOREIGN KEY (artifact_id) REFERENCES artifacts(artifact_id) ON DELETE SET NULL
    -- REMOVED INLINE FK: FOREIGN KEY (thought_id) REFERENCES thoughts(thought_id) ON DELETE SET NULL DEFERRABLE INITIALLY DEFERRED;
);

-- Add back reference from embeddings to memories now that memories exists
ALTER TABLE embeddings ADD CONSTRAINT fk_embeddings_memory FOREIGN KEY (memory_id) REFERENCES memories(memory_id) ON DELETE CASCADE;


-- Thoughts table (Create after memories)
CREATE TABLE IF NOT EXISTS thoughts (
    thought_id TEXT PRIMARY KEY,
    thought_chain_id TEXT NOT NULL,
    parent_thought_id TEXT,
    thought_type TEXT NOT NULL,        -- Uses ThoughtType enum
    content TEXT NOT NULL,
    sequence_number INTEGER NOT NULL,
    created_at INTEGER NOT NULL,
    relevant_action_id TEXT,           -- Action this thought relates to/caused
    relevant_artifact_id TEXT,         -- Artifact this thought relates to
    relevant_memory_id TEXT,           -- *** FK: Memory entry this thought relates to - REMOVED INLINE, ADDED VIA ALTER LATER ***
    FOREIGN KEY (thought_chain_id) REFERENCES thought_chains(thought_chain_id) ON DELETE CASCADE,
    FOREIGN KEY (parent_thought_id) REFERENCES thoughts(thought_id) ON DELETE SET NULL,
    FOREIGN KEY (relevant_action_id) REFERENCES actions(action_id) ON DELETE SET NULL,
    FOREIGN KEY (relevant_artifact_id) REFERENCES artifacts(artifact_id) ON DELETE SET NULL
    -- REMOVED INLINE FK: FOREIGN KEY (relevant_memory_id) REFERENCES memories(memory_id) ON DELETE SET NULL DEFERRABLE INITIALLY DEFERRED;
);


-- Memory links table ---
CREATE TABLE IF NOT EXISTS memory_links (
    link_id TEXT PRIMARY KEY,
    source_memory_id TEXT NOT NULL,
    target_memory_id TEXT NOT NULL,
    link_type TEXT NOT NULL,          -- Uses LinkType enum
    strength REAL DEFAULT 1.0,
    description TEXT,
    created_at INTEGER NOT NULL,
    FOREIGN KEY (source_memory_id) REFERENCES memories(memory_id) ON DELETE CASCADE,
    FOREIGN KEY (target_memory_id) REFERENCES memories(memory_id) ON DELETE CASCADE,
    UNIQUE(source_memory_id, target_memory_id, link_type)
);

-- Cognitive states table (will store memory_ids)
CREATE TABLE IF NOT EXISTS cognitive_states (
    state_id TEXT PRIMARY KEY,
    workflow_id TEXT NOT NULL,
    title TEXT NOT NULL,
    working_memory TEXT,               -- JSON array of memory_ids in active working memory
    focus_areas TEXT,                  -- JSON array of memory_ids or descriptive strings
    context_actions TEXT,              -- JSON array of relevant action_ids
    current_goals TEXT,                -- JSON array of goal descriptions or thought_ids
    created_at INTEGER NOT NULL,
    is_latest BOOLEAN NOT NULL,
    FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE
);

-- Reflections table (for meta-cognitive analysis)
CREATE TABLE IF NOT EXISTS reflections (
    reflection_id TEXT PRIMARY KEY,
    workflow_id TEXT NOT NULL,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    reflection_type TEXT NOT NULL,     -- summary, insight, planning, etc.
    created_at INTEGER NOT NULL,
    referenced_memories TEXT,          -- JSON array of memory_ids
    FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE
);

-- Memory operations log (for auditing/debugging)
CREATE TABLE IF NOT EXISTS memory_operations (
    operation_log_id TEXT PRIMARY KEY,
    workflow_id TEXT NOT NULL,
    memory_id TEXT,                    -- Related memory, if applicable
    action_id TEXT,                    -- Related action, if applicable
    operation TEXT NOT NULL,           -- create, update, access, link, consolidate, expire, reflect, etc.
    operation_data TEXT,               -- JSON of operation details
    timestamp INTEGER NOT NULL,        -- Unix timestamp
    FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
    FOREIGN KEY (memory_id) REFERENCES memories(memory_id) ON DELETE SET NULL,
    FOREIGN KEY (action_id) REFERENCES actions(action_id) ON DELETE SET NULL
);

-- Tags table ---
CREATE TABLE IF NOT EXISTS tags (
    tag_id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,
    description TEXT,
    category TEXT,
    created_at INTEGER NOT NULL
);

-- Junction Tables for Tags ---
CREATE TABLE IF NOT EXISTS workflow_tags (
    workflow_id TEXT NOT NULL,
    tag_id INTEGER NOT NULL,
    PRIMARY KEY (workflow_id, tag_id),
    FOREIGN KEY (workflow_id) REFERENCES workflows(workflow_id) ON DELETE CASCADE,
    FOREIGN KEY (tag_id) REFERENCES tags(tag_id) ON DELETE CASCADE
);

CREATE TABLE IF NOT EXISTS action_tags (
    action_id TEXT NOT NULL,
    tag_id INTEGER NOT NULL,
    PRIMARY KEY (action_id, tag_id),
    FOREIGN KEY (action_id) REFERENCES actions(action_id) ON DELETE CASCADE,
    FOREIGN KEY (tag_id) REFERENCES tags(tag_id) ON DELETE CASCADE
);

CREATE TABLE IF NOT EXISTS artifact_tags (
    artifact_id TEXT NOT NULL,
    tag_id INTEGER NOT NULL,
    PRIMARY KEY (artifact_id, tag_id),
    FOREIGN KEY (artifact_id) REFERENCES artifacts(artifact_id) ON DELETE CASCADE,
    FOREIGN KEY (tag_id) REFERENCES tags(tag_id) ON DELETE CASCADE
);

-- Dependencies table ---
CREATE TABLE IF NOT EXISTS dependencies (
    dependency_id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_action_id TEXT NOT NULL,    -- The action that depends on the target
    target_action_id TEXT NOT NULL,    -- The action that is depended upon
    dependency_type TEXT NOT NULL,     -- Type of dependency (e.g., 'requires', 'informs')
    created_at INTEGER NOT NULL,       -- When the dependency was created
    FOREIGN KEY (source_action_id) REFERENCES actions (action_id) ON DELETE CASCADE,
    FOREIGN KEY (target_action_id) REFERENCES actions (action_id) ON DELETE CASCADE,
    UNIQUE(source_action_id, target_action_id, dependency_type)
);


-- Create Indices ---
-- Workflow indices
CREATE INDEX IF NOT EXISTS idx_workflows_status ON workflows(status);
CREATE INDEX IF NOT EXISTS idx_workflows_parent ON workflows(parent_workflow_id);
CREATE INDEX IF NOT EXISTS idx_workflows_last_active ON workflows(last_active DESC);
-- Action indices
CREATE INDEX IF NOT EXISTS idx_actions_workflow_id ON actions(workflow_id);
CREATE INDEX IF NOT EXISTS idx_actions_parent ON actions(parent_action_id);
CREATE INDEX IF NOT EXISTS idx_actions_sequence ON actions(workflow_id, sequence_number);
CREATE INDEX IF NOT EXISTS idx_actions_type ON actions(action_type);
-- Artifact indices
CREATE INDEX IF NOT EXISTS idx_artifacts_workflow_id ON artifacts(workflow_id);
CREATE INDEX IF NOT EXISTS idx_artifacts_action_id ON artifacts(action_id);
CREATE INDEX IF NOT EXISTS idx_artifacts_type ON artifacts(artifact_type);
-- Thought indices
CREATE INDEX IF NOT EXISTS idx_thought_chains_workflow ON thought_chains(workflow_id);
CREATE INDEX IF NOT EXISTS idx_thoughts_chain ON thoughts(thought_chain_id);
CREATE INDEX IF NOT EXISTS idx_thoughts_sequence ON thoughts(thought_chain_id, sequence_number);
CREATE INDEX IF NOT EXISTS idx_thoughts_type ON thoughts(thought_type);
CREATE INDEX IF NOT EXISTS idx_thoughts_relevant_memory ON thoughts(relevant_memory_id); -- Index still useful
-- Memory indices
CREATE INDEX IF NOT EXISTS idx_memories_workflow ON memories(workflow_id);
CREATE INDEX IF NOT EXISTS idx_memories_level ON memories(memory_level);
CREATE INDEX IF NOT EXISTS idx_memories_type ON memories(memory_type);
CREATE INDEX IF NOT EXISTS idx_memories_importance ON memories(importance DESC);
CREATE INDEX IF NOT EXISTS idx_memories_confidence ON memories(confidence DESC);
CREATE INDEX IF NOT EXISTS idx_memories_created ON memories(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_memories_accessed ON memories(last_accessed DESC);
CREATE INDEX IF NOT EXISTS idx_memories_embedding ON memories(embedding_id);
CREATE INDEX IF NOT EXISTS idx_memories_action_id ON memories(action_id);
CREATE INDEX IF NOT EXISTS idx_memories_thought_id ON memories(thought_id); -- Index still useful
CREATE INDEX IF NOT EXISTS idx_memories_artifact_id ON memories(artifact_id);
-- Link indices
CREATE INDEX IF NOT EXISTS idx_memory_links_source ON memory_links(source_memory_id);
CREATE INDEX IF NOT EXISTS idx_memory_links_target ON memory_links(target_memory_id);
CREATE INDEX IF NOT EXISTS idx_memory_links_type ON memory_links(link_type);
-- Embedding indices
CREATE INDEX IF NOT EXISTS idx_embeddings_memory_id ON embeddings(memory_id); -- Index the FK
CREATE INDEX IF NOT EXISTS idx_embeddings_dimension ON embeddings(dimension); -- Index for dimension filtering
-- Cognitive State indices
CREATE INDEX IF NOT EXISTS idx_cognitive_states_workflow ON cognitive_states(workflow_id);
CREATE INDEX IF NOT EXISTS idx_cognitive_states_latest ON cognitive_states(workflow_id, is_latest);
-- Reflection indices
CREATE INDEX IF NOT EXISTS idx_reflections_workflow ON reflections(workflow_id);
-- Operation Log indices
CREATE INDEX IF NOT EXISTS idx_operations_workflow ON memory_operations(workflow_id);
CREATE INDEX IF NOT EXISTS idx_operations_memory ON memory_operations(memory_id);
CREATE INDEX IF NOT EXISTS idx_operations_timestamp ON memory_operations(timestamp DESC);
-- Tag indices
CREATE INDEX IF NOT EXISTS idx_tags_name ON tags(name);
CREATE INDEX IF NOT EXISTS idx_workflow_tags ON workflow_tags(tag_id); -- Index tag_id for lookups
CREATE INDEX IF NOT EXISTS idx_action_tags ON action_tags(tag_id);
CREATE INDEX IF NOT EXISTS idx_artifact_tags ON artifact_tags(tag_id);
-- Dependency indices
CREATE INDEX IF NOT EXISTS idx_dependencies_source ON dependencies(source_action_id);
CREATE INDEX IF NOT EXISTS idx_dependencies_target ON dependencies(target_action_id);

-- Virtual table for memories ---
CREATE VIRTUAL TABLE IF NOT EXISTS memory_fts USING fts5(
    content, description, reasoning, tags,
    workflow_id UNINDEXED,
    memory_id UNINDEXED,
    content='memories',
    content_rowid='rowid',
    tokenize='porter unicode61'
);

-- Triggers to keep virtual table in sync (Updated for new table/columns)
CREATE TRIGGER IF NOT EXISTS memories_after_insert AFTER INSERT ON memories BEGIN
    INSERT INTO memory_fts(rowid, content, description, reasoning, tags, workflow_id, memory_id)
    VALUES (new.rowid, new.content, new.description, new.reasoning, new.tags, new.workflow_id, new.memory_id);
END;
CREATE TRIGGER IF NOT EXISTS memories_after_delete AFTER DELETE ON memories BEGIN
    INSERT INTO memory_fts(memory_fts, rowid, content, description, reasoning, tags, workflow_id, memory_id)
    VALUES ('delete', old.rowid, old.content, old.description, old.reasoning, old.tags, old.workflow_id, old.memory_id);
END;
CREATE TRIGGER IF NOT EXISTS memories_after_update AFTER UPDATE ON memories BEGIN
    INSERT INTO memory_fts(memory_fts, rowid, content, description, reasoning, tags, workflow_id, memory_id)
    VALUES ('delete', old.rowid, old.content, old.description, old.reasoning, old.tags, old.workflow_id, old.memory_id);
    INSERT INTO memory_fts(rowid, content, description, reasoning, tags, workflow_id, memory_id)
    VALUES (new.rowid, new.content, new.description, new.reasoning, new.tags, new.workflow_id, new.memory_id);
END;

-- Deferrable Circular Foreign Key Constraints for thoughts <-> memories
-- Execute after tables are created and within an explicit transaction
BEGIN IMMEDIATE TRANSACTION; -- Use IMMEDIATE for exclusive lock during schema change
PRAGMA defer_foreign_keys = ON; -- Enable deferral specifically for this transaction

ALTER TABLE thoughts ADD CONSTRAINT fk_thoughts_memory
    FOREIGN KEY (relevant_memory_id) REFERENCES memories(memory_id)
    ON DELETE SET NULL DEFERRABLE INITIALLY DEFERRED;

ALTER TABLE memories ADD CONSTRAINT fk_memories_thought
    FOREIGN KEY (thought_id) REFERENCES thoughts(thought_id)
    ON DELETE SET NULL DEFERRABLE INITIALLY DEFERRED;

COMMIT; -- Commit the transaction containing the PRAGMA and ALTERs
"""

# ======================================================
# Database Connection Management (Adapted from agent_memory)
# ======================================================

class DBConnection:
    """Context manager for database connections using aiosqlite."""

    _instance: Optional[aiosqlite.Connection] = None # Added type hint for clarity
    _lock = asyncio.Lock()
    _db_path_used: Optional[str] = None
    _init_lock_timeout = 15.0 # Configurable timeout in seconds

    def __init__(self, db_path: str = DEFAULT_DB_PATH):
        self.db_path = db_path
        self.conn: Optional[aiosqlite.Connection] = None
        # Ensure directory exists synchronously during init
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)

    async def _initialize_instance(self) -> aiosqlite.Connection:
        """Handles the actual creation and setup of the database connection."""
        logger.info(f"Connecting to database: {self.db_path}", emoji_key="database")
        conn = await aiosqlite.connect(
            self.db_path,
            timeout=CONNECTION_TIMEOUT # Use timeout from cognitive_memory
            # isolation_level=ISOLATION_LEVEL # aiosqlite handles transactions differently
        )
        conn.row_factory = aiosqlite.Row

        # Apply optimizations
        for pragma in SQLITE_PRAGMAS:
            await conn.execute(pragma)

        # Enable custom functions needed by cognitive_memory parts
        await conn.create_function("json_contains", 2, _json_contains, deterministic=True)
        await conn.create_function("json_contains_any", 2, _json_contains_any, deterministic=True)
        await conn.create_function("json_contains_all", 2, _json_contains_all, deterministic=True)
        await conn.create_function("compute_memory_relevance", 5, _compute_memory_relevance, deterministic=True)

        # Initialize schema if needed
        # Check if tables exist before running the full script
        cursor = await conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='workflows'")
        table_exists = await cursor.fetchone()
        await cursor.close() # Explicitly close cursor after fetch
        if not table_exists:
            logger.info("Database schema not found. Initializing...", emoji_key="gear")
            # Ensure foreign keys are enabled before schema execution for ALTER TABLE
            await conn.execute("PRAGMA foreign_keys = ON;")
            # Use executescript for potentially multi-statement SCHEMA_SQL
            # NOTE: executescript implicitly commits before and after execution
            # This is generally okay for schema setup but problematic for transactions.
            # The explicit transaction for DEFERRABLE FKs in SCHEMA_SQL is handled separately.
            await conn.executescript(SCHEMA_SQL)
            # No explicit commit needed after executescript typically
            logger.success("Database schema initialized successfully.", emoji_key="white_check_mark")
        else:
            # Optionally, add schema migration logic here in the future
            logger.info("Database schema already exists.", emoji_key="database")
            # Ensure foreign keys are on for existing connections
            await conn.execute("PRAGMA foreign_keys = ON;")

        # Set the path used for this instance *after* successful connection
        DBConnection._db_path_used = self.db_path
        return conn

    async def __aenter__(self) -> aiosqlite.Connection:
        """Acquires the singleton database connection instance."""
        # 1. Quick check without lock
        instance = DBConnection._instance
        if instance is not None:
            # Path consistency check for singleton reuse
            if self.db_path != DBConnection._db_path_used:
                logger.error(f"DBConnection singleton mismatch: Already initialized with path '{DBConnection._db_path_used}', but requested '{self.db_path}'.")
                raise RuntimeError(f"DBConnection singleton initialized with path '{DBConnection._db_path_used}', requested '{self.db_path}'")
            # Ensure foreign keys are enabled for this specific use of the connection
            # Doing this on every enter ensures it's set for the current operation context
            await instance.execute("PRAGMA foreign_keys = ON;")
            return instance

        # 2. Acquire lock with timeout only if instance might need initialization
        try:
            # Use asyncio.timeout for the lock acquisition itself
            async with asyncio.timeout(DBConnection._init_lock_timeout):
                async with DBConnection._lock:
                    # 3. Double-check instance after acquiring lock
                    if DBConnection._instance is None:
                        # Call the separate initialization method
                        DBConnection._instance = await self._initialize_instance()
                    # Re-check path consistency inside lock to handle race condition if multiple threads tried init
                    elif self.db_path != DBConnection._db_path_used:
                         logger.error(f"DBConnection singleton mismatch detected inside lock: Already initialized with path '{DBConnection._db_path_used}', but requested '{self.db_path}'.")
                         raise RuntimeError(f"DBConnection singleton initialized with path '{DBConnection._db_path_used}', requested '{self.db_path}'")

        except asyncio.TimeoutError:
             # Log timeout error and raise a ToolError
             logger.error(f"Timeout acquiring DB initialization lock after {DBConnection._init_lock_timeout}s. Possible deadlock or hang.", emoji_key="alarm_clock")
             raise ToolError("Database initialization timed out.") from None
        except Exception as init_err:
             # Catch potential errors during _initialize_instance
             logger.error(f"Error during database initialization: {init_err}", exc_info=True, emoji_key="x")
             # Ensure instance is None if initialization failed
             DBConnection._instance = None
             DBConnection._db_path_used = None
             raise ToolError(f"Database initialization failed: {init_err}") from init_err

        # Ensure FKs enabled for the first use after initialization and return the instance
        # Note: _initialize_instance also sets PRAGMA foreign_keys=ON, but setting it again here
        # ensures it's applied for the context manager's immediate use.
        await DBConnection._instance.execute("PRAGMA foreign_keys = ON;")
        return DBConnection._instance

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Releases the connection context (but doesn't close singleton).

        Propagates exceptions that occurred within the context.
        """
        # Note: This context manager manages access, not the lifecycle of the singleton connection.
        # Closing is handled by the explicit close_connection method.
        # The transaction manager context handles commit/rollback.
        if exc_type is not None:
             # Log the error that occurred *within* the 'async with DBConnection(...)' block
             logger.error(f"Database error occurred within DBConnection context: {exc_val}", exc_info=(exc_type, exc_val, exc_tb))
             # Re-raise the exception to notify the caller
             raise exc_val
        pass # If no exception, just pass (commit/rollback handled by transaction manager)

    @classmethod
    async def close_connection(cls):
        """Closes the singleton database connection if it exists.

        This method should be called explicitly by the application during shutdown
        to ensure resources are released cleanly.
        """
        if cls._instance:
            async with cls._lock: # Ensure exclusive access for closing
                if cls._instance: # Double check after lock
                    logger.info("Attempting to close database connection.", emoji_key="lock")
                    try:
                        await cls._instance.close()
                        logger.success("Database connection closed successfully.", emoji_key="white_check_mark")
                    except Exception as e:
                        logger.error(f"Error closing database connection: {e}", exc_info=True)
                    finally:
                        # Ensure the instance reference is cleared even if close fails
                        cls._instance = None
                        cls._db_path_used = None
                else:
                     logger.info("Database connection was closed by another task while waiting for lock.")
        else:
            logger.info("No active database connection instance to close.")

    # --- NEW TRANSACTION MANAGER ---
    @contextlib.asynccontextmanager
    async def transaction(self) -> AsyncIterator[aiosqlite.Connection]:
        """Provides an atomic transaction block using the singleton connection."""
        conn = await self.__aenter__() # Acquire the connection instance
        try:
            # Explicitly BEGIN transaction. aiosqlite defaults might differ.
            # Using DEFERRED is generally fine unless immediate locking is needed.
            await conn.execute("BEGIN DEFERRED TRANSACTION")
            logger.debug("DB Transaction Started.")
            yield conn # Provide the connection to the 'async with' block
        except Exception as e:
            logger.error(f"Exception during transaction, rolling back: {e}", exc_info=True)
            await conn.rollback()
            logger.warning("DB Transaction Rolled Back.", emoji_key="rewind")
            raise # Re-raise the exception after rollback
        else:
            await conn.commit()
            logger.debug("DB Transaction Committed.")
        finally:
            # __aexit__ for the base DBConnection doesn't close the connection,
            # so we don't need to call it explicitly here. The transaction is finished.
            pass

# Custom SQLite helper functions (from cognitive_memory) - Keep these
def _json_contains(json_text, search_value):
    if not json_text: 
        return False
    try: 
        return search_value in json.loads(json_text) if isinstance(json.loads(json_text), list) else False
    except Exception: 
        return False

def _json_contains_any(json_text, search_values_json):
    if not json_text or not search_values_json: 
        return False
    try:
        data = json.loads(json_text)
        search_values = json.loads(search_values_json)
        if not isinstance(data, list) or not isinstance(search_values, list): 
            return False
        return any(value in data for value in search_values)
    except Exception: 
        return False

def _json_contains_all(json_text, search_values_json):
    if not json_text or not search_values_json: 
        return False
    try:
        data = json.loads(json_text)
        search_values = json.loads(search_values_json)
        if not isinstance(data, list) or not isinstance(search_values, list): 
            return False
        return all(value in data for value in search_values)
    except Exception: 
        return False

def _compute_memory_relevance(importance, confidence, created_at, access_count, last_accessed):
    """Computes a relevance score based on multiple factors. Uses Unix Timestamps."""
    now = time.time()
    age_hours = (now - created_at) / 3600 if created_at else 0
    recency_factor = 1.0 / (1.0 + (now - (last_accessed or created_at)) / 86400) # Use created_at if never accessed

    decayed_importance = max(0, importance * (1.0 - MEMORY_DECAY_RATE * age_hours))
    usage_boost = min(1.0 + (access_count / 10.0), 2.0) if access_count else 1.0

    relevance = (decayed_importance * usage_boost * confidence * recency_factor)
    return min(max(relevance, 0.0), 10.0)


# ======================================================
# Utilities
# ======================================================

def to_iso_z(ts: float) -> str:        # helper ‚áí  ISO‚Äë8601 with trailing ‚ÄúZ‚Äù
    return (
        datetime.fromtimestamp(ts, tz=timezone.utc)
        .isoformat(timespec="seconds")
        .replace("+00:00", "Z")
    )

class MemoryUtils:
    """Utility methods for memory operations."""

    @staticmethod
    def generate_id() -> str:
        """Generate a unique UUID V4 string for database records."""
        return str(uuid.uuid4())

    @staticmethod
    async def serialize(obj: Any) -> Optional[str]:
        """Safely serialize an arbitrary Python object to a JSON string.

        Handles potential serialization errors and very large objects.
        Attempts to represent complex objects that fail direct serialization.
        If the final JSON string exceeds MAX_TEXT_LENGTH, it returns a
        JSON object indicating truncation.

        Args:
            obj: The Python object to serialize.

        Returns:
            A JSON string representation, or None if the input is None.
            Returns a specific error JSON structure if serialization fails or
            if the resulting JSON string exceeds MAX_TEXT_LENGTH.
        """
        if obj is None:
            return None

        json_str = None # Initialize variable

        try:
            # Attempt direct JSON serialization with reasonable defaults
            # Use default=str as a basic fallback for common non-serializable types like datetime
            json_str = json.dumps(obj, ensure_ascii=False, default=str)

        except TypeError as e:
            # Handle objects that are not directly serializable (like sets, custom classes)
            logger.debug(f"Direct JSON serialization failed for type {type(obj)}: {e}. Trying fallback.")
            try:
                # Attempt a fallback using string representation
                fallback_repr = str(obj)
                # Ensure fallback doesn't exceed limits either, using robust UTF-8 handling
                fallback_bytes = fallback_repr.encode('utf-8')
                if len(fallback_bytes) > MAX_TEXT_LENGTH:
                    # Truncate the byte representation
                    truncated_bytes = fallback_bytes[:MAX_TEXT_LENGTH]
                    # Decode back to string, replacing invalid byte sequences caused by truncation
                    truncated_repr = truncated_bytes.decode('utf-8', errors='replace')

                    # Optional refinement: Check if the last character is the replacement char (U+FFFD)
                    # If so, try truncating one byte less to avoid splitting a multi-byte char right at the end.
                    # This is a heuristic and might not always be perfect but can improve readability.
                    if truncated_repr.endswith('\ufffd') and MAX_TEXT_LENGTH > 1:
                         # Try decoding one byte less
                         shorter_repr = fallback_bytes[:MAX_TEXT_LENGTH-1].decode('utf-8', errors='replace')
                         # If the shorter version *doesn't* end with the replacement character, use it.
                         if not shorter_repr.endswith('\ufffd'):
                              truncated_repr = shorter_repr

                    truncated_repr += "[TRUNCATED]" # Add ellipsis to indicate truncation
                    logger.warning(f"Fallback string representation truncated for type {type(obj)}.")
                else:
                    # No truncation needed for the fallback string itself
                    truncated_repr = fallback_repr

                # Create the JSON string containing the error and the (potentially truncated) fallback
                json_str = json.dumps({
                    "error": f"Serialization failed for type {type(obj)}.",
                    "fallback_repr": truncated_repr # Store the safely truncated string representation
                }, ensure_ascii=False)

            except Exception as fallback_e:
                # Final fallback if even string conversion fails
                logger.error(f"Could not serialize object of type {type(obj)} even with fallback: {fallback_e}", exc_info=True)
                json_str = json.dumps({
                    "error": f"Unserializable object type {type(obj)}. Fallback failed.",
                    "critical_error": str(fallback_e)
                }, ensure_ascii=False)

        # --- Check final length AFTER serialization attempt (success or fallback) ---
        # Ensure json_str is assigned before checking length
        if json_str is None:
             # This case should theoretically not be reached if the logic above is sound,
             # but added as a safeguard. It implies an unexpected path where serialization
             # didn't succeed but also didn't fall into the error handlers properly.
             logger.error(f"Internal error: json_str is None after serialization attempt for object of type {type(obj)}")
             return json.dumps({
                 "error": "Internal serialization error occurred.",
                 "original_type": str(type(obj))
             }, ensure_ascii=False)


        # Check final length against MAX_TEXT_LENGTH (bytes)
        final_bytes = json_str.encode('utf-8')
        if len(final_bytes) > MAX_TEXT_LENGTH:
            # If the generated JSON (even if it's an error JSON from fallback) is too long,
            # return a standard "too long" error marker with a preview.
            logger.warning(f"Serialized JSON string exceeds max length ({MAX_TEXT_LENGTH} bytes). Returning truncated indicator.")
            # Provide a preview of the oversized JSON string
            preview_str = json_str[:200] + ("..." if len(json_str) > 200 else "")
            return json.dumps({
                "error": "Serialized content exceeded maximum length.",
                "original_type": str(type(obj)),
                "preview": preview_str # Provide a small preview of the oversized content
            }, ensure_ascii=False)
        else:
            # Return the valid JSON string if within limits
            return json_str

    @staticmethod
    async def deserialize(json_str: Optional[str]) -> Any:
        """Safely deserialize a JSON string back into a Python object.

        Handles None input and potential JSON decoding errors. If decoding fails,
        it returns the original string, assuming it might not have been JSON
        in the first place (e.g., a truncated representation).
        """
        if json_str is None:
            return None
        if not json_str.strip(): # Handle empty strings
             return None
        try:
            # Attempt to load the JSON string
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            # If it fails, log the issue and return the original string
            # This might happen if the string stored was an error message or truncated data
            logger.debug(f"Failed to deserialize JSON: {e}. Content was: '{json_str[:100]}...'. Returning raw string.")
            return json_str
        except Exception as e:
             # Catch other potential errors during deserialization
             logger.error(f"Unexpected error deserializing JSON: {e}. Content: '{json_str[:100]}...'", exc_info=True)
             return json_str # Return original string as fallback

    @staticmethod
    def _validate_sql_identifier(identifier: str, identifier_type: str = "column/table") -> str:
        """Validates a string intended for use as an SQL table or column name.

        Prevents SQL injection by ensuring the identifier only contains
        alphanumeric characters and underscores. Raises ToolInputError if invalid.

        Args:
            identifier: The string to validate.
            identifier_type: A description of what the identifier represents (for error messages).

        Returns:
            The validated identifier if it's safe.

        Raises:
            ToolInputError: If the identifier is invalid.
        """
        # Simple regex: Allows letters, numbers, and underscores. Must start with a letter or underscore.
        # Adjust regex if more complex identifiers (e.g., quoted) are needed, but keep it strict.
        if not identifier or not re.fullmatch(r"^[a-zA-Z_][a-zA-Z0-9_]*$", identifier):
            logger.error(f"Invalid SQL identifier provided: '{identifier}'")
            raise ToolInputError(f"Invalid {identifier_type} name provided. Must be alphanumeric/underscore.", param_name=identifier_type)
        # Optional: Check against a known allowlist of tables/columns if possible
        # known_tables = {"actions", "thoughts", "memories", ...}
        # if identifier_type == "table" and identifier not in known_tables:
        #     raise ToolInputError(f"Unknown table name provided: {identifier}", param_name=identifier_type)
        return identifier
    
    @staticmethod
    async def get_next_sequence_number(conn: aiosqlite.Connection, parent_id: str, table: str, parent_col: str) -> int:
        """Get the next sequence number for ordering items within a parent scope.

        Args:
            conn: The database connection.
            parent_id: The ID of the parent entity (e.g., workflow_id, thought_chain_id).
            table: The name of the table containing the sequence number (e.g., 'actions', 'thoughts').
            parent_col: The name of the column linking to the parent entity.

        Returns:
            The next available integer sequence number (starting from 1).
        """
        # --- Validate dynamic identifiers to prevent SQL injection ---
        validated_table = MemoryUtils._validate_sql_identifier(table, "table")
        validated_parent_col = MemoryUtils._validate_sql_identifier(parent_col, "parent_col")
        # --- End Validation ---
        #         
        # --- Concurrency Note ---
        # This read-then-write operation (SELECT MAX + 1, then INSERT) is
        # generally safe within the transaction managed by the DBConnection
        # context manager wrapping the calling tool function. This makes it
        # atomic relative to other *completed* tool calls.
        # However, a theoretical race condition exists if *multiple concurrent*
        # calls to the *same* tool function attempt to get the sequence number
        # for the *exact same parent_id* before the transaction commits.
        # They might both read the same MAX value.
        # In practice, SQLite's isolation levels (especially WAL mode) and the
        # typical single-threaded nature of agent actions within a workflow
        # make this unlikely to cause issues.
        # If duplicate sequence numbers are observed under very high concurrency,
        # consider implementing an explicit asyncio.Lock per parent_id,
        # managed in a shared dictionary within the MemoryUtils class or a
        # dedicated sequence manager. For now, we rely on transaction isolation.
        # --- End Concurrency Note ---

        # Use validated identifiers in the f-string
        sql = f"SELECT MAX(sequence_number) FROM {validated_table} WHERE {validated_parent_col} = ?"
        # Use execute directly on the connection for context management
        async with conn.execute(sql, (parent_id,)) as cursor:
            row = await cursor.fetchone()
            # If no rows exist (row is None) or MAX is NULL, start at 1. Otherwise, increment max.
            # Access by index as row might be None or a tuple/row object
            max_sequence = row[0] if row and row[0] is not None else 0
            return max_sequence + 1

    @staticmethod
    async def process_tags(conn: aiosqlite.Connection, entity_id: str, tags: List[str],
                          entity_type: str) -> None:
        """Ensures tags exist in the 'tags' table and associates them with a given entity
           in the appropriate junction table (e.g., 'workflow_tags').

        Args:
            conn: The database connection.
            entity_id: The ID of the entity (workflow, action, artifact).
            tags: A list of tag names (strings) to associate. Duplicates are handled.
            entity_type: The type of the entity ('workflow', 'action', 'artifact'). Must form valid SQL identifiers when combined with '_tags' or '_id'.
        """
        if not tags:
            return # Nothing to do if no tags are provided

        # Validate entity_type first as it forms part of identifiers
        # Allow only specific expected entity types
        allowed_entity_types = {"workflow", "action", "artifact"}
        if entity_type not in allowed_entity_types:
             raise ToolInputError(f"Invalid entity_type for tagging: {entity_type}", param_name="entity_type")

        # Define and validate dynamic identifiers
        junction_table_name = f"{entity_type}_tags"
        id_column_name = f"{entity_type}_id"
        validated_junction_table = MemoryUtils._validate_sql_identifier(junction_table_name, "junction_table")
        validated_id_column = MemoryUtils._validate_sql_identifier(id_column_name, "id_column")
        # --- End Validation ---

        tag_ids_to_link = []
        unique_tags = list(set(str(tag).strip().lower() for tag in tags if str(tag).strip())) # Clean, lowercase, unique tags
        now_unix = int(time.time())

        if not unique_tags:
            return # Nothing to do if tags are empty after cleaning

        # Ensure all unique tags exist in the 'tags' table and get their IDs
        for tag_name in unique_tags:
            # Attempt to insert the tag, ignoring if it already exists
            await conn.execute(
                """
                INSERT INTO tags (name, created_at) VALUES (?, ?)
                ON CONFLICT(name) DO NOTHING;
                """,
                (tag_name, now_unix)
            )
            # Retrieve the tag_id (whether newly inserted or existing)
            cursor = await conn.execute("SELECT tag_id FROM tags WHERE name = ?", (tag_name,))
            row = await cursor.fetchone()
            await cursor.close() # Close cursor

            if row:
                tag_ids_to_link.append(row["tag_id"])
            else:
                # This should ideally not happen due to the upsert logic, but log if it does
                logger.warning(f"Could not find or create tag_id for tag: {tag_name}")

        # Link the retrieved tag IDs to the entity in the junction table
        if tag_ids_to_link:
            link_values = [(entity_id, tag_id) for tag_id in tag_ids_to_link]
            # Use INSERT OR IGNORE to handle potential race conditions or duplicate calls gracefully
            # Use validated identifiers in the f-string
            await conn.executemany(
                f"INSERT OR IGNORE INTO {validated_junction_table} ({validated_id_column}, tag_id) VALUES (?, ?)",
                link_values
            )
            logger.debug(f"Associated {len(link_values)} tags with {entity_type} {entity_id}")

    @staticmethod
    async def _log_memory_operation(conn: aiosqlite.Connection, workflow_id: str, operation: str,
                                   memory_id: Optional[str] = None, action_id: Optional[str] = None,
                                   operation_data: Optional[Dict] = None):
        """Logs an operation related to memory management or agent activity. Internal helper."""
        try:
            op_id = MemoryUtils.generate_id()
            timestamp_unix = int(time.time())
            # Serialize operation_data carefully using the updated serialize method
            op_data_json = await MemoryUtils.serialize(operation_data) if operation_data is not None else None

            await conn.execute(
                """
                INSERT INTO memory_operations
                (operation_log_id, workflow_id, memory_id, action_id, operation, operation_data, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (op_id, workflow_id, memory_id, action_id, operation, op_data_json, timestamp_unix)
            )
        except Exception as e:
            # Log failures robustly, don't let logging break main logic
            logger.error(f"CRITICAL: Failed to log memory operation '{operation}': {e}", exc_info=True)

    @staticmethod
    async def _update_memory_access(conn: aiosqlite.Connection, memory_id: str):
        """Updates the last_accessed timestamp and increments access_count for a memory. Internal helper."""
        now_unix = int(time.time())
        try:
            # Use COALESCE to handle the first access correctly
            await conn.execute(
                """
                UPDATE memories
                SET last_accessed = ?,
                    access_count = COALESCE(access_count, 0) + 1
                WHERE memory_id = ?
                """,
                (now_unix, memory_id)
            )
        except Exception as e:
            logger.warning(f"Failed to update memory access stats for {memory_id}: {e}", exc_info=True)

# ======================================================
# Embedding Service Integration & Semantic Search Logic
# ======================================================

async def _store_embedding(conn: aiosqlite.Connection, memory_id: str, text: str) -> Optional[str]:
    """Generates and stores an embedding for a memory using the EmbeddingService.

    Args:
        conn: Database connection.
        memory_id: ID of the memory.
        text: Text content to generate embedding for (often content + description).

    Returns:
        ID of the stored embedding record in the embeddings table, or None if failed.
    """
    try:
        embedding_service = get_embedding_service() # Get singleton instance
        if not embedding_service.client: # Check if service was initialized correctly (has client)
             logger.warning("EmbeddingService client not available. Cannot generate embedding.", emoji_key="warning")
             return None

        # Generate embedding using the service (handles caching internally)
        embedding_list = await embedding_service.create_embeddings(texts=[text])
        if not embedding_list or not embedding_list[0]: # Extra check for empty embedding
             logger.warning(f"Failed to generate embedding for memory {memory_id}")
             return None
        embedding_array = np.array(embedding_list[0], dtype=np.float32) # Ensure consistent dtype
        if embedding_array.size == 0:
             logger.warning(f"Generated embedding is empty for memory {memory_id}")
             return None

        # Get the embedding dimension
        embedding_dimension = embedding_array.shape[0]

        # Generate a unique ID for this embedding entry in our DB table
        embedding_db_id = MemoryUtils.generate_id()
        embedding_bytes = embedding_array.tobytes()
        model_used = embedding_service.default_model # Or get model used if service provides it

        # Store embedding in our DB
        await conn.execute(
            """
            INSERT INTO embeddings (id, memory_id, model, embedding, dimension, created_at)
            VALUES (?, ?, ?, ?, ?, ?)
            ON CONFLICT(memory_id) DO UPDATE SET
                id = excluded.id,
                model = excluded.model,
                embedding = excluded.embedding,
                dimension = excluded.dimension,
                created_at = excluded.created_at
            """,
            (
                embedding_db_id,
                memory_id,
                model_used,
                embedding_bytes,
                embedding_dimension,
                int(time.time())
            )
        )
        # Update the memory record to link to this *embedding table entry ID*
        # Note: The cognitive_memory schema had embedding_id as FK to embeddings.id
        # We will store embedding_db_id here.
        await conn.execute(
            "UPDATE memories SET embedding_id = ? WHERE memory_id = ?",
            (embedding_db_id, memory_id)
        )

        logger.debug(f"Stored embedding {embedding_db_id} (Dim: {embedding_dimension}) for memory {memory_id}")
        return embedding_db_id # Return the ID of the row in the embeddings table

    except Exception as e:
        logger.error(f"Failed to store embedding for memory {memory_id}: {e}", exc_info=True)
        return None


async def _find_similar_memories(
    conn: aiosqlite.Connection,
    query_text: str,
    workflow_id: Optional[str] = None,
    limit: int = 5,
    threshold: float = SIMILARITY_THRESHOLD,
    memory_level: Optional[str] = None,
    memory_type: Optional[str] = None
) -> List[Tuple[str, float]]:
    """Finds memories with similar semantic meaning using embeddings stored in SQLite.
       Filters by workflow, level, type, dimension, and TTL.

    Args:
        conn: Database connection.
        query_text: Query text to find similar memories.
        workflow_id: Optional workflow ID to limit search.
        limit: Maximum number of results to return *after similarity calculation*.
        threshold: Minimum similarity score (0-1).
        memory_level: Optional memory level to filter by.
        memory_type: Optional memory type to filter by.

    Returns:
        List of tuples (memory_id, similarity_score) sorted by similarity descending.
    """
    try:
        embedding_service = get_embedding_service()
        if not embedding_service.client:
            logger.warning("EmbeddingService client not available. Cannot perform semantic search.", emoji_key="warning")
            return []

        # 1. Generate query embedding
        query_embedding_list = await embedding_service.create_embeddings(texts=[query_text])
        if not query_embedding_list or not query_embedding_list[0]: # Extra check
            logger.warning(f"Failed to generate query embedding for: '{query_text[:50]}...'")
            return []
        query_embedding = np.array(query_embedding_list[0], dtype=np.float32) # Ensure consistent dtype
        if query_embedding.size == 0:
            logger.warning(f"Generated query embedding is empty for: '{query_text[:50]}...'")
            return []

        query_dimension = query_embedding.shape[0]
        query_embedding_2d = query_embedding.reshape(1, -1) # Reshape for scikit-learn

        # 2. Build query to fetch candidate embeddings from DB, including filters
        sql = """
        SELECT m.memory_id, e.embedding
        FROM memories m
        JOIN embeddings e ON m.embedding_id = e.id
        WHERE e.dimension = ?
        """ 
        params: List[Any] = [query_dimension]

        if workflow_id:
            sql += " AND m.workflow_id = ?"
            params.append(workflow_id)
        if memory_level:
            sql += " AND m.memory_level = ?"
            params.append(memory_level.lower()) # Ensure lowercase for comparison
        if memory_type:
            sql += " AND m.memory_type = ?"
            params.append(memory_type.lower()) # Ensure lowercase

        # Add TTL check
        now_unix = int(time.time())
        sql += " AND (m.ttl = 0 OR m.created_at + m.ttl > ?)"
        params.append(now_unix)

        # Optimization: Potentially limit candidates fetched *before* calculating all similarities
        # Fetching more candidates than `limit` allows for better ranking after similarity calculation
        candidate_limit = max(limit * 5, 50) # Fetch more candidates than needed
        sql += " ORDER BY m.last_accessed DESC NULLS LAST LIMIT ?" # Prioritize recently accessed
        params.append(candidate_limit)

        # 3. Fetch candidate embeddings (only those with matching dimension)
        candidates: List[Tuple[str, bytes]] = []
        async with conn.execute(sql, params) as cursor:
            candidates = await cursor.fetchall() # Fetchall is ok for limited candidates

        if not candidates:
            logger.debug(f"No candidate memories found matching filters (including dimension {query_dimension}) for semantic search.")
            return []

        # 4. Calculate similarities for candidates
        similarities: List[Tuple[str, float]] = []
        for memory_id, embedding_bytes in candidates:
            try:
                # Deserialize embedding from bytes
                memory_embedding = np.frombuffer(embedding_bytes, dtype=np.float32)
                if memory_embedding.size == 0:
                    logger.warning(f"Skipping empty embedding blob for memory {memory_id}")
                    continue

                # Reshape for scikit-learn compatibility
                memory_embedding_2d = memory_embedding.reshape(1, -1)

                # --- Safety Check: Verify dimensions again (should match due to SQL filter) ---
                # This primarily guards against database corruption or schema inconsistencies.
                if query_embedding_2d.shape[1] != memory_embedding_2d.shape[1]:
                    logger.warning(f"Dimension mismatch detected for memory {memory_id} (Query: {query_embedding_2d.shape[1]}, DB: {memory_embedding_2d.shape[1]}) despite DB filter. Skipping.")
                    continue
                # --- End Safety Check ---

                # Calculate cosine similarity
                similarity = sk_cosine_similarity(query_embedding_2d, memory_embedding_2d)[0][0]

                # 5. Filter by threshold
                if similarity >= threshold:
                    similarities.append((memory_id, float(similarity)))

            except Exception as e:
                logger.warning(f"Error processing embedding for memory {memory_id}: {e}")
                continue

        # 6. Sort by similarity and limit to the final requested count
        similarities.sort(key=lambda x: x[1], reverse=True)

        logger.debug(f"Calculated similarities for {len(candidates)} candidates (Dim: {query_dimension}). Found {len(similarities)} memories above threshold {threshold} before limiting to {limit}.")
        return similarities[:limit]

    except Exception as e:
        logger.error(f"Failed to find similar memories: {e}", exc_info=True)
        return []
    
# ======================================================
# Public Tool Functions (Integrated & Adapted)
# ======================================================

# --- 1. Initialization ---
@with_tool_metrics
@with_error_handling
async def initialize_memory_system(db_path: str = DEFAULT_DB_PATH) -> Dict[str, Any]:
    """Initializes the Unified Agent Memory system and checks embedding service status.

    Creates or verifies the database schema using aiosqlite, applies optimizations,
    and attempts to initialize the singleton EmbeddingService. **Raises ToolError if
    the embedding service fails to initialize or is non-functional.**

    Args:
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Initialization status dictionary (only if successful).
        {
            "success": true,
            "message": "Unified Memory System initialized successfully.",
            "db_path": "/path/to/unified_agent_memory.db",
            "embedding_service_functional": true, # Will always be true if function returns successfully
            "embedding_service_warning": null,
            "processing_time": 0.123
        }

    Raises:
        ToolError: If database initialization fails OR if the EmbeddingService
                   cannot be initialized or lacks a functional client (e.g., missing API key).
    """
    start_time = time.time()
    logger.info("Initializing Unified Memory System...", emoji_key="rocket")
    embedding_service_warning = None # This will now likely be part of the error message

    try:
        # Initialize/Verify Database Schema via DBConnection context manager
        async with DBConnection(db_path) as conn:
             # Perform a simple check to ensure DB connection is working
            cursor = await conn.execute("SELECT count(*) FROM workflows")
            _ = await cursor.fetchone()
            await cursor.close() # Close cursor
            # No explicit commit needed here if using default aiosqlite behavior or autocommit
        logger.success("Unified Memory System database connection verified.", emoji_key="database")

        # Attempt to initialize/get the EmbeddingService singleton and VERIFY functionality
        try:
            # This call triggers the service's __init__ if it's the first time
            embedding_service = get_embedding_service()
            # Check if the service has its client (e.g., requires API key)
            if embedding_service.client is not None:
                logger.info("EmbeddingService initialized and functional.", emoji_key="brain")
            else:
                embedding_service_warning = "EmbeddingService client not available (check API key?). Embeddings disabled."
                logger.error(embedding_service_warning, emoji_key="warning") # Log as error
                # Raise explicit error instead of just returning False status
                raise ToolError(embedding_service_warning)
        except Exception as embed_init_err:
             # This includes the explicit ToolError raised above for missing client
             if not isinstance(embed_init_err, ToolError): # Avoid double wrapping if it was the specific client error
                 embedding_service_warning = f"Failed to initialize EmbeddingService: {str(embed_init_err)}. Embeddings disabled."
                 logger.error(embedding_service_warning, emoji_key="error", exc_info=True)
                 raise ToolError(embedding_service_warning) from embed_init_err
             else:
                 # Re-raise the ToolError directly if it was the specific missing client error
                 raise embed_init_err

        # If we reach here, both DB and Embedding Service are functional
        processing_time = time.time() - start_time
        logger.success("Unified Memory System initialized successfully (DB and Embeddings OK).", emoji_key="white_check_mark", time=processing_time)

        return {
            "success": True,
            "message": "Unified Memory System initialized successfully.",
            "db_path": os.path.abspath(db_path),
            "embedding_service_functional": True, # Will always be true if this return is reached
            "embedding_service_warning": None, # No warning if successful
            "processing_time": processing_time
        }
    except Exception as e:
        # This catches errors during DB initialization OR the ToolError raised from embedding failure
        processing_time = time.time() - start_time
        # Ensure it's logged as a critical failure
        logger.error(f"Failed to initialize memory system: {str(e)}", emoji_key="x", exc_info=True, time=processing_time)
        # Re-raise as ToolError if it wasn't already one
        if isinstance(e, ToolError):
            raise e
        else:
            # Wrap unexpected DB errors
            raise ToolError(f"Memory system initialization failed: {str(e)}") from e

# --- 2. Workflow Management Tools (Ported/Adapted from agent_memory) ---
@with_tool_metrics
@with_error_handling
async def create_workflow(
    title: str,
    description: Optional[str] = None,
    goal: Optional[str] = None,
    tags: Optional[List[str]] = None,
    metadata: Optional[Dict[str, Any]] = None,
    parent_workflow_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Creates a new workflow, including a default thought chain and initial goal thought if specified.

    Args:
        title: A clear, descriptive title for the workflow.
        description: (Optional) A more detailed explanation of the workflow's purpose.
        goal: (Optional) The high-level goal or objective. If provided, an initial 'goal' thought is created.
        tags: (Optional) List of keyword tags to categorize this workflow.
        metadata: (Optional) Additional structured data about the workflow.
        parent_workflow_id: (Optional) ID of a parent workflow.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing information about the created workflow and its primary thought chain.
        Timestamps are returned as ISO 8601 strings.
        {
            "workflow_id": "uuid-string",
            "title": "Workflow Title",
            "description": "...",
            "goal": "...",
            "status": "active",
            "created_at": "iso-timestampZ",
            "updated_at": "iso-timestampZ",
            "tags": ["tag1"],
            "primary_thought_chain_id": "uuid-string",
            "success": true
        }

    Raises:
        ToolInputError: If title is empty or parent workflow doesn't exist.
        ToolError: If the database operation fails.
    """
    # Validate required input
    if not title or not isinstance(title, str):
        raise ToolInputError("Workflow title must be a non-empty string", param_name="title")

    # Generate IDs and timestamps
    workflow_id = MemoryUtils.generate_id()
    now_unix = int(time.time())

    try:
        async with DBConnection(db_path) as conn:
            # Check parent workflow existence if provided
            if parent_workflow_id:
                cursor = await conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (parent_workflow_id,))
                parent_exists = await cursor.fetchone()
                await cursor.close() # Close cursor
                if not parent_exists:
                    raise ToolInputError(f"Parent workflow not found: {parent_workflow_id}", param_name="parent_workflow_id")

            # Serialize metadata
            metadata_json = await MemoryUtils.serialize(metadata)

            # Insert the main workflow record
            await conn.execute(
                """
                INSERT INTO workflows
                (workflow_id, title, description, goal, status, created_at, updated_at, parent_workflow_id, metadata, last_active)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (workflow_id, title, description, goal, WorkflowStatus.ACTIVE.value,
                 now_unix, now_unix, parent_workflow_id, metadata_json, now_unix) # *** CHANGED: Use now_unix ***
            )

            # Process and associate tags with the workflow
            await MemoryUtils.process_tags(conn, workflow_id, tags or [], "workflow")

            # Create the default thought chain associated with this workflow
            thought_chain_id = MemoryUtils.generate_id()
            chain_title = f"Main reasoning for: {title}" # Default title
            await conn.execute(
                "INSERT INTO thought_chains (thought_chain_id, workflow_id, title, created_at) VALUES (?, ?, ?, ?)",
                (thought_chain_id, workflow_id, chain_title, now_unix)
            )

            # If a goal was provided, add it as the first thought in the default chain
            if goal:
                thought_id = MemoryUtils.generate_id()
                # Get sequence number (will be 1 for the first thought)
                seq_no = await MemoryUtils.get_next_sequence_number(conn, thought_chain_id, "thoughts", "thought_chain_id")
                await conn.execute(
                    """
                    INSERT INTO thoughts
                    (thought_id, thought_chain_id, thought_type, content, sequence_number, created_at)
                    VALUES (?, ?, ?, ?, ?, ?)
                    """,
                    (thought_id, thought_chain_id, ThoughtType.GOAL.value, goal, seq_no, now_unix)
                )

            # Commit the transaction
            await conn.commit()

            # Prepare the result dictionary, formatting timestamps for output

            result = {
                "workflow_id": workflow_id,
                "title": title,
                "description": description,
                "goal": goal,
                "status": WorkflowStatus.ACTIVE.value,
                "created_at": to_iso_z(now_unix),
                "updated_at": to_iso_z(now_unix),
                "tags": tags or [],
                "primary_thought_chain_id": thought_chain_id,  # default chain ID for the agent
                "success": True,
            }
            logger.info(f"Created workflow '{title}' ({workflow_id}) with primary thought chain {thought_chain_id}", emoji_key="clipboard")
            return result

    except ToolInputError:
        raise # Re-raise specific input errors
    except Exception as e:
        # Log the error and raise a generic ToolError
        logger.error(f"Error creating workflow: {e}", exc_info=True)
        raise ToolError(f"Failed to create workflow: {str(e)}") from e

@with_tool_metrics
@with_error_handling
async def update_workflow_status(
    workflow_id: str,
    status: str,
    completion_message: Optional[str] = None,
    update_tags: Optional[List[str]] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Updates the status of a workflow. (Ported from agent_memory, adapted).
       Timestamps are returned as ISO 8601 strings.
    """
    try:
        status_enum = WorkflowStatus(status.lower())
    except ValueError as e:
        valid_statuses = [s.value for s in WorkflowStatus]
        raise ToolInputError(f"Invalid status '{status}'. Must be one of: {', '.join(valid_statuses)}", param_name="status") from e

    now_unix = int(time.time())

    try:
        async with DBConnection(db_path) as conn:
            # Check existence first
            cursor = await conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,))
            exists = await cursor.fetchone()
            await cursor.close()
            if not exists:
                raise ToolInputError(f"Workflow not found: {workflow_id}", param_name="workflow_id")

            update_params = [status_enum.value, now_unix, now_unix] # status, updated_at, last_active
            set_clauses = "status = ?, updated_at = ?, last_active = ?"

            if status_enum in [WorkflowStatus.COMPLETED, WorkflowStatus.FAILED, WorkflowStatus.ABANDONED]:
                set_clauses += ", completed_at = ?"
                update_params.append(now_unix)
            
            # Add workflow_id to params for WHERE clause
            update_params.append(workflow_id)

            await conn.execute(
                f"UPDATE workflows SET {set_clauses} WHERE workflow_id = ?",
                update_params
            )

            # Add completion message as thought
            if completion_message:
                 cursor = await conn.execute("SELECT thought_chain_id FROM thought_chains WHERE workflow_id = ? ORDER BY created_at ASC LIMIT 1", (workflow_id,))
                 row = await cursor.fetchone()
                 await cursor.close()
                 if row:
                     thought_chain_id = row["thought_chain_id"]
                     seq_no = await MemoryUtils.get_next_sequence_number(conn, thought_chain_id, "thoughts", "thought_chain_id")
                     thought_id = MemoryUtils.generate_id()
                     thought_type = ThoughtType.SUMMARY.value if status_enum == WorkflowStatus.COMPLETED else ThoughtType.REFLECTION.value
                     await conn.execute(
                         "INSERT INTO thoughts (thought_id, thought_chain_id, thought_type, content, sequence_number, created_at) VALUES (?, ?, ?, ?, ?, ?)",
                         (thought_id, thought_chain_id, thought_type, completion_message, seq_no, now_unix)
                     )

            # Process additional tags
            await MemoryUtils.process_tags(conn, workflow_id, update_tags or [], "workflow")
            await conn.commit()

            result = {
                "workflow_id": workflow_id,
                "status": status_enum.value,
                "updated_at": to_iso_z(now_unix),
                "success": True,
            }

            if status_enum in (
                WorkflowStatus.COMPLETED,
                WorkflowStatus.FAILED,
                WorkflowStatus.ABANDONED,
            ):
                result["completed_at"] = to_iso_z(now_unix)
                logger.info(f"Updated workflow {workflow_id} status to '{status_enum.value}'", emoji_key="arrows_counterclockwise")
                return result

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error updating workflow status: {e}", exc_info=True)
        raise ToolError(f"Failed to update workflow status: {str(e)}") from e
    

# --- 3. Action Tracking Tools (Ported/Adapted from agent_memory & Integrated) ---
@with_tool_metrics
@with_error_handling
async def record_action_start(
    workflow_id: str,
    action_type: str,
    reasoning: str,
    tool_name: Optional[str] = None,
    tool_args: Optional[Dict[str, Any]] = None,
    title: Optional[str] = None,
    parent_action_id: Optional[str] = None,
    tags: Optional[List[str]] = None,
    related_thought_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Records the start of an action within a workflow and creates a corresponding episodic memory.

    Use this tool whenever you begin a significant step in your workflow. It logs the action details
    and automatically creates a linked memory entry summarizing the action's initiation and reasoning.

    Args:
        workflow_id: The ID of the workflow this action belongs to.
        action_type: The type of action (e.g., 'tool_use', 'reasoning', 'planning'). See ActionType enum.
        reasoning: An explanation of why this action is being taken.
        tool_name: (Optional) The name of the tool being used (required if action_type is 'tool_use').
        tool_args: (Optional) Arguments passed to the tool (used if action_type is 'tool_use').
        title: (Optional) A brief, descriptive title for this action. Auto-generated if omitted.
        parent_action_id: (Optional) ID of parent action if this is a sub-action.
        tags: (Optional) List of tags to categorize this action.
        related_thought_id: (Optional) ID of a thought that led to this action.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        A dictionary containing information about the started action and the linked memory.

    Raises:
        ToolInputError: If required parameters are missing or invalid, or referenced entities don't exist.
        ToolError: If the database operation fails.
    """
    # --- Input Validation ---
    try:
        action_type_enum = ActionType(action_type.lower())
    except ValueError as e:
        valid_types = [t.value for t in ActionType]
        raise ToolInputError(f"Invalid action_type '{action_type}'. Must be one of: {', '.join(valid_types)}", param_name="action_type") from e

    if not reasoning or not isinstance(reasoning, str):
        raise ToolInputError("Reasoning must be a non-empty string", param_name="reasoning")
    if action_type_enum == ActionType.TOOL_USE and not tool_name:
        raise ToolInputError("Tool name is required for 'tool_use' action type", param_name="tool_name")

    # --- Initialization ---
    action_id = MemoryUtils.generate_id()
    memory_id = MemoryUtils.generate_id() # Pre-generate ID for the linked memory
    now_unix = int(time.time()) 

    try:
        async with DBConnection(db_path) as conn:
            # --- Existence Checks (Workflow, Parent Action, Related Thought) ---
            cursor = await conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,))
            wf_exists = await cursor.fetchone()
            await cursor.close()
            if not wf_exists:
                raise ToolInputError(f"Workflow not found: {workflow_id}", param_name="workflow_id")

            if parent_action_id:
                cursor = await conn.execute("SELECT 1 FROM actions WHERE action_id = ? AND workflow_id = ?", (parent_action_id, workflow_id))
                parent_exists = await cursor.fetchone()
                await cursor.close()
                if not parent_exists:
                    raise ToolInputError(f"Parent action '{parent_action_id}' not found or does not belong to workflow '{workflow_id}'.", param_name="parent_action_id")

            if related_thought_id:
                 cursor = await conn.execute("SELECT 1 FROM thoughts t JOIN thought_chains tc ON t.thought_chain_id = tc.thought_chain_id WHERE t.thought_id = ? AND tc.workflow_id = ?", (related_thought_id, workflow_id))
                 thought_exists = await cursor.fetchone()
                 await cursor.close()
                 if not thought_exists:
                     raise ToolInputError(f"Related thought '{related_thought_id}' not found or does not belong to workflow '{workflow_id}'.", param_name="related_thought_id")

            # --- Determine Action Title ---
            sequence_number = await MemoryUtils.get_next_sequence_number(conn, workflow_id, "actions", "workflow_id")
            auto_title = title
            if not auto_title:
                if action_type_enum == ActionType.TOOL_USE and tool_name:
                    auto_title = f"Using {tool_name}"
                else:
                    first_sentence = reasoning.split('.')[0].strip()
                    auto_title = first_sentence[:50] + ("..." if len(first_sentence) > 50 else "")
            if not auto_title: # Fallback if reasoning was very short
                auto_title = f"{action_type_enum.value.capitalize()} Action #{sequence_number}"

            # --- Insert Action Record ---
            tool_args_json = await MemoryUtils.serialize(tool_args)
            await conn.execute(
                """
                INSERT INTO actions (action_id, workflow_id, parent_action_id, action_type, title,
                reasoning, tool_name, tool_args, status, started_at, sequence_number)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (action_id, workflow_id, parent_action_id, action_type_enum.value, auto_title,
                 reasoning, tool_name, tool_args_json, ActionStatus.IN_PROGRESS.value, now_unix, sequence_number)
            )

            # --- Process Tags for Action ---
            await MemoryUtils.process_tags(conn, action_id, tags or [], "action")

            # --- Link Action to Related Thought ---
            if related_thought_id:
                await conn.execute("UPDATE thoughts SET relevant_action_id = ? WHERE thought_id = ?", (action_id, related_thought_id))

            # --- Create Linked Episodic Memory ---
            memory_content = f"Started action [{sequence_number}] '{auto_title}' ({action_type_enum.value}). Reasoning: {reasoning}"
            if tool_name:
                 memory_content += f" Tool: {tool_name}."
            mem_tags = ["action_start", action_type_enum.value] + (tags or [])
            mem_tags_json = json.dumps(list(set(mem_tags)))

            await conn.execute(
                 """
                 INSERT INTO memories (memory_id, workflow_id, action_id, content, memory_level, memory_type,
                 importance, confidence, tags, created_at, updated_at, access_count)
                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                 """,
                 (memory_id, workflow_id, action_id, memory_content, MemoryLevel.EPISODIC.value, MemoryType.ACTION_LOG.value,
                  5.0, 1.0, mem_tags_json, now_unix, now_unix, 0) # Memories already use Unix timestamps
            )
            await MemoryUtils._log_memory_operation(conn, workflow_id, "create_from_action_start", memory_id, action_id)

            # --- Update Workflow Timestamp ---
            await conn.execute("UPDATE workflows SET updated_at = ?, last_active = ? WHERE workflow_id = ?", (now_unix, now_unix, workflow_id))

            # --- Commit Transaction ---
            await conn.commit()

            # --- Prepare Result (Format timestamp for output) ---
            result = {
                "action_id": action_id,
                "workflow_id": workflow_id,
                "action_type": action_type_enum.value,
                "title": auto_title,
                "tool_name": tool_name,
                "status": ActionStatus.IN_PROGRESS.value,
                "started_at": to_iso_z(now_unix),     
                "sequence_number": sequence_number,
                "tags": tags or [],
                "linked_memory_id": memory_id,
                "success": True,
            }

            logger.info(
                f"Started action '{auto_title}' ({action_id}) in workflow {workflow_id}",
                emoji_key="fast_forward"
            )

            return result

    except ToolInputError:
        raise # Re-raise for specific handling
    except Exception as e:
        logger.error(f"Error recording action start: {e}", exc_info=True)
        raise ToolError(f"Failed to record action start: {str(e)}") from e
    

@with_tool_metrics
@with_error_handling
async def record_action_completion(
    action_id: str,
    status: str = "completed",
    tool_result: Optional[Any] = None,
    summary: Optional[str] = None,
    conclusion_thought: Optional[str] = None,
    conclusion_thought_type: str = "inference", # Default type for conclusion
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Records the completion or failure of an action and updates its linked memory.

    Marks an action (previously started with record_action_start) as finished,
    stores the tool result if applicable, optionally adds a summary or concluding thought,
    and updates the corresponding 'action_log' memory entry.

    Args:
        action_id: The ID of the action to complete.
        status: (Optional) Final status: 'completed', 'failed', or 'skipped'. Default 'completed'.
        tool_result: (Optional) The result returned by the tool for 'tool_use' actions.
        summary: (Optional) A brief summary of the action's outcome or findings.
        conclusion_thought: (Optional) A thought derived from this action's completion.
        conclusion_thought_type: (Optional) Type for the conclusion thought. Default 'inference'.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary confirming the action completion.
        {
            "action_id": "action-uuid",
            "workflow_id": "workflow-uuid",
            "status": "completed" | "failed" | "skipped",
            "completed_at": "iso-timestamp",
            "conclusion_thought_id": "thought-uuid" | None,
            "success": true
        }

    Raises:
        ToolInputError: If action not found or status/thought type is invalid.
        ToolError: If database operation fails.
    """
    start_time = time.time()
    # --- Validate Status ---
    try:
        status_enum = ActionStatus(status.lower())
        if status_enum not in [ActionStatus.COMPLETED, ActionStatus.FAILED, ActionStatus.SKIPPED]:
            raise ValueError("Status must indicate completion, failure, or skipping.")
    except ValueError as e:
        valid_statuses = [s.value for s in [ActionStatus.COMPLETED, ActionStatus.FAILED, ActionStatus.SKIPPED]]
        raise ToolInputError(f"Invalid completion status '{status}'. Must be one of: {', '.join(valid_statuses)}", param_name="status") from e

    # --- Validate Thought Type (if conclusion thought provided) ---
    thought_type_enum = None
    if conclusion_thought:
        try:
            thought_type_enum = ThoughtType(conclusion_thought_type.lower())
        except ValueError as e:
             valid_types = [t.value for t in ThoughtType]
             raise ToolInputError(f"Invalid thought type '{conclusion_thought_type}'. Must be one of: {', '.join(valid_types)}", param_name="conclusion_thought_type") from e

    now_unix = int(time.time())

    try:
        async with DBConnection(db_path) as conn:
            # --- 1. Verify Action and Get Workflow ID ---
            cursor = await conn.execute("SELECT workflow_id, status FROM actions WHERE action_id = ?", (action_id,))
            action_row = await cursor.fetchone()
            await cursor.close()
            if not action_row:
                raise ToolInputError(f"Action not found: {action_id}", param_name="action_id")
            workflow_id = action_row["workflow_id"]
            current_status = action_row["status"]
            if current_status not in [ActionStatus.IN_PROGRESS.value, ActionStatus.PLANNED.value]:
                 logger.warning(f"Action {action_id} already has terminal status '{current_status}'. Allowing update anyway.")

            # --- 2. Update Action Record ---
            tool_result_json = await MemoryUtils.serialize(tool_result)
            await conn.execute(
                """
                UPDATE actions
                SET status = ?,
                    completed_at = ?,
                    tool_result = ?
                WHERE action_id = ?
                """,
                (status_enum.value, now_unix, tool_result_json, action_id) # *** Use now_unix ***
            )

            # --- 3. Update Workflow Timestamp ---
            await conn.execute(
                "UPDATE workflows SET updated_at = ?, last_active = ? WHERE workflow_id = ?",
                (now_unix, now_unix, workflow_id) # *** Use now_unix ***
            )

            # --- 4. Add Conclusion Thought (if provided) ---
            conclusion_thought_id = None
            if conclusion_thought and thought_type_enum:
                cursor = await conn.execute("SELECT thought_chain_id FROM thought_chains WHERE workflow_id = ? ORDER BY created_at ASC LIMIT 1", (workflow_id,))
                chain_row = await cursor.fetchone()
                await cursor.close()
                if chain_row:
                    thought_chain_id = chain_row["thought_chain_id"]
                    seq_no = await MemoryUtils.get_next_sequence_number(conn, thought_chain_id, "thoughts", "thought_chain_id")
                    conclusion_thought_id = MemoryUtils.generate_id()
                    await conn.execute(
                        """
                        INSERT INTO thoughts
                            (thought_id, thought_chain_id, thought_type, content, sequence_number, created_at, relevant_action_id)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                        """,
                        (conclusion_thought_id, thought_chain_id, thought_type_enum.value, conclusion_thought, seq_no, now_unix, action_id) # *** Use now_unix ***
                    )
                    logger.debug(f"Recorded conclusion thought {conclusion_thought_id} for action {action_id}")
                else:
                     logger.warning(f"Could not find primary thought chain for workflow {workflow_id} to add conclusion thought.")

            # --- 5. Update Linked Episodic Memory ---
            cursor = await conn.execute("SELECT memory_id, content FROM memories WHERE action_id = ? AND memory_type = ?", (action_id, MemoryType.ACTION_LOG.value))
            memory_row = await cursor.fetchone()
            await cursor.close()
            if memory_row:
                memory_id = memory_row["memory_id"]
                original_content = memory_row["content"]
                update_parts = [f"Completed ({status_enum.value})."]
                if summary: 
                    update_parts.append(f"Summary: {summary}")
                if tool_result is not None:
                    if isinstance(tool_result, dict): 
                        update_parts.append(f"Result: [Dict with {len(tool_result)} keys]")
                    elif isinstance(tool_result, list): 
                        update_parts.append(f"Result: [List with {len(tool_result)} items]")
                    elif tool_result: 
                        update_parts.append("Result: Success")
                    elif tool_result is False: 
                        update_parts.append("Result: Failure")
                    else: 
                        update_parts.append("Result obtained.")
                update_text = " ".join(update_parts)
                new_content = original_content + " " + update_text
                importance_mult = 1.0
                if status_enum == ActionStatus.FAILED: 
                    importance_mult = 1.2
                elif status_enum == ActionStatus.SKIPPED: 
                    importance_mult = 0.8
                await conn.execute(
                    """
                    UPDATE memories
                    SET content = ?,
                        importance = importance * ?,
                        updated_at = ?
                    WHERE memory_id = ?
                    """,
                    (new_content, importance_mult, now_unix, memory_id)
                )
                await MemoryUtils._log_memory_operation(conn, workflow_id, "update_from_action_completion", memory_id, action_id, {"status": status_enum.value, "summary_added": bool(summary)})
                logger.debug(f"Updated linked memory {memory_id} for completed action {action_id}")
            else:
                  logger.warning(f"Could not find corresponding action_log memory for completed action {action_id} to update.")

            # --- 6. Commit Transaction ---
            await conn.commit()

            # --- 7. Prepare Result (Format timestamp for output) ---

            result = {
                "action_id": action_id,
                "workflow_id": workflow_id,
                "status": status_enum.value,
                "completed_at": to_iso_z(now_unix),
                "conclusion_thought_id": conclusion_thought_id,
                "success": True,
                "processing_time": time.time() - start_time,
            }

            logger.info(
                f"Completed action {action_id} with status {status_enum.value}",
                emoji_key="white_check_mark",
                duration=result["processing_time"],
            )

            return result

    except ToolInputError:
        raise # Re-raise specific input errors
    except Exception as e:
        logger.error(f"Error recording action completion for {action_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to record action completion: {str(e)}") from e
    

@with_tool_metrics
@with_error_handling
async def get_action_details(
    action_id: Optional[str] = None,
    action_ids: Optional[List[str]] = None,
    include_dependencies: bool = False,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves detailed information about one or more actions.

    Fetch complete details about specific actions by their IDs, either individually
    or in batch. Optionally includes information about action dependencies.

    Args:
        action_id: ID of a single action to retrieve (ignored if action_ids is provided)
        action_ids: Optional list of action IDs to retrieve in batch
        include_dependencies: Whether to include dependency information for each action
        db_path: Path to the SQLite database file

    Returns:
        Dictionary containing action details:
        {
            "actions": [
                {
                    "action_id": "uuid-string",
                    "workflow_id": "workflow-uuid",
                    "action_type": "tool_use",
                    "status": "completed",
                    "title": "Load data",
                    ... other action fields ...
                    "dependencies": { # Only if include_dependencies=True
                        "depends_on": [{"action_id": "action-id-1", "type": "requires"}],
                        "dependent_actions": [{"action_id": "action-id-3", "type": "informs"}]
                    }
                },
                ... more actions if batch ...
            ],
            "success": true,
            "processing_time": 0.123
        }

    Raises:
        ToolInputError: If neither action_id nor action_ids is provided, or if no matching actions found
        ToolError: If database operation fails
    """
    start_time = time.time()

    # Validate inputs
    if not action_id and not action_ids:
        raise ToolInputError("Either action_id or action_ids must be provided", param_name="action_id")

    # Ensure target_action_ids is a list
    target_action_ids = []
    if action_ids:
        if isinstance(action_ids, list):
            target_action_ids = action_ids
        else:
            # Handle potential non-list input gracefully
            logger.warning(f"action_ids provided was not a list ({type(action_ids)}). Attempting to use action_id.")
            if action_id:
                target_action_ids = [action_id]
            else:
                raise ToolInputError("action_ids must be a list or action_id must be provided.", param_name="action_ids")
    elif action_id:
        target_action_ids = [action_id]

    if not target_action_ids: # Should not happen due to initial check, but safeguard
         raise ToolInputError("No valid action IDs specified.", param_name="action_id")


    try:
        async with DBConnection(db_path) as conn:
            placeholders = ', '.join(['?'] * len(target_action_ids))
            # Ensure the query correctly joins tags and groups
            select_query = f"""
                SELECT a.*, GROUP_CONCAT(DISTINCT t.name) as tags_str
                FROM actions a
                LEFT JOIN action_tags at ON a.action_id = at.action_id
                LEFT JOIN tags t ON at.tag_id = t.tag_id
                WHERE a.action_id IN ({placeholders})
                GROUP BY a.action_id
            """

            actions_result = []
            cursor = await conn.execute(select_query, target_action_ids)
            # Iterate using async for
            async for row in cursor:
                # Convert row to dict for easier manipulation
                action_data = dict(row)

                # Format timestamps
                if action_data.get("started_at"):
                    action_data["started_at"] = to_iso_z(action_data["started_at"])
                if action_data.get("completed_at"):
                    action_data["completed_at"] = to_iso_z(action_data["completed_at"])

                # Process tags
                if action_data.get("tags_str"):
                    action_data["tags"] = action_data["tags_str"].split(',')
                else:
                    action_data["tags"] = []
                action_data.pop("tags_str", None) # Remove the intermediate column

                if action_data.get("tool_args"):
                    action_data["tool_args"] = await MemoryUtils.deserialize(action_data["tool_args"])
                if action_data.get("tool_result"):
                    action_data["tool_result"] = await MemoryUtils.deserialize(action_data["tool_result"])

                # Include dependencies if requested
                if include_dependencies:
                    action_data["dependencies"] = {"depends_on": [], "dependent_actions": []}
                    # Fetch actions this one depends ON (target_action_id is the dependency)
                    dep_cursor_on = await conn.execute(
                        "SELECT target_action_id, dependency_type FROM dependencies WHERE source_action_id = ?",
                        (action_data["action_id"],)
                    )
                    depends_on_rows = await dep_cursor_on.fetchall()
                    await dep_cursor_on.close()
                    action_data["dependencies"]["depends_on"] = [{"action_id": r["target_action_id"], "type": r["dependency_type"]} for r in depends_on_rows]

                    # Fetch actions that depend ON this one (source_action_id depends on this)
                    dep_cursor_by = await conn.execute(
                        "SELECT source_action_id, dependency_type FROM dependencies WHERE target_action_id = ?",
                        (action_data["action_id"],)
                    )
                    dependent_rows = await dep_cursor_by.fetchall()
                    await dep_cursor_by.close()
                    action_data["dependencies"]["dependent_actions"] = [{"action_id": r["source_action_id"], "type": r["dependency_type"]} for r in dependent_rows]


                actions_result.append(action_data)
            await cursor.close() # Close the main cursor

            if not actions_result:
                action_ids_str = ", ".join(target_action_ids[:5]) + ("..." if len(target_action_ids) > 5 else "")
                raise ToolInputError(f"No actions found with IDs: {action_ids_str}", param_name="action_id" if action_id else "action_ids")

            processing_time = time.time() - start_time
            logger.info(f"Retrieved details for {len(actions_result)} actions", emoji_key="search", time=processing_time)

            result = {
                "actions": actions_result,
                "success": True,
                "processing_time": processing_time
            }
            return result

    except ToolInputError:
        raise # Re-raise specific input errors
    except Exception as e:
        logger.error(f"Error retrieving action details: {e}", exc_info=True)
        raise ToolError(f"Failed to retrieve action details: {str(e)}") from e

# ======================================================
# Contextual Summarization (Used in Agent Context Compression)
# ======================================================

@with_tool_metrics
@with_error_handling
async def summarize_context_block(
    text_to_summarize: str,
    target_tokens: int = 500,
    context_type: str = "actions",  # "actions", "memories", "thoughts", etc.
    workflow_id: Optional[str] = None,
    provider: str = LLMGatewayProvider.ANTHROPIC.value, # Use enum/constant for default
    model: Optional[str] = "claude-3-5-haiku-20241022", # Default model
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Summarizes a specific block of context for an agent, optimized for preserving key information.

    A specialized version of summarize_text designed specifically for compressing agent context
    blocks like action histories, memory sets, or thought chains. Uses optimized prompting
    based on context_type to preserve the most relevant information for agent decision-making.

    Args:
        text_to_summarize: Context block text to summarize
        target_tokens: Desired length of summary (default 500)
        context_type: Type of context being summarized (affects prompting)
        workflow_id: Optional workflow ID for logging
        provider: (Optional) LLM provider to use (e.g., 'openai', 'anthropic').
                  Default 'anthropic'.
        model: (Optional) Specific LLM model name (e.g., 'gpt-4.1-mini',
               'claude-3-5-haiku-20241022'). If None, uses provider's default.
               Default 'claude-3-5-haiku-20241022'.
        db_path: Path to the SQLite database file

    Returns:
        Dictionary containing the generated summary:
        {
            "summary": "Concise context summary...",
            "context_type": "actions",
            "compression_ratio": 0.25,  # ratio of summary length to original length
            "success": true,
            "processing_time": 0.123
        }

    Raises:
        ToolInputError: If text_to_summarize is empty
        ToolError: If summarization fails or provider is invalid
    """
    start_time = time.time()

    if not text_to_summarize:
        raise ToolInputError("Text to summarize cannot be empty", param_name="text_to_summarize")

    # Select appropriate prompt template based on context type
    if context_type == "actions":
        prompt_template = """
You are an expert context summarizer for an AI agent. Your task is to summarize the following ACTION HISTORY logs
while preserving the most important information for the agent to maintain situational awareness.

For actions, focus on:
1. Key actions that changed state or produced important outputs
2. Failed actions and their error reasons
3. The most recent 2-3 actions regardless of importance
4. Any actions that created artifacts or memories
5. Sequential relationships between actions

Produce a VERY CONCISE summary that maintains the chronological flow and preserves action IDs
when referring to specific actions. Aim for approximately {target_tokens} tokens.

ACTION HISTORY TO SUMMARIZE:
{text_to_summarize}

CONCISE ACTION HISTORY SUMMARY:
"""
    elif context_type == "memories":
        prompt_template = """
You are an expert context summarizer for an AI agent. Your task is to summarize the following MEMORY ENTRIES
while preserving the most important information for the agent to maintain understanding.

For memories, focus on:
1. High importance memories (importance > 7)
2. High confidence memories (confidence > 0.8)
3. Insights and facts over observations
4. Memory IDs should be preserved when referring to specific memories
5. Connected memories that form knowledge networks

Produce a VERY CONCISE summary that preserves the key information, high-value insights, and
critical relationships. Aim for approximately {target_tokens} tokens.

MEMORY ENTRIES TO SUMMARIZE:
{text_to_summarize}

CONCISE MEMORY SUMMARY:
"""
    elif context_type == "thoughts":
        prompt_template = """
You are an expert context summarizer for an AI agent. Your task is to summarize the following THOUGHT CHAINS
while preserving the reasoning, decisions, and insights.

For thoughts, focus on:
1. Goals, decisions, and conclusions
2. Key hypotheses and critical reflections
3. The most recent thoughts that may affect current reasoning
4. Thought IDs should be preserved when referring to specific thoughts

Produce a VERY CONCISE summary that captures the agent's reasoning process and main insights.
Aim for approximately {target_tokens} tokens.

THOUGHT CHAINS TO SUMMARIZE:
{text_to_summarize}

CONCISE THOUGHT SUMMARY:
"""
    else:
        # Generic template for other context types
        prompt_template = """
You are an expert context summarizer for an AI agent. Your task is to create a concise summary of the following text
while preserving the most important information for the agent to maintain awareness and functionality.

Focus on information that is:
1. Recent and relevant to current goals
2. Critical for understanding the current state
3. Containing unique identifiers that need to be preserved
4. Representing significant events, insights, or patterns

Produce a VERY CONCISE summary that maximizes the agent's ability to operate with this reduced context.
Aim for approximately {target_tokens} tokens.

TEXT TO SUMMARIZE:
{text_to_summarize}

CONCISE SUMMARY:
"""

    try:
        # Get provider instance using the function argument
        provider_instance = await get_provider(provider)
        if not provider_instance:
            raise ToolError(f"Failed to initialize provider '{provider}'. Check configuration.")

        # Prepare prompt
        prompt = prompt_template.format(
            text_to_summarize=text_to_summarize,
            target_tokens=target_tokens
        )

        # Use the model parameter passed to the function.
        # The get_provider instance might handle None model by using its default,
        # or we rely on the default value set in the function signature if None is passed.
        model_to_use = model # Pass the model argument (which defaults if None wasn't explicitly passed)

        # Generate summary
        generation_result = await provider_instance.generate_completion(
            prompt=prompt,
            model=model_to_use, # Use the variable holding the desired model
            max_tokens=target_tokens + 50,  # Add some buffer for prompt tokens
            temperature=0.2  # Lower temperature for more deterministic summaries
        )

        summary_text = generation_result.text.strip()
        if not summary_text:
            raise ToolError("LLM returned empty context summary.")

        # Calculate compression ratio
        # Avoid division by zero if text_to_summarize is empty (although checked earlier)
        original_length = max(1, len(text_to_summarize))
        compression_ratio = len(summary_text) / original_length

        # Log the operation if workflow_id provided
        if workflow_id:
            async with DBConnection(db_path) as conn:
                await MemoryUtils._log_memory_operation(
                    conn, workflow_id, "compress_context", None, None,
                    {
                        "context_type": context_type,
                        "original_length": len(text_to_summarize),
                        "summary_length": len(summary_text),
                        "compression_ratio": compression_ratio,
                        "provider": provider, # Log the provider used
                        "model": model_to_use # Log the model used
                    }
                )
                await conn.commit()

        processing_time = time.time() - start_time
        logger.info(
            f"Compressed {context_type} context: {len(text_to_summarize)} -> {len(summary_text)} chars (Ratio: {compression_ratio:.2f}, LLM: {provider}/{model_to_use or 'default'})",
            emoji_key="compression", time=processing_time
        )

        return {
            "summary": summary_text,
            "context_type": context_type,
            "compression_ratio": compression_ratio,
            "success": True,
            "processing_time": processing_time
        }

    except ToolInputError:
        raise # Re-raise specific input errors
    except Exception as e:
        logger.error(f"Error summarizing context block: {e}", exc_info=True)
        raise ToolError(f"Failed to summarize context block: {str(e)}") from e
            
# ======================================================
# 3.5 Action Dependency Tools
# ======================================================

@with_tool_metrics
@with_error_handling
async def add_action_dependency(
    source_action_id: str,
    target_action_id: str,
    dependency_type: str = "requires", # e.g., requires, informs, blocks
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Records a dependency between two actions within the same workflow.

    Use this during planning or reflection to explicitly state relationships, like:
    - Action B 'requires' the output of Action A.
    - Action C 'informs' the decision made in Action D.
    - Action E 'blocks' Action F until E is complete.

    Args:
        source_action_id: The ID of the action that depends on the target action.
        target_action_id: The ID of the action that the source action depends upon.
        dependency_type: (Optional) Describes the nature of the dependency (e.g., 'requires', 'informs', 'blocks'). Default 'requires'.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary confirming the dependency creation.
        {
            "source_action_id": "source-uuid",
            "target_action_id": "target-uuid",
            "dependency_type": "requires",
            "dependency_id": 123, # Auto-incremented ID
            "created_at": "iso-timestamp",
            "success": true,
            "processing_time": 0.04
        }

    Raises:
        ToolInputError: If IDs are missing, the same, actions not found, or actions belong to different workflows.
        ToolError: If the database operation fails.
    """
    if not source_action_id:
        raise ToolInputError("Source action ID required.", param_name="source_action_id")
    if not target_action_id:
        raise ToolInputError("Target action ID required.", param_name="target_action_id")
    if source_action_id == target_action_id:
        raise ToolInputError("Source and target action IDs cannot be the same.", param_name="source_action_id")
    if not dependency_type:
        raise ToolInputError("Dependency type cannot be empty.", param_name="dependency_type")

    start_time = time.time()
    now_unix = int(time.time()) 

    try:
        async with DBConnection(db_path) as conn:
            # --- Validate Actions & Workflow Consistency ---
            source_workflow_id = None
            target_workflow_id = None
            cursor = await conn.execute("SELECT workflow_id FROM actions WHERE action_id = ?", (source_action_id,))
            source_row = await cursor.fetchone()
            await cursor.close()
            if not source_row:
                 raise ToolInputError(f"Source action {source_action_id} not found.", param_name="source_action_id")
            source_workflow_id = source_row["workflow_id"]

            cursor = await conn.execute("SELECT workflow_id FROM actions WHERE action_id = ?", (target_action_id,))
            target_row = await cursor.fetchone()
            await cursor.close()
            if not target_row:
                 raise ToolInputError(f"Target action {target_action_id} not found.", param_name="target_action_id")
            target_workflow_id = target_row["workflow_id"]

            if source_workflow_id != target_workflow_id:
                raise ToolInputError(
                    f"Source action ({source_action_id}) and target action ({target_action_id}) belong to different workflows.",
                    param_name="target_action_id"
                )
            workflow_id = source_workflow_id # Both actions are in this workflow

            # --- Insert Dependency (Ignoring duplicates) ---
            dependency_id = None
            cursor = await conn.execute( # Use explicit cursor variable
                """
                INSERT OR IGNORE INTO dependencies
                (source_action_id, target_action_id, dependency_type, created_at)
                VALUES (?, ?, ?, ?)
                """,
                (source_action_id, target_action_id, dependency_type, now_unix) # *** Use now_unix ***
            )
            # Check if a row was actually inserted
            if cursor.rowcount > 0:
                dependency_id = cursor.lastrowid
                logger.debug(f"Inserted new dependency row with ID: {dependency_id}")
            else:
                # If IGNORE occurred, fetch the existing dependency_id
                existing_cursor = await conn.execute( # Use different cursor variable
                     "SELECT dependency_id FROM dependencies WHERE source_action_id = ? AND target_action_id = ? AND dependency_type = ?",
                     (source_action_id, target_action_id, dependency_type)
                )
                existing_row = await existing_cursor.fetchone()
                await existing_cursor.close()
                if existing_row:
                    dependency_id = existing_row["dependency_id"]
                    logger.debug(f"Dependency already existed. Retrieved existing ID: {dependency_id}")
                else:
                    logger.warning(f"Dependency insert was ignored, but couldn't retrieve existing row for ({source_action_id}, {target_action_id}, {dependency_type})")

            await cursor.close() # Close the main insertion cursor

            # --- Update Workflow Timestamp ---
            await conn.execute(
                "UPDATE workflows SET updated_at = ?, last_active = ? WHERE workflow_id = ?",
                (now_unix, now_unix, workflow_id)
            )

            # Log operation (even if ignored, log the attempt)
            log_data = {
                "source_action_id": source_action_id, "target_action_id": target_action_id,
                "dependency_type": dependency_type, "db_dependency_id": dependency_id
            }
            await MemoryUtils._log_memory_operation(conn, workflow_id, "add_dependency", None, source_action_id, log_data)

            await conn.commit()

            processing_time = time.time() - start_time
            logger.info(
                f"Added dependency ({dependency_type}) from {source_action_id} to {target_action_id}",
                emoji_key="link",
            )

            return {
                "source_action_id": source_action_id,
                "target_action_id": target_action_id,
                "dependency_type": dependency_type,
                "dependency_id": dependency_id,  # May be None if IGNORE failed lookup
                "created_at": to_iso_z(now_unix),     
                "success": True,
                "processing_time": processing_time,
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error adding action dependency: {e}", exc_info=True)
        raise ToolError(f"Failed to add action dependency: {str(e)}") from e


@with_tool_metrics
@with_error_handling
async def get_action_dependencies(
    action_id: str,
    direction: str = "downstream", # "downstream" (depends on this) or "upstream" (this depends on)
    dependency_type: Optional[str] = None,
    include_details: bool = False, # Whether to fetch full action details
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves actions that depend on the given action (downstream) or actions the given action depends on (upstream).

    Use this to understand the relationship between actions in a workflow.
    - direction='downstream': Find actions that need this one to complete first ('get_dependent_actions').
    - direction='upstream': Find actions that this one needs to complete first ('get_action_prerequisites').

    Args:
        action_id: The ID of the action to query dependencies for.
        direction: (Optional) 'downstream' (actions depending on this one) or 'upstream' (actions this one depends on). Default 'downstream'.
        dependency_type: (Optional) Filter by the type of dependency (e.g., 'requires').
        include_details: (Optional) If True, returns full details of the related actions, otherwise just their IDs and titles. Default False.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing a list of dependent or prerequisite actions.
        {
            "action_id": "query-action-uuid",
            "direction": "downstream" | "upstream",
            "related_actions": [
                {
                    "action_id": "related-action-uuid",
                    "title": "Related Action Title",
                    "dependency_type": "requires",
                    # ... more details if include_details=True ...
                },
                ...
            ],
            "success": true,
            "processing_time": 0.06
        }

    Raises:
        ToolInputError: If action ID not found or direction is invalid.
        ToolError: If the database operation fails.
    """
    if not action_id: 
        raise ToolInputError("Action ID required.", param_name="action_id")
    if direction not in ["downstream", "upstream"]: 
        raise ToolInputError("Direction must be 'downstream' or 'upstream'.", param_name="direction")

    start_time = time.time()

    try:
        async with DBConnection(db_path) as conn:
            cursor = await conn.execute("SELECT 1 FROM actions WHERE action_id = ?", (action_id,))
            action_exists = await cursor.fetchone()
            await cursor.close()
            if not action_exists: 
                raise ToolInputError(f"Action {action_id} not found.", param_name="action_id")

            select_cols = "a.action_id, a.title, dep.dependency_type"
            if include_details:
                 # Fetch timestamps as integers
                 select_cols += ", a.action_type, a.status, a.started_at, a.completed_at, a.sequence_number"

            if direction == "downstream":
                 query = f"SELECT {select_cols} FROM dependencies dep JOIN actions a ON dep.source_action_id = a.action_id WHERE dep.target_action_id = ?"
                 params = [action_id]
            else: # upstream
                 query = f"SELECT {select_cols} FROM dependencies dep JOIN actions a ON dep.target_action_id = a.action_id WHERE dep.source_action_id = ?"
                 params = [action_id]

            if dependency_type:
                 query += " AND dep.dependency_type = ?"
                 params.append(dependency_type)

            query += " ORDER BY a.sequence_number ASC"

            related_actions = []
            cursor = await conn.execute(query, params)
            async for row in cursor:
                action_data = dict(row)
                if include_details:
                    if action_data.get("started_at"):
                        action_data["started_at"] = to_iso_z(action_data["started_at"])
                    if action_data.get("completed_at"):
                        action_data["completed_at"] = to_iso_z(action_data["completed_at"])
                related_actions.append(action_data)
            await cursor.close()

            processing_time = time.time() - start_time
            logger.info(f"Retrieved {len(related_actions)} {direction} dependencies for action {action_id}", emoji_key="left_right_arrow")
            return {
                "action_id": action_id,
                "direction": direction,
                "related_actions": related_actions,
                "success": True,
                "processing_time": processing_time
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error getting action dependencies: {e}", exc_info=True)
        raise ToolError(f"Failed to get action dependencies: {str(e)}") from e

# --- 4. Artifact Tracking Tools (Ported/Adapted from agent_memory & Integrated) ---
@with_tool_metrics
@with_error_handling
async def record_artifact(
    workflow_id: str,
    name: str,
    artifact_type: str,
    action_id: Optional[str] = None,
    description: Optional[str] = None,
    path: Optional[str] = None,
    content: Optional[str] = None,
    metadata: Optional[Dict[str, Any]] = None,
    is_output: bool = False,
    tags: Optional[List[str]] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Records information about an artifact created during a workflow
       and creates a corresponding linked episodic memory entry.

    Use this tool to keep track of files, code, data, or other outputs generated during your
    workflow. This creates a persistent record of these artifacts that you can reference later
    and include in reports. It also creates a memory entry about the artifact's creation.

    Args:
        workflow_id: The ID of the workflow this artifact belongs to.
        name: A descriptive name for the artifact.
        artifact_type: Type of artifact. Use a value from ArtifactType enum: 'file', 'text',
                      'image', 'table', 'chart', 'code', 'data', 'json', 'url'.
        action_id: (Optional) The ID of the action that created this artifact.
        description: (Optional) A detailed description of the artifact's purpose or contents.
        path: (Optional) Filesystem path to the artifact if it's a file.
        content: (Optional) The content of the artifact if it's text-based. Can be large.
        metadata: (Optional) Additional structured information about the artifact.
        is_output: (Optional) Whether this is a final output of the workflow. Default False.
        tags: (Optional) List of tags to categorize this artifact.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        A dictionary containing information about the recorded artifact and linked memory.
        {
            "artifact_id": "artifact-uuid",
            "workflow_id": "workflow-uuid",
            "name": "requirements.txt",
            "artifact_type": "file",
            "path": "/path/to/requirements.txt",
            "created_at": "iso-timestamp",
            "is_output": false,
            "tags": ["dependency"],
            "linked_memory_id": "memory-uuid", # ID of the memory entry about this artifact
            "success": true,
            "processing_time": 0.09
        }

    Raises:
        ToolInputError: If required parameters are missing or invalid.
        ToolError: If the database operation fails.
    """
    start_time = time.time()
    # --- Input Validation ---
    if not name:
        raise ToolInputError("Artifact name required", param_name="name")
    try:
        artifact_type_enum = ArtifactType(artifact_type.lower())
    except ValueError as e:
        valid_types = [t.value for t in ArtifactType]
        raise ToolInputError(f"Invalid artifact_type '{artifact_type}'. Must be one of: {', '.join(valid_types)}", param_name="artifact_type") from e

    artifact_id = MemoryUtils.generate_id()
    now_unix = int(time.time())

    try:
        async with DBConnection(db_path) as conn:
            # --- Existence Checks ---
            cursor = await conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,))
            wf_exists = await cursor.fetchone()
            await cursor.close()
            if not wf_exists:
                raise ToolInputError(f"Workflow not found: {workflow_id}", param_name="workflow_id")
            if action_id:
                cursor = await conn.execute("SELECT 1 FROM actions WHERE action_id = ? AND workflow_id = ?", (action_id, workflow_id))
                action_exists = await cursor.fetchone()
                await cursor.close()
                if not action_exists:
                    raise ToolInputError(f"Action {action_id} not found or does not belong to workflow {workflow_id}", param_name="action_id")

            # --- Prepare Data ---
            metadata_json = await MemoryUtils.serialize(metadata)
            db_content = None
            if content:
                content_bytes = content.encode('utf-8')
                if len(content_bytes) > MAX_TEXT_LENGTH:
                    logger.warning(f"Artifact content for '{name}' exceeds max length ({MAX_TEXT_LENGTH} bytes). Storing truncated version in DB.")
                    truncated_bytes = content_bytes[:MAX_TEXT_LENGTH]
                    db_content = truncated_bytes.decode('utf-8', errors='replace')
                    if db_content.endswith('\ufffd') and MAX_TEXT_LENGTH > 1:
                         db_content_shorter = content_bytes[:MAX_TEXT_LENGTH-1].decode('utf-8', errors='replace')
                         if not db_content_shorter.endswith('\ufffd'):
                              db_content = db_content_shorter
                    db_content += "..."
                else:
                    db_content = content

            # --- Insert Artifact Record ---
            await conn.execute(
                """
                INSERT INTO artifacts (artifact_id, workflow_id, action_id, artifact_type, name,
                description, path, content, metadata, created_at, is_output)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (artifact_id, workflow_id, action_id, artifact_type_enum.value, name,
                 description, path, db_content, metadata_json, now_unix, is_output) # *** Use now_unix ***
            )
            logger.debug(f"Inserted artifact record {artifact_id}")

            # --- Process Tags ---
            artifact_tags = tags or []
            await MemoryUtils.process_tags(conn, artifact_id, artifact_tags, "artifact")
            logger.debug(f"Processed {len(artifact_tags)} tags for artifact {artifact_id}")

            # --- Update Workflow Timestamp ---
            await conn.execute("UPDATE workflows SET updated_at = ?, last_active = ? WHERE workflow_id = ?", (now_unix, now_unix, workflow_id)) # *** Use now_unix ***

            # --- Create Linked Episodic Memory about the Artifact Creation ---
            memory_id = MemoryUtils.generate_id()
            memory_content = f"Artifact '{name}' (type: {artifact_type_enum.value}) was created"
            if action_id: 
                memory_content += f" during action '{action_id[:8]}...'"
            if description: 
                memory_content += f". Description: {description[:100]}..."
            if path: 
                memory_content += f". Located at: {path}"
            elif db_content and db_content != content: 
                memory_content += ". Content stored (truncated)."
            elif content: 
                memory_content += ". Content stored directly."
            if is_output: 
                memory_content += ". Marked as a final workflow output."
            mem_tags = list(set(["artifact_creation", artifact_type_enum.value] + artifact_tags))
            mem_importance = 6.0 if is_output else 5.0

            await conn.execute(
                 """
                 INSERT INTO memories (memory_id, workflow_id, action_id, artifact_id, content, memory_level, memory_type,
                 importance, confidence, tags, created_at, updated_at, access_count)
                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                 """,
                 (memory_id, workflow_id, action_id, artifact_id, memory_content, MemoryLevel.EPISODIC.value, MemoryType.ARTIFACT_CREATION.value,
                  mem_importance, 1.0, json.dumps(mem_tags), now_unix, now_unix, 0) # Memories already use Unix timestamps
            )
            logger.debug(f"Inserted linked memory record {memory_id} for artifact {artifact_id}")

            # --- Log Operations ---
            await MemoryUtils._log_memory_operation(
                 conn, workflow_id, "create_artifact", None, action_id,
                 {"artifact_id": artifact_id, "name": name, "type": artifact_type_enum.value}
            )
            await MemoryUtils._log_memory_operation(
                conn, workflow_id, "create_from_artifact", memory_id, action_id,
                {"artifact_id": artifact_id}
            )

            # --- Commit Transaction ---
            await conn.commit()

            # --- Prepare Result (Format timestamp for output) ---

            result = {
                "artifact_id": artifact_id,
                "workflow_id": workflow_id,
                "name": name,
                "artifact_type": artifact_type_enum.value,
                "path": path,
                "content_stored_in_db": bool(db_content),
                "created_at": to_iso_z(now_unix),
                "is_output": is_output,
                "tags": artifact_tags,
                "linked_memory_id": memory_id,
                "success": True,
                "processing_time": time.time() - start_time,
            }

            logger.info(
                f"Recorded artifact '{name}' ({artifact_id}) and linked memory {memory_id} in workflow {workflow_id}",
                emoji_key="package",
            )

            return result
    except ToolInputError:
        raise # Re-raise specific input errors
    except Exception as e:
        logger.error(f"Error recording artifact: {e}", exc_info=True)
        raise ToolError(f"Failed to record artifact: {str(e)}") from e
    

# --- 5. Thought & Reasoning Tools (Ported/Adapted from agent_memory & Integrated) ---
@with_tool_metrics
@with_error_handling
async def record_thought(
    workflow_id: str,
    content: str,
    thought_type: str = "inference",
    thought_chain_id: Optional[str] = None,
    parent_thought_id: Optional[str] = None,
    relevant_action_id: Optional[str] = None,
    relevant_artifact_id: Optional[str] = None,
    relevant_memory_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH,
    conn: Optional[aiosqlite.Connection] = None # Optional connection for transactions
) -> Dict[str, Any]:
    """Records a thought in a reasoning chain, potentially linking to memory and creating an associated memory entry.

    If an existing database connection (`conn`) is provided, this function will use it
    and operate within the existing transaction context (no internal commit). Otherwise,
    it will acquire a new connection and manage its own transaction.

    Args:
        workflow_id: The ID of the workflow this thought belongs to.
        content: The textual content of the thought.
        thought_type: (Optional) Type of thought (e.g., 'goal', 'plan', 'inference'). Default 'inference'.
        thought_chain_id: (Optional) ID of the chain to add to. If None, adds to the primary chain.
        parent_thought_id: (Optional) ID of the parent thought in the chain.
        relevant_action_id: (Optional) ID of an action this thought relates to.
        relevant_artifact_id: (Optional) ID of an artifact this thought relates to.
        relevant_memory_id: (Optional) ID of a memory this thought relates to.
        db_path: (Optional) Path to the SQLite database file. Used if `conn` is not provided.
        conn: (Optional) An existing `aiosqlite.Connection` to use for database operations.
              If provided, commit/rollback is handled externally.

    Returns:
        Dictionary containing information about the recorded thought.
        Timestamps are returned as ISO 8601 strings.
        {
            "thought_id": "uuid-string",
            "thought_chain_id": "uuid-string",
            "thought_type": "inference",
            "content": "Thought content...",
            "sequence_number": 5,
            "created_at": "iso-timestampZ",
            "linked_memory_id": "uuid-string" | None,
            "success": true
        }

    Raises:
        ToolInputError: If required parameters are missing or invalid, or referenced entities don't exist.
        ToolError: If the database operation fails.
    """
    # --- Input Validation ---
    if not content or not isinstance(content, str):
        raise ToolInputError("Thought content must be a non-empty string", param_name="content")

    try:
        thought_type_enum = ThoughtType(thought_type.lower())
    except ValueError as e:
        valid_types = [t.value for t in ThoughtType]
        raise ToolInputError(
            f"Invalid thought_type '{thought_type}'. Must be one of: {', '.join(valid_types)}",
            param_name="thought_type"
        ) from e

    thought_id = MemoryUtils.generate_id()
    now_unix = int(time.time())
    linked_memory_id = None # Initialize

    async def _perform_db_operations(db_conn: aiosqlite.Connection):
        """Inner function to perform DB ops using the provided connection."""
        nonlocal linked_memory_id # Allow modification of outer scope variable

        # --- Existence Checks for Foreign Keys ---
        cursor = await db_conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,))
        wf_exists = await cursor.fetchone()
        await cursor.close()
        if not wf_exists:
            raise ToolInputError(f"Workflow not found: {workflow_id}", param_name="workflow_id")

        if parent_thought_id:
             cursor = await db_conn.execute("SELECT 1 FROM thoughts WHERE thought_id = ?", (parent_thought_id,))
             pthought_exists = await cursor.fetchone()
             await cursor.close()
             if not pthought_exists:
                 raise ToolInputError(f"Parent thought not found: {parent_thought_id}", param_name="parent_thought_id")

        if relevant_action_id:
             cursor = await db_conn.execute("SELECT 1 FROM actions WHERE action_id = ?", (relevant_action_id,))
             raction_exists = await cursor.fetchone()
             await cursor.close()
             if not raction_exists:
                 raise ToolInputError(f"Relevant action not found: {relevant_action_id}", param_name="relevant_action_id")

        if relevant_artifact_id:
             cursor = await db_conn.execute("SELECT 1 FROM artifacts WHERE artifact_id = ?", (relevant_artifact_id,))
             rartifact_exists = await cursor.fetchone()
             await cursor.close()
             if not rartifact_exists:
                 raise ToolInputError(f"Relevant artifact not found: {relevant_artifact_id}", param_name="relevant_artifact_id")

        if relevant_memory_id:
             cursor = await db_conn.execute("SELECT 1 FROM memories WHERE memory_id = ?", (relevant_memory_id,))
             rmemory_exists = await cursor.fetchone()
             await cursor.close()
             if not rmemory_exists:
                 raise ToolInputError(f"Relevant memory not found: {relevant_memory_id}", param_name="relevant_memory_id")

        # --- Determine Target Thought Chain ---
        target_thought_chain_id = thought_chain_id
        if not target_thought_chain_id:
            cursor = await db_conn.execute("SELECT thought_chain_id FROM thought_chains WHERE workflow_id = ? ORDER BY created_at ASC LIMIT 1", (workflow_id,))
            row = await cursor.fetchone()
            await cursor.close()
            if not row:
                # If running within a transaction (conn provided), creating the chain here might be complex.
                # Assume for now the primary chain should exist if adding thoughts without specifying one.
                # A more robust approach might involve ensuring chain creation happens before thought recording.
                if conn: # Check if running in external transaction
                     raise ToolError(f"Primary thought chain for workflow {workflow_id} not found. Cannot auto-create within existing transaction.")
                else:
                     # If running standalone, we can create it.
                     target_thought_chain_id = MemoryUtils.generate_id()
                     logger.info(f"No existing thought chain found for workflow {workflow_id}, creating default.")
                     await db_conn.execute(
                         "INSERT INTO thought_chains (thought_chain_id, workflow_id, title, created_at) VALUES (?, ?, ?, ?)",
                         (target_thought_chain_id, workflow_id, "Main reasoning", now_unix)
                     )
            else:
                target_thought_chain_id = row["thought_chain_id"]
        else:
            cursor = await db_conn.execute("SELECT 1 FROM thought_chains WHERE thought_chain_id = ? AND workflow_id = ?", (target_thought_chain_id, workflow_id))
            chain_exists = await cursor.fetchone()
            await cursor.close()
            if not chain_exists:
                raise ToolInputError(f"Provided thought chain {target_thought_chain_id} not found or does not belong to workflow {workflow_id}", param_name="thought_chain_id")

        # --- Get Sequence Number ---
        sequence_number = await MemoryUtils.get_next_sequence_number(db_conn, target_thought_chain_id, "thoughts", "thought_chain_id")

        # --- Insert Thought Record ---
        await db_conn.execute(
            """
            INSERT INTO thoughts (
                thought_id, thought_chain_id, parent_thought_id, thought_type, content,
                sequence_number, created_at, relevant_action_id, relevant_artifact_id, relevant_memory_id
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                thought_id, target_thought_chain_id, parent_thought_id, thought_type_enum.value, content,
                sequence_number, now_unix, relevant_action_id, relevant_artifact_id, relevant_memory_id
            )
        )

        # --- Update Workflow Timestamp ---
        await db_conn.execute(
            "UPDATE workflows SET updated_at = ?, last_active = ? WHERE workflow_id = ?",
            (now_unix, now_unix, workflow_id)
        )

        # --- Create Linked Memory for Important Thoughts ---
        important_thought_types = [
            ThoughtType.GOAL.value, ThoughtType.DECISION.value, ThoughtType.SUMMARY.value,
            ThoughtType.REFLECTION.value, ThoughtType.HYPOTHESIS.value, ThoughtType.INSIGHT.value # Added insight type to the list
        ]
        # Check if ThoughtType.INSIGHT is defined in the enum, if not remove it
        if not hasattr(ThoughtType, 'INSIGHT'):
            if ThoughtType.INSIGHT.value in important_thought_types:
                important_thought_types.remove(ThoughtType.INSIGHT.value)


        if thought_type_enum.value in important_thought_types:
            linked_memory_id = MemoryUtils.generate_id()
            mem_content = f"Thought [{sequence_number}] ({thought_type_enum.value.capitalize()}): {content}"
            mem_tags = ["reasoning", thought_type_enum.value]
            mem_importance = 7.5 if thought_type_enum.value in [ThoughtType.GOAL.value, ThoughtType.DECISION.value] else 6.5

            await db_conn.execute(
                 """
                 INSERT INTO memories (
                     memory_id, workflow_id, thought_id, content, memory_level, memory_type,
                     importance, confidence, tags, created_at, updated_at, access_count
                 )
                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                 """,
                 (
                     linked_memory_id, workflow_id, thought_id, mem_content,
                     MemoryLevel.SEMANTIC.value, MemoryType.REASONING_STEP.value,
                     mem_importance, 1.0, json.dumps(mem_tags), now_unix, now_unix, 0
                 )
            )
            await MemoryUtils._log_memory_operation(
                db_conn, workflow_id, "create_from_thought", linked_memory_id, None, {"thought_id": thought_id}
            )
        return target_thought_chain_id, sequence_number # Return values needed for result dict

    try:
        target_thought_chain_id_res = None
        sequence_number_res = None

        if conn:
            # Use the provided connection within the existing transaction
            target_thought_chain_id_res, sequence_number_res = await _perform_db_operations(conn)
            # --- NO COMMIT HERE - Handled by the outer transaction manager ---
        else:
            # Manage connection and transaction locally
            db_manager = DBConnection(db_path)
            async with db_manager.transaction() as local_conn:
                target_thought_chain_id_res, sequence_number_res = await _perform_db_operations(local_conn)
            # --- COMMIT/ROLLBACK handled by the transaction manager ---

        # --- Prepare Result ---
        result = {
            "thought_id": thought_id,
            "thought_chain_id": target_thought_chain_id_res,
            "thought_type": thought_type_enum.value,
            "content": content,
            "sequence_number": sequence_number_res,
            "created_at": to_iso_z(now_unix),
            "linked_memory_id": linked_memory_id,
            "success": True,
        }

        logger.info(
            f"Recorded thought ({thought_type_enum.value}) in workflow {workflow_id}",
            emoji_key="brain",
        )

        return result

    except ToolInputError:
        raise # Re-raise specific input errors
    except Exception as e:
        logger.error(f"Error recording thought: {e}", exc_info=True)
        raise ToolError(f"Failed to record thought: {str(e)}") from e
    

# --- 6. Core Memory Tools (Adapted from cognitive_memory) ---
@with_tool_metrics
@with_error_handling
async def store_memory(
    workflow_id: str,
    content: str,
    memory_type: str,
    memory_level: str = MemoryLevel.EPISODIC.value,
    importance: float = 5.0,
    confidence: float = 1.0,
    description: Optional[str] = None,
    reasoning: Optional[str] = None,
    source: Optional[str] = None,
    tags: Optional[List[str]] = None,
    ttl: Optional[int] = None,
    context_data: Optional[Dict[str, Any]] = None,
    generate_embedding: bool = True, # Flag to control embedding generation
    suggest_links: bool = True, # Flag to control link suggestion (NEW)
    link_suggestion_threshold: float = SIMILARITY_THRESHOLD, # Use constant (NEW)
    max_suggested_links: int = 3, # Limit suggestions (NEW)
    action_id: Optional[str] = None,
    thought_id: Optional[str] = None,
    artifact_id: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Stores a new memory entry, generates embeddings, and suggests semantic links.

    Stores a new piece of knowledge or observation, potentially linking it to actions,
    thoughts, or artifacts. Optionally generates a vector embedding and uses it to
    find and suggest links to existing semantically similar memories within the same workflow.

    Args:
        workflow_id: The ID of the workflow this memory belongs to.
        content: The main content of the memory.
        memory_type: The type classification (e.g., 'observation', 'fact', 'insight'). See MemoryType enum.
        memory_level: (Optional) The memory hierarchy level. Default 'episodic'. See MemoryLevel enum.
        importance: (Optional) Importance score (1.0-10.0). Default 5.0.
        confidence: (Optional) Confidence score (0.0-1.0). Default 1.0.
        description: (Optional) A brief description or title for the memory.
        reasoning: (Optional) Explanation of why this memory is relevant or how it was derived.
        source: (Optional) Origin of the memory (e.g., tool name, user input, filename).
        tags: (Optional) List of keywords for categorization. Type and level are added automatically.
        ttl: (Optional) Time-to-live in seconds (0 for permanent, None for level default).
        context_data: (Optional) Additional JSON-serializable context about the memory's creation.
        generate_embedding: (Optional) Whether to generate a vector embedding. Default True.
        suggest_links: (Optional) Whether to find and suggest links to similar memories. Default True.
        link_suggestion_threshold: (Optional) Min similarity score for suggested links. Default SIMILARITY_THRESHOLD.
        max_suggested_links: (Optional) Max number of links to suggest. Default 3.
        action_id: (Optional) ID of an associated action.
        thought_id: (Optional) ID of an associated thought.
        artifact_id: (Optional) ID of an associated artifact.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing details of the stored memory and any suggested links.
        {
            "memory_id": "uuid",
            "workflow_id": "uuid",
            "memory_level": "episodic",
            "memory_type": "observation",
            "content_preview": "Column A seems...",
            "importance": 6.0,
            "confidence": 1.0,
            "created_at_unix": 1678886400,
            "tags": ["observation", "episodic", ...],
            "embedding_id": "uuid" | None,
            "linked_action_id": "uuid" | None,
            "linked_thought_id": "uuid" | None,
            "linked_artifact_id": "uuid" | None,
            "suggested_links": [ # Included if suggest_links=True and matches found
                {
                    "target_memory_id": "uuid",
                    "target_description": "Similar memory desc...",
                    "target_type": "observation",
                    "similarity": 0.85,
                    "suggested_link_type": "related"
                }, ...
            ],
            "success": true,
            "processing_time": 0.25
        }

    Raises:
        ToolInputError: If required parameters are missing or invalid.
        ToolError: If the database operation fails.
    """
    # Parameter validation
    if not content:
        raise ToolInputError("Content cannot be empty.", param_name="content")
    try:
        mem_type = MemoryType(memory_type.lower())
    except ValueError as e:
        valid_types_str = ', '.join([mt.value for mt in MemoryType])
        raise ToolInputError(f"Invalid memory_type. Use one of: {valid_types_str}", param_name="memory_type") from e
    try:
        mem_level = MemoryLevel(memory_level.lower())
    except ValueError as e:
        valid_levels_str = ', '.join([ml.value for ml in MemoryLevel])
        raise ToolInputError(f"Invalid memory_level. Use one of: {valid_levels_str}", param_name="memory_level") from e
    if not 1.0 <= importance <= 10.0:
        raise ToolInputError("Importance must be 1.0-10.0.", param_name="importance")
    if not 0.0 <= confidence <= 1.0:
        raise ToolInputError("Confidence must be 0.0-1.0.", param_name="confidence")
    if not 0.0 <= link_suggestion_threshold <= 1.0:
        raise ToolInputError("Link suggestion threshold must be 0.0-1.0.", param_name="link_suggestion_threshold")
    if not isinstance(max_suggested_links, int) or max_suggested_links < 0:
        raise ToolInputError("Max suggested links must be a non-negative integer.", param_name="max_suggested_links")


    memory_id = MemoryUtils.generate_id()
    now_unix = int(time.time())
    now_iso = datetime.now(timezone.utc).isoformat()
    start_time = time.time()

    # Prepare tags and TTL
    final_tags = list(set([str(t).lower() for t in (tags or [])] + [mem_type.value, mem_level.value])) # Also add level as tag
    effective_ttl = ttl if ttl is not None else DEFAULT_TTL.get(mem_level.value, 0)

    try:
        async with DBConnection(db_path) as conn:
            # --- 1. Existence checks for foreign keys ---
            async with conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,)) as cursor:
                if not await cursor.fetchone():
                    raise ToolInputError(f"Workflow not found: {workflow_id}", param_name="workflow_id")
            if action_id:
                 async with conn.execute("SELECT 1 FROM actions WHERE action_id = ?", (action_id,)) as cursor:
                     if not await cursor.fetchone():
                         raise ToolInputError(f"Action {action_id} not found", param_name="action_id")
            if thought_id:
                 async with conn.execute("SELECT 1 FROM thoughts WHERE thought_id = ?", (thought_id,)) as cursor:
                     if not await cursor.fetchone():
                         raise ToolInputError(f"Thought {thought_id} not found", param_name="thought_id")
            if artifact_id:
                 async with conn.execute("SELECT 1 FROM artifacts WHERE artifact_id = ?", (artifact_id,)) as cursor:
                     if not await cursor.fetchone():
                         raise ToolInputError(f"Artifact {artifact_id} not found", param_name="artifact_id")

            # --- 2. Insert the main memory record ---
            await conn.execute(
                """
                INSERT INTO memories (memory_id, workflow_id, content, memory_level, memory_type, importance, confidence,
                description, reasoning, source, context, tags, created_at, updated_at, last_accessed, access_count, ttl,
                action_id, thought_id, artifact_id, embedding_id)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, NULL)
                """,
                (memory_id, workflow_id, content, mem_level.value, mem_type.value, importance, confidence,
                 description or "", reasoning or "", source or "",
                 await MemoryUtils.serialize(context_data) if context_data else "{}", json.dumps(final_tags),
                 now_unix, now_unix, None, 0, effective_ttl,
                 action_id, thought_id, artifact_id)
            )

            # --- 3. Generate and store embedding (if requested) ---
            embedding_db_id = None
            embedding_generated_successfully = False
            if generate_embedding:
                text_for_embedding = f"{description}: {content}" if description else content
                try:
                    embedding_db_id = await _store_embedding(conn, memory_id, text_for_embedding)
                    if embedding_db_id:
                        embedding_generated_successfully = True
                        logger.debug(f"Successfully generated embedding {embedding_db_id} for memory {memory_id}")
                    else:
                        logger.warning(f"Embedding generation skipped or failed for memory {memory_id}")
                except Exception as embed_err:
                    logger.error(f"Error during embedding generation/storage for memory {memory_id}: {embed_err}", exc_info=True)

            # --- 4. Suggest Semantic Links (if requested and embedding succeeded) ---
            suggested_links_list = []
            if suggest_links and embedding_generated_successfully and max_suggested_links > 0:
                logger.debug(f"Attempting to find similar memories for link suggestion (threshold={link_suggestion_threshold})...")
                try:
                    # Use the text used for embedding generation for the search
                    text_for_search = f"{description}: {content}" if description else content
                    similar_memories = await _find_similar_memories(
                        conn=conn,
                        query_text=text_for_search,
                        workflow_id=workflow_id, # Limit suggestions to the same workflow
                        limit=max_suggested_links + 1, # Fetch slightly more to filter self
                        threshold=link_suggestion_threshold,
                        memory_level=None # Search across all levels for links initially
                    )

                    if similar_memories:
                        # Fetch details for potential link targets
                        similar_ids = [sim_id for sim_id, _ in similar_memories if sim_id != memory_id] # Exclude self
                        if similar_ids:
                            placeholders = ','.join('?' * len(similar_ids))
                            async with conn.execute(f"SELECT memory_id, description, memory_type FROM memories WHERE memory_id IN ({placeholders})", similar_ids) as cursor:
                                 target_details = {row["memory_id"]: dict(row) for row in await cursor.fetchall()}

                            # Format suggestions
                            score_map = dict(similar_memories)
                            for sim_id in similar_ids:
                                if sim_id in target_details:
                                    details = target_details[sim_id]
                                    similarity = score_map.get(sim_id, 0.0)
                                    # Basic suggested link type logic (can be expanded)
                                    suggested_type = LinkType.RELATED.value
                                    if mem_type.value == details.get('memory_type'):
                                        suggested_type = LinkType.SEQUENTIAL.value if mem_level.value == MemoryLevel.EPISODIC.value else LinkType.SUPPORTS.value
                                    elif mem_type.value == MemoryType.INSIGHT.value and details.get('memory_type') == MemoryType.FACT.value:
                                         suggested_type = LinkType.GENERALIZES.value

                                    suggested_links_list.append({
                                        "target_memory_id": sim_id,
                                        "target_description": details.get("description", ""),
                                        "target_type": details.get("memory_type", ""),
                                        "similarity": round(similarity, 4),
                                        "suggested_link_type": suggested_type
                                    })
                                    if len(suggested_links_list) >= max_suggested_links:
                                        break # Stop if we reached the limit
                        logger.info(f"Generated {len(suggested_links_list)} link suggestions for memory {memory_id}")

                except Exception as link_err:
                    logger.error(f"Error suggesting links for memory {memory_id}: {link_err}", exc_info=True)

            # --- 5. Update Workflow Timestamp ---
            await conn.execute("UPDATE workflows SET updated_at = ?, last_active = ? WHERE workflow_id = ?", (now_iso, now_unix, workflow_id))

            # --- 6. Log Operation ---
            await MemoryUtils._log_memory_operation(conn, workflow_id, "create", memory_id, action_id, {
                "memory_level": mem_level.value, "memory_type": mem_type.value, "importance": importance,
                "embedding_generated": embedding_generated_successfully, "links_suggested": len(suggested_links_list), "tags": final_tags
            })

            # --- 7. Commit Transaction ---
            await conn.commit()

            # --- 8. Prepare Result ---
            processing_time = time.time() - start_time
            result = {
                "memory_id": memory_id,
                "workflow_id": workflow_id,
                "memory_level": mem_level.value,
                "memory_type": mem_type.value,
                "content_preview": content[:100] + ("..." if len(content)>100 else ""),
                "importance": importance,
                "confidence": confidence,
                "created_at_unix": now_unix,
                "tags": final_tags,
                "embedding_id": embedding_db_id,
                "linked_action_id": action_id,
                "linked_thought_id": thought_id,
                "linked_artifact_id": artifact_id,
                "suggested_links": suggested_links_list, # Include suggestions
                "success": True,
                "processing_time": processing_time
            }
            logger.info(f"Stored memory {memory_id} ({mem_type.value}) in workflow {workflow_id}. Links suggested: {len(suggested_links_list)}.", emoji_key="floppy_disk", time=processing_time)
            return result

    except ToolInputError:
        raise # Re-raise specific input errors
    except Exception as e:
        logger.error(f"Failed to store memory: {str(e)}", emoji_key="x", exc_info=True)
        raise ToolError(f"Failed to store memory: {str(e)}") from e
    

@with_tool_metrics
@with_error_handling
async def get_memory_by_id(
    memory_id: str,
    include_links: bool = True, # Default True for richer context
    include_context: bool = True, # Default True for semantic context
    context_limit: int = 5, # Limit for semantic context results
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves a specific memory by its ID, optionally including links and semantic context.

    Fetches the core memory details, updates access time, and can optionally retrieve:
    - Both incoming and outgoing links to other memories.
    - Semantically similar memories based on embedding comparison.

    Args:
        memory_id: ID of the memory to retrieve.
        include_links: (Optional) Whether to include incoming/outgoing links. Default True.
        include_context: (Optional) Whether to include semantically similar memories. Default True.
        context_limit: (Optional) Max number of semantic context memories to return. Default 5.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing the memory details, links, and context.
        {
            "memory_id": "uuid-string",
            "workflow_id": "uuid-string",
            "content": "Memory content...",
            # ... other core memory fields ...
            "tags": ["tag1", "tag2"],
            "created_at_unix": 1649712000,
            "updated_at_unix": 1649712000,
            "last_accessed_unix": 1649712000,
            "outgoing_links": [ { ...link_details... } ], # Only if include_links=True
            "incoming_links": [ { ...link_details... } ], # Only if include_links=True
            "semantic_context": [ { ...context_memory_details... } ], # Only if include_context=True
            "success": true,
            "processing_time": 0.123
        }

    Raises:
        ToolInputError: If the memory ID is not provided or the memory is not found.
        ToolError: If the memory has expired or a database operation fails.
    """
    if not memory_id:
        raise ToolInputError("Memory ID required.", param_name="memory_id")

    start_time = time.time()
    result_memory = {} # Initialize result dictionary

    try:
        async with DBConnection(db_path) as conn:
            # --- 1. Fetch Core Memory Data ---
            async with conn.execute("SELECT * FROM memories WHERE memory_id = ?", (memory_id,)) as cursor:
                row = await cursor.fetchone()
                if not row:
                    raise ToolInputError(f"Memory {memory_id} not found.", param_name="memory_id")

                # Convert row to dict
                result_memory = dict(row) # aiosqlite.Row is dict-like

            # --- 2. Check TTL ---
            if result_memory.get("ttl", 0) > 0:
                expiry_time = result_memory["created_at"] + result_memory["ttl"]
                if expiry_time <= int(time.time()):
                    # Optionally delete expired memory here? Or just raise error.
                    # await conn.execute("DELETE FROM memories WHERE memory_id = ?", (memory_id,))
                    # await conn.commit()
                    logger.warning(f"Attempted to access expired memory {memory_id}.")
                    raise ToolError(f"Memory {memory_id} has expired.")

            # --- 3. Parse JSON Fields ---
            result_memory["tags"] = await MemoryUtils.deserialize(result_memory.get("tags"))
            result_memory["context"] = await MemoryUtils.deserialize(result_memory.get("context")) # Original creation context

            # --- 4. Update Access Statistics ---
            await MemoryUtils._update_memory_access(conn, memory_id)
            await MemoryUtils._log_memory_operation(conn, result_memory["workflow_id"], "access_by_id", memory_id)

            # --- 5. Fetch Links (Incoming & Outgoing) ---
            result_memory["outgoing_links"] = []
            result_memory["incoming_links"] = []
            if include_links:
                # Fetch Outgoing Links (Source = current memory)
                outgoing_query = """
                SELECT ml.link_id, ml.target_memory_id, ml.link_type, ml.strength, ml.description,
                       m.description AS target_description, m.memory_type AS target_type
                FROM memory_links ml
                JOIN memories m ON ml.target_memory_id = m.memory_id
                WHERE ml.source_memory_id = ?
                """
                async with conn.execute(outgoing_query, (memory_id,)) as cursor:
                    async for link_row in cursor:
                        result_memory["outgoing_links"].append({
                            "link_id": link_row["link_id"],
                            "target_memory_id": link_row["target_memory_id"],
                            "target_description": link_row["target_description"],
                            "target_type": link_row["target_type"],
                            "link_type": link_row["link_type"],
                            "strength": link_row["strength"],
                            "description": link_row["description"], # Link description
                        })

                # Fetch Incoming Links (Target = current memory)
                incoming_query = """
                SELECT ml.link_id, ml.source_memory_id, ml.link_type, ml.strength, ml.description,
                       m.description AS source_description, m.memory_type AS source_type
                FROM memory_links ml
                JOIN memories m ON ml.source_memory_id = m.memory_id
                WHERE ml.target_memory_id = ?
                """
                async with conn.execute(incoming_query, (memory_id,)) as cursor:
                    async for link_row in cursor:
                        result_memory["incoming_links"].append({
                            "link_id": link_row["link_id"],
                            "source_memory_id": link_row["source_memory_id"],
                            "source_description": link_row["source_description"],
                            "source_type": link_row["source_type"],
                            "link_type": link_row["link_type"],
                            "strength": link_row["strength"],
                            "description": link_row["description"], # Link description
                        })

            # --- 6. Fetch Semantic Context ---
            result_memory["semantic_context"] = []
            if include_context and result_memory.get("embedding_id"):
                # Formulate text for similarity search (use description + content)
                search_text = result_memory.get("content", "")
                if result_memory.get("description"):
                    search_text = f"{result_memory['description']}: {search_text}"

                if search_text:
                    try:
                        # Find similar memories (excluding self)
                        similar_results = await _find_similar_memories(
                            conn=conn,
                            query_text=search_text,
                            workflow_id=result_memory["workflow_id"], # Search within same workflow
                            limit=context_limit + 1, # Fetch one extra in case self is included
                            threshold=SIMILARITY_THRESHOLD * 0.9 # Slightly lower threshold for context
                        )

                        if similar_results:
                             # Get IDs, excluding the current memory ID
                            similar_ids = [mem_id for mem_id, score in similar_results if mem_id != memory_id][:context_limit]
                            score_map = dict(similar_results) # Keep scores

                            if similar_ids:
                                 # Fetch details for context memories
                                 placeholders = ', '.join(['?'] * len(similar_ids))
                                 context_query = "SELECT memory_id, description, memory_type, importance FROM memories WHERE memory_id IN ({})".format(placeholders)
                                 async with conn.execute(context_query, similar_ids) as context_cursor:
                                      context_rows = await context_cursor.fetchall()
                                      # Order by original similarity score
                                      ordered_context = sorted(context_rows, key=lambda r: score_map.get(r['memory_id'], -1.0), reverse=True)
                                      for context_row in ordered_context:
                                          result_memory["semantic_context"].append({
                                               "memory_id": context_row["memory_id"],
                                               "description": context_row["description"],
                                               "memory_type": context_row["memory_type"],
                                               "importance": context_row["importance"],
                                               "similarity": score_map.get(context_row["memory_id"], 0.0)
                                          })
                    except Exception as context_err:
                         logger.warning(f"Could not retrieve semantic context for memory {memory_id}: {context_err}")
                         # Continue without semantic context if it fails

            # --- 7. Finalize and Return ---
            await conn.commit() # Commit the access updates

            result_memory["success"] = True
            # Add consistently named timestamp keys
            result_memory["created_at_unix"] = result_memory["created_at"]
            result_memory["updated_at_unix"] = result_memory["updated_at"]
            result_memory["last_accessed_unix"] = result_memory["last_accessed"] # Already updated by _update_memory_access

            result_memory["processing_time"] = time.time() - start_time

            logger.info(f"Retrieved memory {memory_id} with links={include_links}, context={include_context}", emoji_key="inbox_tray")
            return result_memory # Return the enhanced dictionary

    except (ToolInputError, ToolError):
        raise # Re-raise specific handled errors
    except Exception as e:
        logger.error(f"Failed to get memory {memory_id}: {str(e)}", emoji_key="x", exc_info=True)
        raise ToolError(f"Failed to get memory {memory_id}: {str(e)}") from e
    

@with_tool_metrics
@with_error_handling
async def search_semantic_memories(
    query: str,
    workflow_id: Optional[str] = None, # Allow searching across workflows if None
    limit: int = 5,
    threshold: float = SIMILARITY_THRESHOLD, # Use constant
    memory_level: Optional[str] = None,
    memory_type: Optional[str] = None, # Filter by type is now handled by _find_similar_memories
    include_content: bool = True,      # Control whether full content is returned
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Searches memories based on semantic similarity using EmbeddingService.

    Retrieves memories whose embeddings are semantically close to the query text's embedding.
    Optionally filters by workflow, memory level, and type. Updates access stats for retrieved memories.
    """
    # Input validation
    if not query:
        raise ToolInputError("Search query required.", param_name="query")
    if not isinstance(limit, int) or limit < 1:
        raise ToolInputError("Limit must be positive integer.", param_name="limit")
    if not 0.0 <= threshold <= 1.0:
        raise ToolInputError("Threshold must be 0.0-1.0.", param_name="threshold")
    if memory_level:
        try:
            MemoryLevel(memory_level.lower()) # Validate enum value
        except ValueError as e:
            raise ToolInputError("Invalid memory_level.", param_name="memory_level") from e
    if memory_type:
        try:
            MemoryType(memory_type.lower()) # Validate enum value
        except ValueError as e:
            raise ToolInputError("Invalid memory_type.", param_name="memory_type") from e

    start_time = time.time()
    try:
        async with DBConnection(db_path) as conn:
            # Verify workflow exists only if a specific one is provided
            if workflow_id:
                 async with conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,)) as cursor:
                     if not await cursor.fetchone():
                         raise ToolInputError(f"Workflow {workflow_id} not found.", param_name="workflow_id")

            # --- Step 1: Find similar memory IDs and scores ---
            # Pass the memory_type filter to the helper function now
            similar_results: List[Tuple[str, float]] = await _find_similar_memories(
                conn=conn,
                query_text=query,
                workflow_id=workflow_id,
                limit=limit,
                threshold=threshold,
                memory_level=memory_level,
                memory_type=memory_type # Pass the filter here
            )

            # If no similar memories found, return early
            if not similar_results:
                logger.info(f"Semantic search for '{query[:50]}...' found no results matching filters above threshold {threshold}.", emoji_key="zzz")
                return {
                    "memories": [], "query": query, "workflow_id": workflow_id,
                    "success": True, "processing_time": time.time() - start_time
                }

            # --- Step 2: Fetch full details for the matching memories ---
            memory_ids = [mem_id for mem_id, score in similar_results]
            placeholders = ', '.join(['?'] * len(memory_ids))

            # Define columns to select based on include_content flag
            select_cols = "memory_id, workflow_id, description, memory_type, memory_level, importance, confidence, created_at, tags, action_id, thought_id, artifact_id"
            if include_content:
                select_cols += ", content"

            # Create a score mapping for ordering results correctly later
            score_map = dict(similar_results)

            # Fetch memory data from the database
            memories_data = []
            async with conn.execute(f"SELECT {select_cols} FROM memories WHERE memory_id IN ({placeholders})", memory_ids) as cursor:
                rows = await cursor.fetchall()

                # --- Step 3: Process and format results, update access stats ---
                # Order rows based on the similarity score (desc) and then memory_id (asc) for stability
                ordered_rows = sorted(
                    rows,
                    key=lambda r: (score_map.get(r['memory_id'], -1.0), r['memory_id']),
                    reverse=True # Sort primarily by score descending
                )
                # Note: Secondary sort by memory_id will be ascending due to how tuple sorting works with reverse=True

                for row in ordered_rows:
                    # Type filter already applied in _find_similar_memories, no need to check again here

                    mem_dict = dict(row) # Convert row to dict
                    mem_dict["similarity"] = score_map.get(row["memory_id"], 0.0) # Add similarity score
                    mem_dict["created_at_unix"] = row["created_at"] # Keep unix ts name consistent
                    mem_dict["tags"] = await MemoryUtils.deserialize(mem_dict.get("tags")) # Deserialize tags

                    # Update access time and log operation for this retrieved memory
                    await MemoryUtils._update_memory_access(conn, row["memory_id"])
                    await MemoryUtils._log_memory_operation(conn, row["workflow_id"], "semantic_access", row["memory_id"], None, {"query": query[:100], "score": mem_dict["similarity"]})

                    memories_data.append(mem_dict)

                    # Limit should have been handled by _find_similar_memories, but double-check
                    if len(memories_data) >= limit:
                        break

            # Commit the access updates
            await conn.commit()

            processing_time = time.time() - start_time
            logger.info(f"Semantic search found {len(memories_data)} results for query: '{query[:50]}...'", emoji_key="mag", time=processing_time)

            # Return the formatted results
            return {
                "memories": memories_data, # Return list of full memory dictionaries
                "query": query,
                "workflow_id": workflow_id, # Indicate which workflow was searched, or None
                "success": True,
                "processing_time": processing_time
            }

    except ToolInputError:
        raise # Re-raise specific input errors
    except Exception as e:
        logger.error(f"Failed semantic search: {str(e)}", emoji_key="x", exc_info=True)
        raise ToolError(f"Failed semantic search: {str(e)}") from e
    

@with_tool_metrics
@with_error_handling
async def hybrid_search_memories(
    query: str,
    workflow_id: Optional[str] = None,
    limit: int = 10,
    offset: int = 0,
    semantic_weight: float = 0.6, # Default weight for semantic score
    keyword_weight: float = 0.4,  # Default weight for keyword/relevance score
    # Filters from query_memories
    memory_level: Optional[str] = None,
    memory_type: Optional[str] = None,
    tags: Optional[List[str]] = None,
    min_importance: Optional[float] = None,
    max_importance: Optional[float] = None,
    min_confidence: Optional[float] = None,
    min_created_at_unix: Optional[int] = None,
    max_created_at_unix: Optional[int] = None,
    # Control flags
    include_content: bool = True,
    include_links: bool = False, # Keep False by default for performance in search
    link_direction: str = "outgoing", # 'outgoing', 'incoming', 'both' - Determines which links to fetch if include_links=True
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Performs a hybrid search combining semantic similarity and keyword/filtered relevance.

    Retrieves memories ranked by a weighted combination of their semantic similarity
    to the query and their relevance based on keywords (FTS) and other attributes
    (importance, recency, confidence).

    Args:
        query: The search query text (used for both semantic and keyword/FTS search).
        workflow_id: (Optional) ID of the workflow to search within. If None, searches globally.
        limit: (Optional) Maximum number of results to return. Default 10.
        offset: (Optional) Number of results to skip for pagination. Default 0.
        semantic_weight: (Optional) Weight (0.0-1.0) for the semantic similarity score. Default 0.6.
        keyword_weight: (Optional) Weight (0.0-1.0) for the keyword/attribute relevance score. Default 0.4.
        memory_level: (Optional) Filter by memory level.
        memory_type: (Optional) Filter by memory type.
        tags: (Optional) Filter memories containing ALL specified tags.
        min_importance: (Optional) Minimum importance score.
        max_importance: (Optional) Maximum importance score.
        min_confidence: (Optional) Minimum confidence score.
        min_created_at_unix: (Optional) Minimum creation timestamp (Unix seconds).
        max_created_at_unix: (Optional) Maximum creation timestamp (Unix seconds).
        include_content: (Optional) Whether to include full memory content. Default True.
        include_links: (Optional) Whether to include detailed link info. Default False.
        link_direction: (Optional) Direction for links if included ('outgoing', 'incoming', 'both'). Default 'outgoing'.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing the ranked list of matching memories and scores.
        {
            "memories": [
                {
                    ...memory_details...,
                    "hybrid_score": 0.85,
                    "semantic_score": 0.92,
                    "keyword_relevance_score": 0.75, # Normalized 0-1
                    "links": { # Populated if include_links=True
                        "outgoing": [ ... ],
                        "incoming": [ ... ]
                    }
                },
                ...
            ],
            "total_candidates_considered": 55, # Total unique memories found by either search before ranking/limit
            "success": true,
            "processing_time": 0.45
        }

    Raises:
        ToolInputError: If parameters are invalid (weights out of range, bad filters, etc.).
        ToolError: If the database operation or semantic search fails.
    """
    start_time = time.time()

    # --- Input Validation ---
    if not query:
        raise ToolInputError("Query string cannot be empty.", param_name="query")
    if not 0.0 <= semantic_weight <= 1.0:
        raise ToolInputError("semantic_weight must be between 0.0 and 1.0", param_name="semantic_weight")
    if not 0.0 <= keyword_weight <= 1.0:
        raise ToolInputError("keyword_weight must be between 0.0 and 1.0", param_name="keyword_weight")
    if semantic_weight + keyword_weight <= 0:
        raise ToolInputError("Sum of semantic_weight and keyword_weight must be positive.", param_name="semantic_weight")
    if limit < 1:
        raise ToolInputError("Limit must be >= 1", param_name="limit")
    if offset < 0:
        raise ToolInputError("Offset must be >= 0", param_name="offset")
    # Reuse validation for filters from query_memories if needed, or perform here
    if memory_level:
        try:
            MemoryLevel(memory_level.lower())
        except ValueError as e:
            raise ToolInputError("Invalid memory_level.", param_name="memory_level") from e
    if memory_type:
        try:
            MemoryType(memory_type.lower())
        except ValueError as e:
            raise ToolInputError("Invalid memory_type.", param_name="memory_type") from e
    valid_link_directions = ["outgoing", "incoming", "both"]
    link_direction_lower = link_direction.lower() # Use lower case consistently
    if link_direction_lower not in valid_link_directions:
         raise ToolInputError(f"link_direction must be one of: {', '.join(valid_link_directions)}", param_name="link_direction")

    # Normalize weights
    total_weight = semantic_weight + keyword_weight
    norm_sem_weight = semantic_weight / total_weight
    norm_key_weight = keyword_weight / total_weight

    # Dictionary to hold scores for each memory ID
    # Structure: { memory_id: {"semantic": score, "keyword": score, "hybrid": score} }
    combined_scores: Dict[str, Dict[str, float]] = defaultdict(lambda: {"semantic": 0.0, "keyword": 0.0, "hybrid": 0.0})

    try:
        async with DBConnection(db_path) as conn:
            # --- Step 1: Semantic Search ---
            semantic_results: List[Tuple[str, float]] = []
            if norm_sem_weight > 0:
                try:
                    # Use internal helper _find_similar_memories
                    # Fetch more candidates than final limit initially to allow keyword scores to influence ranking
                    semantic_candidate_limit = min(max(limit * 5, 50), MAX_SEMANTIC_CANDIDATES)
                    semantic_results = await _find_similar_memories(
                        conn=conn,
                        query_text=query,
                        workflow_id=workflow_id,
                        limit=semantic_candidate_limit, # Fetch more initially
                        threshold=0.1, # Lower threshold initially to get more candidates
                        memory_level=memory_level,
                        memory_type=memory_type
                    )
                    for mem_id, score in semantic_results:
                        combined_scores[mem_id]["semantic"] = score
                    logger.debug(f"Hybrid search: Found {len(semantic_results)} semantic candidates.")
                except Exception as sem_err:
                    logger.warning(f"Semantic search part failed in hybrid search: {sem_err}", exc_info=True)
                    # Allow hybrid search to continue with keyword results if semantic fails

            # --- Step 2: Keyword/Filtered Search & Relevance Score ---
            if norm_key_weight > 0:
                # Build query similar to query_memories, but only fetch IDs and scoring components
                select_cols = "m.memory_id, m.importance, m.confidence, m.created_at, m.access_count, m.last_accessed"
                data_query = f"SELECT {select_cols} FROM memories m"
                where_clauses = ["1=1"]
                params: List[Any] = []
                fts_params: List[Any] = []
                joins = "" # Keep track of needed joins

                # Apply filters (same logic as query_memories)
                if workflow_id:
                    where_clauses.append("m.workflow_id = ?")
                    params.append(workflow_id)
                if memory_level:
                    where_clauses.append("m.memory_level = ?")
                    params.append(memory_level.lower())
                if memory_type:
                    where_clauses.append("m.memory_type = ?")
                    params.append(memory_type.lower())
                if min_importance is not None:
                    where_clauses.append("m.importance >= ?")
                    params.append(min_importance)
                if max_importance is not None:
                    where_clauses.append("m.importance <= ?")
                    params.append(max_importance)
                if min_confidence is not None:
                    where_clauses.append("m.confidence >= ?")
                    params.append(min_confidence)
                if min_created_at_unix is not None:
                    where_clauses.append("m.created_at >= ?")
                    params.append(min_created_at_unix)
                if max_created_at_unix is not None:
                    where_clauses.append("m.created_at <= ?")
                    params.append(max_created_at_unix)
                now_unix = int(time.time())
                where_clauses.append("(m.ttl = 0 OR m.created_at + m.ttl > ?)")
                params.append(now_unix)
                if tags and isinstance(tags, list) and len(tags) > 0:
                    tags_json = json.dumps([str(tag).lower() for tag in tags])
                    where_clauses.append("json_contains_all(m.tags, ?)")
                    params.append(tags_json)
                if query: # Use the main query for FTS search
                    # Important: FTS requires JOINing the FTS table
                    if "memory_fts" not in joins:
                        joins += " JOIN memory_fts fts ON m.rowid = fts.rowid"
                    where_clauses.append("fts.memory_fts MATCH ?")
                    fts_query_term = ' OR '.join(query.strip().split()) # Basic query formation
                    fts_params.append(fts_query_term)

                # Combine WHERE clauses and add joins
                where_sql = " WHERE " + " AND ".join(where_clauses)
                final_query = data_query + joins + where_sql

                # Fetch *all* matching results for keyword scoring, don't limit yet
                # We limit *after* hybrid scoring
                keyword_candidates = []
                cursor = await conn.execute(final_query, params + fts_params)
                keyword_candidates = await cursor.fetchall()
                await cursor.close() # Close cursor

                # Calculate raw keyword relevance scores (0-10 range)
                raw_keyword_scores = {}
                for row in keyword_candidates:
                    mem_id = row["memory_id"]
                    kw_relevance = _compute_memory_relevance(
                        row["importance"], row["confidence"], row["created_at"],
                        row["access_count"], row["last_accessed"]
                    )
                    # Store the raw score (0-10 range)
                    raw_keyword_scores[mem_id] = kw_relevance
                    # Initialize the keyword score in combined_scores (will be normalized later)
                    if mem_id not in combined_scores:
                        combined_scores[mem_id] # Ensure entry exists if only found by keyword search
                    combined_scores[mem_id]["keyword"] = 0.0 # Placeholder

                # Find the maximum observed raw keyword score
                max_kw_score = 0.0
                if raw_keyword_scores:
                    max_kw_score = max(raw_keyword_scores.values())

                # Normalize keyword scores based on the observed maximum (or 10 if max is 0)
                # Avoid division by zero or near-zero scores causing massive inflation.
                normalization_factor = max(max_kw_score, 1e-6) # Use a small epsilon if max_kw_score is 0

                for mem_id, raw_score in raw_keyword_scores.items():
                    normalized_kw_score = min(max(raw_score / normalization_factor, 0.0), 1.0)
                    combined_scores[mem_id]["keyword"] = normalized_kw_score

                logger.debug(f"Hybrid search: Found and scored {len(keyword_candidates)} keyword/filtered candidates (Max raw score: {max_kw_score:.2f}).")

            # --- Step 3: Calculate Hybrid Score ---
            final_ranked_ids = []
            final_scores_map = {}
            if not combined_scores:
                 logger.info("Hybrid search yielded no candidates from either semantic or keyword search.")
            else:
                for _mem_id, scores in combined_scores.items():
                    scores["hybrid"] = (scores["semantic"] * norm_sem_weight) + (scores["keyword"] * norm_key_weight)

                # Sort by hybrid score
                sorted_ids_scores = sorted(combined_scores.items(), key=lambda item: item[1]["hybrid"], reverse=True)

                # Apply pagination *after* ranking
                paginated_ids_scores = sorted_ids_scores[offset : offset + limit]
                final_ranked_ids = [item[0] for item in paginated_ids_scores] # Get just the IDs
                final_scores_map = {item[0]: item[1] for item in paginated_ids_scores} # Keep scores for final list

            # --- Step 4: Fetch Full Details for Ranked & Paginated IDs ---
            memories_results = [] # Final list of processed memories
            total_candidates_considered = len(combined_scores) # Total unique matches found
            rows_map = {} # To store fetched memory data

            if final_ranked_ids:
                placeholders = ','.join('?' * len(final_ranked_ids))
                # Select columns based on include_content
                select_cols_final = "m.memory_id, m.workflow_id, m.memory_level, m.memory_type, m.importance, m.confidence, m.description, m.reasoning, m.source, m.tags, m.created_at, m.updated_at, m.last_accessed, m.access_count, m.ttl, m.action_id, m.thought_id, m.artifact_id"
                if include_content:
                    select_cols_final += ", m.content"

                # Fetch data
                query_final = f"SELECT {select_cols_final} FROM memories m WHERE m.memory_id IN ({placeholders})"
                cursor = await conn.execute(query_final, final_ranked_ids)
                # Store fetched data in a map for easy access
                rows_map = {row['memory_id']: dict(row) for row in await cursor.fetchall()}
                await cursor.close()

            # --- Step 5: Batch Fetch Links if Requested ---
            links_map = defaultdict(lambda: {"outgoing": [], "incoming": []})
            if include_links and final_ranked_ids:
                placeholders = ','.join('?' * len(final_ranked_ids))

                # Fetch Outgoing Links
                if link_direction_lower in ["outgoing", "both"]:
                    outgoing_query = f"""
                    SELECT ml.link_id, ml.source_memory_id, ml.target_memory_id, ml.link_type, ml.strength, ml.description AS link_description,
                           target_mem.description AS target_description, target_mem.memory_type AS target_type
                    FROM memory_links ml JOIN memories target_mem ON ml.target_memory_id = target_mem.memory_id
                    WHERE ml.source_memory_id IN ({placeholders})
                    """
                    outgoing_cursor = await conn.execute(outgoing_query, final_ranked_ids)
                    async for link_row in outgoing_cursor:
                        links_map[link_row["source_memory_id"]]["outgoing"].append(dict(link_row))
                    await outgoing_cursor.close()

                # Fetch Incoming Links
                if link_direction_lower in ["incoming", "both"]:
                    incoming_query = f"""
                    SELECT ml.link_id, ml.source_memory_id, ml.target_memory_id, ml.link_type, ml.strength, ml.description AS link_description,
                           source_mem.description AS source_description, source_mem.memory_type AS source_type
                    FROM memory_links ml JOIN memories source_mem ON ml.source_memory_id = source_mem.memory_id
                    WHERE ml.target_memory_id IN ({placeholders})
                    """
                    incoming_cursor = await conn.execute(incoming_query, final_ranked_ids)
                    async for link_row in incoming_cursor:
                        links_map[link_row["target_memory_id"]]["incoming"].append(dict(link_row))
                    await incoming_cursor.close()

            # --- Step 6: Reconstruct Results, Attach Links, Update Access Stats ---
            update_access_tasks = [] # Tasks for updating access stats
            if final_ranked_ids:
                # Reconstruct the list in the final ranked order and add scores/links
                for mem_id in final_ranked_ids:
                    if mem_id in rows_map:
                        memory_dict = rows_map[mem_id]
                        scores = final_scores_map.get(mem_id, {}) # Get scores for this ID
                        memory_dict["hybrid_score"] = round(scores.get("hybrid", 0.0), 4)
                        memory_dict["semantic_score"] = round(scores.get("semantic", 0.0), 4)
                        memory_dict["keyword_relevance_score"] = round(scores.get("keyword", 0.0), 4) # Already normalized 0-1

                        # Add other standard fields
                        memory_dict["tags"] = await MemoryUtils.deserialize(memory_dict.get("tags"))
                        memory_dict["created_at_unix"] = memory_dict.get("created_at")
                        memory_dict["updated_at_unix"] = memory_dict.get("updated_at")
                        memory_dict["last_accessed_unix"] = memory_dict.get("last_accessed")

                        # Attach links from the pre-fetched map
                        if include_links:
                            memory_dict["links"] = links_map[mem_id]

                        memories_results.append(memory_dict)

                        # Prepare access update tasks
                        update_access_tasks.append(
                            MemoryUtils._update_memory_access(conn, mem_id)
                        )
                        update_access_tasks.append(
                            MemoryUtils._log_memory_operation(
                                conn, memory_dict["workflow_id"], "hybrid_access", mem_id, None,
                                {"query": query[:100], "hybrid_score": memory_dict["hybrid_score"]}
                            )
                        )

            # --- Step 7: Update Access Stats Concurrently & Commit ---
            if update_access_tasks:
                await asyncio.gather(*update_access_tasks)
                await conn.commit()

            processing_time = time.time() - start_time
            logger.info(f"Hybrid search returned {len(memories_results)} results for query '{query[:50]}...'", emoji_key="magic_wand", time=processing_time)

            return {
                "memories": memories_results,
                "total_candidates_considered": total_candidates_considered,
                "success": True,
                "processing_time": processing_time
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Hybrid search failed: {str(e)}", emoji_key="x", exc_info=True)
        raise ToolError(f"Hybrid search failed: {str(e)}") from e

@with_tool_metrics
@with_error_handling
async def create_memory_link(
    source_memory_id: str,
    target_memory_id: str,
    link_type: str, # Use LinkType enum
    strength: float = 1.0,
    description: Optional[str] = None,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Creates an associative link between two memories.
       (Adapted from cognitive_memory.create_memory_link).
    """
    if source_memory_id == target_memory_id: 
        raise ToolInputError("Cannot link memory to itself.", param_name="source_memory_id")
    try: 
        link_type_enum = LinkType(link_type.lower())
    except ValueError as e: 
        raise ToolInputError(f"Invalid link_type. Use one of: {[lt.value for lt in LinkType]}", param_name="link_type") from e
    if not 0.0 <= strength <= 1.0: 
        raise ToolInputError("Strength must be 0.0-1.0.", param_name="strength")

    link_id = MemoryUtils.generate_id()
    now_unix = int(time.time())

    try:
        async with DBConnection(db_path) as conn:
             # Check memories exist and get workflow_id (use source memory's workflow)
            async with conn.execute("SELECT workflow_id FROM memories WHERE memory_id = ?", (source_memory_id,)) as cursor:
                source_row = await cursor.fetchone()
                if not source_row: 
                    raise ToolInputError(f"Source memory {source_memory_id} not found.", param_name="source_memory_id")
                workflow_id = source_row["workflow_id"]
            async with conn.execute("SELECT 1 FROM memories WHERE memory_id = ?", (target_memory_id,)) as cursor:
                if not await cursor.fetchone(): 
                    raise ToolInputError(f"Target memory {target_memory_id} not found.", param_name="target_memory_id")

            # Insert or Replace link (handle existing links gracefully)
            # Using INSERT OR REPLACE requires unique constraint on (source, target, type)
            await conn.execute(
                """
                INSERT OR REPLACE INTO memory_links
                (link_id, source_memory_id, target_memory_id, link_type, strength, description, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (link_id, source_memory_id, target_memory_id, link_type_enum.value, strength, description or "", now_unix)
            )

            # Log operation
            await MemoryUtils._log_memory_operation(conn, workflow_id, "link", source_memory_id, None, {
                "target_memory_id": target_memory_id, "link_type": link_type_enum.value, "link_id": link_id
            })

            await conn.commit()

            result = {
                "link_id": link_id,
                "source_memory_id": source_memory_id,
                "target_memory_id": target_memory_id,
                "link_type": link_type_enum.value,
                "strength": strength,
                "created_at_unix": now_unix,
                "success": True
            }
            logger.info(f"Created link {link_id} from {source_memory_id} to {target_memory_id}", emoji_key="link")
            return result

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Failed to create memory link: {str(e)}", emoji_key="x", exc_info=True)
        raise ToolError(f"Failed to create memory link: {str(e)}") from e

# --- 7. Core Memory Retrieval (Adapted from cognitive_memory, FULL link support) ---
@with_tool_metrics
@with_error_handling
async def query_memories(
    workflow_id: Optional[str] = None,
    memory_level: Optional[str] = None,
    memory_type: Optional[str] = None,
    search_text: Optional[str] = None, # Keyword/FTS search
    tags: Optional[List[str]] = None, # Filter by tags
    min_importance: Optional[float] = None,
    max_importance: Optional[float] = None,
    min_confidence: Optional[float] = None,
    min_created_at_unix: Optional[int] = None,
    max_created_at_unix: Optional[int] = None,
    sort_by: str = "relevance", # relevance, importance, created_at, updated_at, confidence, last_accessed
    sort_order: str = "DESC",
    include_content: bool = True,
    include_links: bool = False, # Flag to include detailed links
    link_direction: str = "outgoing", # 'outgoing', 'incoming', 'both' - Determines which links to fetch if include_links=True
    limit: int = 10,
    offset: int = 0,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves memories based on various criteria like level, type, tags, text, importance, etc.

    This is the primary tool for filtering and retrieving memories from the knowledge base
    using structured criteria and keyword search (distinct from pure semantic search).
    Includes option to fetch detailed information about linked memories.

    Args:
        workflow_id: (Optional) ID of the workflow to query. If None, searches across all accessible workflows.
        memory_level: (Optional) Filter by memory level (e.g., 'episodic', 'semantic').
        memory_type: (Optional) Filter by memory type (e.g., 'insight', 'fact').
        search_text: (Optional) Full-text search query for content, description, reasoning, tags.
        tags: (Optional) Filter memories containing ALL specified tags.
        min_importance: (Optional) Minimum importance score (1.0-10.0).
        max_importance: (Optional) Maximum importance score (1.0-10.0).
        min_confidence: (Optional) Minimum confidence score (0.0-1.0).
        min_created_at_unix: (Optional) Minimum creation timestamp (Unix seconds).
        max_created_at_unix: (Optional) Maximum creation timestamp (Unix seconds).
        sort_by: (Optional) Field to sort by. Options: 'relevance', 'importance', 'created_at',
                 'updated_at', 'confidence', 'last_accessed', 'access_count'. Default 'relevance'.
        sort_order: (Optional) Sort direction ('ASC' or 'DESC'). Default 'DESC'.
        include_content: (Optional) Whether to include the full memory content. Default True.
        include_links: (Optional) Whether to include detailed info about linked memories. Default False.
        link_direction: (Optional) Which links to fetch if include_links is True:
                        'outgoing', 'incoming', or 'both'. Default 'outgoing'.
        limit: (Optional) Maximum number of memories to return. Default 10.
        offset: (Optional) Number of memories to skip for pagination. Default 0.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing the list of matching memories and total count.
        {
            "memories": [
                {
                    ...memory_details...,
                    "links": {
                        "outgoing": [ { "link_id": ..., "target_memory_id": ..., "target_description": ..., "link_type": ..., "strength": ... }, ... ],
                        "incoming": [ { "link_id": ..., "source_memory_id": ..., "source_description": ..., "link_type": ..., "strength": ... }, ... ]
                    } # Populated if include_links=True
                },
                ...
            ],
            "total_matching_count": 42,
            "success": true,
            "processing_time": 0.123
        }

    Raises:
        ToolInputError: If filter parameters are invalid.
        ToolError: If the database operation fails.
    """
    start_time = time.time()

    # --- Input Validation ---
    valid_sort_fields = ["relevance", "importance", "created_at", "updated_at", "confidence", "last_accessed", "access_count"]
    if sort_by not in valid_sort_fields:
        raise ToolInputError(f"sort_by must be one of: {', '.join(valid_sort_fields)}", param_name="sort_by")
    if sort_order.upper() not in ["ASC", "DESC"]:
        raise ToolInputError("sort_order must be 'ASC' or 'DESC'", param_name="sort_order")
    sort_order = sort_order.upper()

    valid_link_directions = ["outgoing", "incoming", "both"]
    if link_direction.lower() not in valid_link_directions:
         raise ToolInputError(f"link_direction must be one of: {', '.join(valid_link_directions)}", param_name="link_direction")
    link_direction = link_direction.lower()

    if limit < 1:
        raise ToolInputError("Limit must be >= 1", param_name="limit")
    if offset < 0:
        raise ToolInputError("Offset must be >= 0", param_name="offset")

    if memory_level:
        try:
            MemoryLevel(memory_level.lower())
        except ValueError as e:
            raise ToolInputError("Invalid memory_level.", param_name="memory_level") from e
    if memory_type:
        try:
            MemoryType(memory_type.lower())
        except ValueError as e:
            raise ToolInputError("Invalid memory_type.", param_name="memory_type") from e

    try:
        async with DBConnection(db_path) as conn:
            # Verify workflow exists if provided
            if workflow_id:
                async with conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,)) as cursor:
                    if not await cursor.fetchone():
                        raise ToolInputError(f"Workflow {workflow_id} not found.", param_name="workflow_id")

            # --- Build Query ---
            select_clause = "m.memory_id, m.workflow_id, m.memory_level, m.memory_type, m.importance, m.confidence, m.description, m.reasoning, m.source, m.tags, m.created_at, m.updated_at, m.last_accessed, m.access_count, m.ttl, m.action_id, m.thought_id, m.artifact_id"
            if include_content:
                select_clause += ", m.content"

            count_query = "SELECT COUNT(m.memory_id) FROM memories m"
            data_query = f"SELECT {select_clause} FROM memories m"

            where_clauses = ["1=1"]
            params: List[Any] = []
            fts_params: List[Any] = [] # Params specific to FTS match

            # --- Apply Filters (same logic as before) ---
            if workflow_id:
                where_clauses.append("m.workflow_id = ?")
                params.append(workflow_id)
            if memory_level:
                where_clauses.append("m.memory_level = ?")
                params.append(memory_level.lower())
            if memory_type:
                where_clauses.append("m.memory_type = ?")
                params.append(memory_type.lower())
            if min_importance is not None:
                where_clauses.append("m.importance >= ?")
                params.append(min_importance)
            if max_importance is not None:
                where_clauses.append("m.importance <= ?")
                params.append(max_importance)
            if min_confidence is not None:
                where_clauses.append("m.confidence >= ?")
                params.append(min_confidence)
            if min_created_at_unix is not None:
                where_clauses.append("m.created_at >= ?")
                params.append(min_created_at_unix)
            if max_created_at_unix is not None:
                where_clauses.append("m.created_at <= ?")
                params.append(max_created_at_unix)

            now_unix = int(time.time())
            where_clauses.append("(m.ttl = 0 OR m.created_at + m.ttl > ?)")
            params.append(now_unix)

            if tags and isinstance(tags, list) and len(tags) > 0:
                tags_json = json.dumps([str(tag).lower() for tag in tags])
                where_clauses.append("json_contains_all(m.tags, ?)")
                params.append(tags_json)

            if search_text:
                count_query += " JOIN memory_fts fts ON m.rowid = fts.rowid"
                data_query += " JOIN memory_fts fts ON m.rowid = fts.rowid"
                where_clauses.append("fts.memory_fts MATCH ?")
                fts_query_term = ' OR '.join(search_text.strip().split())
                fts_params.append(fts_query_term)

            # Combine WHERE clauses
            where_sql = ""
            if len(where_clauses) > 1:
                where_sql = " WHERE " + " AND ".join(where_clauses)
                count_query += where_sql
                data_query += where_sql

            # --- Get Total Count ---
            async with conn.execute(count_query, params + fts_params) as cursor:
                row = await cursor.fetchone()
                total_matching_count = row[0] if row else 0

            # --- Apply Sorting ---
            order_clause = ""
            if sort_by == "relevance":
                order_clause = " ORDER BY compute_memory_relevance(m.importance, m.confidence, m.created_at, m.access_count, m.last_accessed)"
            elif sort_by in ["created_at", "updated_at", "importance", "confidence", "last_accessed", "access_count"]:
                order_clause = f" ORDER BY m.{sort_by}"
            else:
                order_clause = " ORDER BY m.created_at" # Default sort fallback
            order_clause += f" {sort_order}"

            # --- Apply Pagination ---
            limit_clause = " LIMIT ? OFFSET ?"

            # --- Execute Data Query ---
            final_query = data_query + order_clause + limit_clause
            final_params = params + fts_params + [limit, offset]

            memories_results = [] # Final list of processed memories
            async with conn.execute(final_query, final_params) as cursor:
                rows = await cursor.fetchall()
                for row in rows:
                    memory_dict = dict(row) # Convert row to dict
                    memory_dict["tags"] = await MemoryUtils.deserialize(memory_dict.get("tags"))
                    # Add consistent unix timestamp keys
                    memory_dict["created_at_unix"] = memory_dict.get("created_at")
                    memory_dict["updated_at_unix"] = memory_dict.get("updated_at")
                    memory_dict["last_accessed_unix"] = memory_dict.get("last_accessed")

                    # --- Fetch Detailed Links (Full Implementation) ---
                    if include_links:
                        current_memory_id = memory_dict["memory_id"]
                        memory_dict["links"] = {"outgoing": [], "incoming": []} # Initialize structure

                        # Fetch Outgoing Links
                        if link_direction in ["outgoing", "both"]:
                            outgoing_query = """
                            SELECT ml.link_id, ml.target_memory_id, ml.link_type, ml.strength, ml.description AS link_description,
                                   target_mem.description AS target_description, target_mem.memory_type AS target_type
                            FROM memory_links ml
                            JOIN memories target_mem ON ml.target_memory_id = target_mem.memory_id
                            WHERE ml.source_memory_id = ?
                            """
                            async with conn.execute(outgoing_query, (current_memory_id,)) as link_cursor:
                                async for link_row in link_cursor:
                                     memory_dict["links"]["outgoing"].append(dict(link_row))

                        # Fetch Incoming Links
                        if link_direction in ["incoming", "both"]:
                            incoming_query = """
                            SELECT ml.link_id, ml.source_memory_id, ml.link_type, ml.strength, ml.description AS link_description,
                                   source_mem.description AS source_description, source_mem.memory_type AS source_type
                            FROM memory_links ml
                            JOIN memories source_mem ON ml.source_memory_id = source_mem.memory_id
                            WHERE ml.target_memory_id = ?
                            """
                            async with conn.execute(incoming_query, (current_memory_id,)) as link_cursor:
                                 async for link_row in link_cursor:
                                     memory_dict["links"]["incoming"].append(dict(link_row))

                    # Update access stats for the primary retrieved memory
                    await MemoryUtils._update_memory_access(conn, memory_dict["memory_id"])
                    await MemoryUtils._log_memory_operation(
                        conn, memory_dict["workflow_id"], "query_access",
                        memory_dict["memory_id"], None,
                        {"query_filters": {"sort":sort_by, "limit":limit}}
                    )

                    memories_results.append(memory_dict)

            # Commit access updates
            await conn.commit()

            processing_time = time.time() - start_time
            logger.info(f"Memory query returned {len(memories_results)} of {total_matching_count} results.", emoji_key="search", time=processing_time)

            return {
                "memories": memories_results,
                "total_matching_count": total_matching_count,
                "success": True,
                "processing_time": processing_time
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Failed to query memories: {str(e)}", emoji_key="x", exc_info=True)
        raise ToolError(f"Failed to query memories: {str(e)}") from e
    
# --- 8. Workflow Listing & Details (Ported from agent_memory) ---
@with_tool_metrics
@with_error_handling
async def list_workflows(
    status: Optional[str] = None,
    tag: Optional[str] = None,
    after_date: Optional[str] = None, # ISO Format string for filtering
    before_date: Optional[str] = None, # ISO Format string for filtering
    limit: int = 10,
    offset: int = 0,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Lists workflows matching specified criteria.
       Timestamps are returned as ISO 8601 strings.
    """
    try:
        # Validate status
        if status:
            try: 
                WorkflowStatus(status.lower())
            except ValueError as e: 
                raise ToolInputError(f"Invalid status: {status}", param_name="status") from e

        # Convert filter dates from ISO strings to Unix timestamps for querying
        after_ts: Optional[int] = None
        if after_date:
            try: 
                after_ts = int(datetime.fromisoformat(after_date.replace('Z', '+00:00')).timestamp())
            except ValueError as e: 
                raise ToolInputError("Invalid after_date format. Use ISO.", param_name="after_date") from e
        before_ts: Optional[int] = None
        if before_date:
            try: 
                before_ts = int(datetime.fromisoformat(before_date.replace('Z', '+00:00')).timestamp())
            except ValueError as e: 
                raise ToolInputError("Invalid before_date format. Use ISO.", param_name="before_date") from e

        # Validate pagination
        if limit < 1: 
            raise ToolInputError("Limit must be >= 1", param_name="limit")
        if offset < 0: 
            raise ToolInputError("Offset must be >= 0", param_name="offset")

        async with DBConnection(db_path) as conn:
            base_query = """
            SELECT DISTINCT w.workflow_id, w.title, w.description, w.goal, w.status,
                   w.created_at, w.updated_at, w.completed_at -- Fetch timestamps as INTEGER
            FROM workflows w
            """
            count_query = "SELECT COUNT(DISTINCT w.workflow_id) FROM workflows w"
            joins = ""
            where_clauses = ["1=1"]
            params = []

            if tag:
                joins += " JOIN workflow_tags wt ON w.workflow_id = wt.workflow_id JOIN tags t ON wt.tag_id = t.tag_id"
                where_clauses.append("t.name = ?")
                params.append(tag)

            if status:
                where_clauses.append("w.status = ?")
                params.append(status.lower())
            if after_ts is not None: # Use timestamp for filtering
                where_clauses.append("w.created_at >= ?")
                params.append(after_ts)
            if before_ts is not None: # Use timestamp for filtering
                where_clauses.append("w.created_at <= ?")
                params.append(before_ts)

            where_sql = " WHERE " + " AND ".join(where_clauses)
            full_base_query = base_query + joins + where_sql
            full_count_query = count_query + joins + where_sql

            # Get total count
            cursor = await conn.execute(full_count_query, params)
            row = await cursor.fetchone()
            await cursor.close()
            total_count = row[0] if row else 0

            # Get workflow data
            data_query = full_base_query + " ORDER BY w.updated_at DESC LIMIT ? OFFSET ?"
            params.extend([limit, offset])
            workflows_list = []
            workflow_ids_fetched = []

            cursor = await conn.execute(data_query, params)
            rows = await cursor.fetchall()
            await cursor.close()

            for row in rows:
                 wf_data = dict(row)
                 if wf_data.get("created_at"):
                     wf_data["created_at"] = to_iso_z(wf_data["created_at"])
                 if wf_data.get("updated_at"):
                     wf_data["updated_at"] = to_iso_z(wf_data["updated_at"])
                 if wf_data.get("completed_at"):
                     wf_data["completed_at"] = to_iso_z(wf_data["completed_at"])
                 wf_data["tags"] = [] # Initialize tags list
                 workflows_list.append(wf_data)
                 workflow_ids_fetched.append(wf_data["workflow_id"])

            # Batch fetch tags for fetched workflows
            if workflow_ids_fetched:
                 placeholders = ",".join("?" * len(workflow_ids_fetched))
                 tags_query = f"""
                 SELECT wt.workflow_id, t.name
                 FROM tags t JOIN workflow_tags wt ON t.tag_id = wt.tag_id
                 WHERE wt.workflow_id IN ({placeholders})
                 """
                 tags_map = defaultdict(list)
                 tags_cursor = await conn.execute(tags_query, workflow_ids_fetched)
                 async for tag_row in tags_cursor:
                      tags_map[tag_row["workflow_id"]].append(tag_row["name"])
                 await tags_cursor.close()

                 # Assign tags to workflows
                 for wf in workflows_list:
                      wf["tags"] = tags_map.get(wf["workflow_id"], [])

            result = {
                "workflows": workflows_list,
                "total_count": total_count,
                "success": True
            }
            logger.info(f"Listed {len(workflows_list)} workflows (total matching: {total_count})", emoji_key="scroll")
            return result

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error listing workflows: {e}", exc_info=True)
        raise ToolError(f"Failed to list workflows: {str(e)}") from e


@with_tool_metrics
@with_error_handling
async def get_workflow_details(
    workflow_id: str,
    include_actions: bool = True, # Default to True now
    include_artifacts: bool = True,
    include_thoughts: bool = True,
    include_memories: bool = False, # Keep memories optional for performance
    memories_limit: int = 20,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves comprehensive details about a specific workflow, including related items.
       Timestamps are returned as ISO 8601 strings.
    """
    if not workflow_id:
        raise ToolInputError("Workflow ID required.", param_name="workflow_id")

    try:
        async with DBConnection(db_path) as conn:
            # --- Get Workflow Core Info & Tags ---
            cursor = await conn.execute("SELECT * FROM workflows WHERE workflow_id = ?", (workflow_id,))
            wf_row = await cursor.fetchone()
            await cursor.close()
            if not wf_row:
                raise ToolInputError(f"Workflow {workflow_id} not found.", param_name="workflow_id")

            workflow_details = dict(wf_row)
            if workflow_details.get("created_at"):
                workflow_details["created_at"] = to_iso_z(workflow_details["created_at"])
            if workflow_details.get("updated_at"):
                workflow_details["updated_at"] = to_iso_z(workflow_details["updated_at"])
            if workflow_details.get("completed_at"):
                workflow_details["completed_at"] = to_iso_z(workflow_details["completed_at"])
            workflow_details["metadata"] = await MemoryUtils.deserialize(workflow_details.get("metadata"))

            # Fetch tags
            workflow_details["tags"] = []
            tags_cursor = await conn.execute("SELECT t.name FROM tags t JOIN workflow_tags wt ON t.tag_id = wt.tag_id WHERE wt.workflow_id = ?", (workflow_id,))
            workflow_details["tags"] = [row["name"] for row in await tags_cursor.fetchall()]
            await tags_cursor.close()

            # --- Get Actions ---
            if include_actions:
                workflow_details["actions"] = []
                actions_query = """
                SELECT a.*, GROUP_CONCAT(t.name) as tags_str
                FROM actions a
                LEFT JOIN action_tags at ON a.action_id = at.action_id
                LEFT JOIN tags t ON at.tag_id = t.tag_id
                WHERE a.workflow_id = ?
                GROUP BY a.action_id
                ORDER BY a.sequence_number ASC
                """
                actions_cursor = await conn.execute(actions_query, (workflow_id,))
                async for row in actions_cursor:
                    action = dict(row)
                    if action.get("started_at"):
                        action["started_at"] = to_iso_z(action["started_at"])
                    if action.get("completed_at"):
                        action["completed_at"] = to_iso_z(action["completed_at"])
                    action["tool_args"] = await MemoryUtils.deserialize(action.get("tool_args"))
                    action["tool_result"] = await MemoryUtils.deserialize(action.get("tool_result"))
                    action["tags"] = row["tags_str"].split(',') if row["tags_str"] else []
                    action.pop("tags_str", None) # Remove the concatenated string
                    workflow_details["actions"].append(action)
                await actions_cursor.close()

            # --- Get Artifacts ---
            if include_artifacts:
                workflow_details["artifacts"] = []
                artifacts_query = """
                SELECT a.*, GROUP_CONCAT(t.name) as tags_str
                FROM artifacts a
                LEFT JOIN artifact_tags att ON a.artifact_id = att.artifact_id
                LEFT JOIN tags t ON att.tag_id = t.tag_id
                WHERE a.workflow_id = ?
                GROUP BY a.artifact_id
                ORDER BY a.created_at ASC
                """
                artifacts_cursor = await conn.execute(artifacts_query, (workflow_id,))
                async for row in artifacts_cursor:
                    artifact = dict(row)
                    if artifact.get("created_at"):
                        artifact["created_at"] = to_iso_z(artifact["created_at"])
                    artifact["metadata"] = await MemoryUtils.deserialize(artifact.get("metadata"))
                    artifact["is_output"] = bool(artifact["is_output"])
                    artifact["tags"] = row["tags_str"].split(',') if row["tags_str"] else []
                    artifact.pop("tags_str", None)
                    if artifact.get("content") and len(artifact["content"]) > 200:
                        artifact["content_preview"] = artifact["content"][:197] + "..."
                    workflow_details["artifacts"].append(artifact)
                await artifacts_cursor.close()

            # --- Get Thought Chains & Thoughts ---
            if include_thoughts:
                workflow_details["thought_chains"] = []
                chain_cursor = await conn.execute("SELECT * FROM thought_chains WHERE workflow_id = ? ORDER BY created_at ASC", (workflow_id,))
                async for chain_row_data in chain_cursor:
                    thought_chain = dict(chain_row_data)
                    if thought_chain.get("created_at"):
                        thought_chain["created_at"] = to_iso_z(thought_chain["created_at"])
                    thought_chain["thoughts"] = []
                    thought_cursor = await conn.execute("SELECT * FROM thoughts WHERE thought_chain_id = ? ORDER BY sequence_number ASC", (thought_chain["thought_chain_id"],))
                    async for thought_row_data in thought_cursor:
                        thought = dict(thought_row_data)
                        if thought.get("created_at"):
                            thought["created_at"] = to_iso_z(thought["created_at"])
                        thought_chain["thoughts"].append(thought)
                    await thought_cursor.close()
                    workflow_details["thought_chains"].append(thought_chain)
                await chain_cursor.close()

            # --- Get Recent/Important Memories (Optional) ---
            if include_memories:
                 memories_query = """
                 SELECT memory_id, content, memory_type, memory_level, importance, created_at
                 FROM memories
                 WHERE workflow_id = ?
                 ORDER BY importance DESC, created_at DESC
                 LIMIT ?
                 """
                 workflow_details["memories_sample"] = []
                 mem_cursor = await conn.execute(memories_query, (workflow_id, memories_limit))
                 async for row in mem_cursor:
                      mem = dict(row)
                      if mem.get("created_at"):
                          mem["created_at_iso"] = to_iso_z(mem["created_at"])
                      mem["created_at_unix"] = mem.get("created_at") 
                      if mem.get("content") and len(mem["content"]) > 150:
                          mem["content_preview"] = mem["content"][:147] + "..."
                      workflow_details["memories_sample"].append(mem)
                 await mem_cursor.close()

            workflow_details["success"] = True
            logger.info(f"Retrieved details for workflow {workflow_id}", emoji_key="books")
            return workflow_details

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error getting workflow details for {workflow_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to get workflow details: {str(e)}") from e
    

# --- 9. Action Details ---
@with_tool_metrics
@with_error_handling
async def get_recent_actions(
    workflow_id: str,
    limit: int = 5,
    action_type: Optional[str] = None,
    status: Optional[str] = None,
    include_tool_results: bool = True,
    include_reasoning: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves the most recent actions for a workflow, optionally filtered.

    Use this tool to refresh your understanding of what has been done recently in a workflow.
    This helps maintain context when working on complex tasks or when resuming a workflow
    after an interruption. By default, includes tool results and reasoning.

    Args:
        workflow_id: The ID of the workflow.
        limit: (Optional) Maximum number of actions to return. Default 5.
        action_type: (Optional) Filter by action type (e.g., 'tool_use', 'reasoning').
        status: (Optional) Filter by action status (e.g., 'completed', 'failed').
        include_tool_results: (Optional) Whether to include tool results. Default True.
        include_reasoning: (Optional) Whether to include reasoning. Default True.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        A dictionary containing the list of recent actions:
        {
            "actions": [
                {
                    "action_id": "uuid-string",
                    "action_type": "tool_use",
                    "title": "Reading customer data",
                    "tool_name": "read_file",
                    "tool_args": { "path": "example.txt" },
                    "tool_result": { "content": "file content..." }, // Included by default
                    "reasoning": "Reading this file to extract project requirements", // Included by default
                    "status": "completed",
                    "started_at": "2025-04-13T12:34:56Z",
                    "completed_at": "2025-04-13T12:35:01Z",
                    "sequence_number": 3,
                    "parent_action_id": null,
                    "tags": ["data_processing"]
                },
                ...
            ],
            "workflow_title": "Data Analysis Project",
            "workflow_id": "uuid-string",
            "success": true
        }

    Raises:
        ToolInputError: If the workflow doesn't exist or parameters are invalid.
        ToolError: If the database operation fails.
    """
    try:
        # --- Validations ---
        if not isinstance(limit, int) or limit < 1:
            raise ToolInputError("Limit must be a positive integer", param_name="limit")
        if action_type:
            try: 
                ActionType(action_type.lower())
            except ValueError as e: 
                raise ToolInputError(f"Invalid action_type '{action_type}'. Must be one of: {[t.value for t in ActionType]}", param_name="action_type") from e
        if status:
            try: 
                ActionStatus(status.lower())
            except ValueError as e: 
                raise ToolInputError(f"Invalid status '{status}'. Must be one of: {[s.value for s in ActionStatus]}", param_name="status") from e

        async with DBConnection(db_path) as conn:
            # --- Check Workflow & Get Title ---
            cursor = await conn.execute("SELECT title FROM workflows WHERE workflow_id = ?", (workflow_id,))
            wf_row = await cursor.fetchone()
            await cursor.close()
            if not wf_row:
                 raise ToolInputError(f"Workflow {workflow_id} not found", param_name="workflow_id")
            workflow_title = wf_row["title"]

            # --- Build Query ---
            select_fields = [
                "a.action_id", "a.action_type", "a.title", "a.tool_name", "a.tool_args",
                "a.status", "a.started_at", "a.completed_at", "a.sequence_number", "a.parent_action_id",
                "GROUP_CONCAT(t.name) as tags_str"
            ]
            if include_reasoning:
                select_fields.append("a.reasoning")
            if include_tool_results: 
                select_fields.append("a.tool_result")

            query = f"SELECT {', '.join(select_fields)} FROM actions a LEFT JOIN action_tags at ON a.action_id = at.action_id LEFT JOIN tags t ON at.tag_id = t.tag_id WHERE a.workflow_id = ?"
            params: List[Any] = [workflow_id]

            if action_type:
                 query += " AND a.action_type = ?"
                 params.append(action_type.lower())
            if status:
                 query += " AND a.status = ?"
                 params.append(status.lower())

            query += " GROUP BY a.action_id ORDER BY a.sequence_number DESC LIMIT ?"
            params.append(limit)

            # --- Execute Query & Process Results ---
            actions_list = []
            cursor = await conn.execute(query, params)
            async for row in cursor:
                action = dict(row)
                if action.get("started_at"):
                    action["started_at"] = to_iso_z(action["started_at"])
                if action.get("completed_at"):
                    action["completed_at"] = to_iso_z(action["completed_at"])

                action["tags"] = row["tags_str"].split(',') if row["tags_str"] else []
                action.pop("tags_str", None)

                if "tool_args" in action: 
                    action["tool_args"] = await MemoryUtils.deserialize(action.get("tool_args"))
                if "tool_result" in action: 
                    action["tool_result"] = await MemoryUtils.deserialize(action.get("tool_result"))

                actions_list.append(action)
            await cursor.close()

            # --- Prepare Final Result ---
            result = {
                "actions": actions_list,
                "workflow_title": workflow_title,
                "workflow_id": workflow_id,
                "success": True
            }
            logger.info(f"Retrieved {len(actions_list)} recent actions for workflow {workflow_id}", emoji_key="rewind")
            return result

    except ToolInputError:
        raise # Propagate validation errors
    except Exception as e:
        logger.error(f"Error getting recent actions for workflow {workflow_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to get recent actions: {str(e)}") from e


# --- 10. Artifact Details ---
@with_tool_metrics
@with_error_handling
async def get_artifacts(
    workflow_id: str,
    artifact_type: Optional[str] = None,
    tag: Optional[str] = None,
    is_output: Optional[bool] = None,
    include_content: bool = False, # Default False for list view
    limit: int = 10,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves artifacts created during a workflow, optionally filtered.
       Timestamps are returned as ISO 8601 strings.
    """
    try:
        # Validations
        if limit < 1: 
            raise ToolInputError("Limit must be >= 1", param_name="limit")
        if artifact_type:
            try:
                ArtifactType(artifact_type.lower())
            except ValueError as e: 
                raise ToolInputError(f"Invalid artifact_type: {artifact_type}", param_name="artifact_type") from e

        async with DBConnection(db_path) as conn:
            # Check workflow
            cursor = await conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,))
            wf_exists = await cursor.fetchone()
            await cursor.close()
            if not wf_exists: 
                raise ToolInputError(f"Workflow {workflow_id} not found", param_name="workflow_id")

            # Build query
            select_cols = "a.artifact_id, a.action_id, a.artifact_type, a.name, a.description, a.path, a.metadata, a.created_at, a.is_output, GROUP_CONCAT(t.name) as tags_str"
            if include_content: 
                select_cols += ", a.content"

            query = f"SELECT {select_cols} FROM artifacts a"
            joins = " LEFT JOIN artifact_tags att ON a.artifact_id = att.artifact_id LEFT JOIN tags t ON att.tag_id = t.tag_id"
            where_clauses = ["a.workflow_id = ?"]
            params = [workflow_id]

            if tag:
                 if "LEFT JOIN artifact_tags" not in joins:
                      joins += " LEFT JOIN artifact_tags att ON a.artifact_id = att.artifact_id LEFT JOIN tags t ON att.tag_id = t.tag_id"
                 where_clauses.append("t.name = ?")
                 params.append(tag)

            if artifact_type:
                 where_clauses.append("a.artifact_type = ?")
                 params.append(artifact_type.lower())
            if is_output is not None:
                 where_clauses.append("a.is_output = ?")
                 params.append(1 if is_output else 0)

            where_sql = " WHERE " + " AND ".join(where_clauses)
            group_by = " GROUP BY a.artifact_id"
            order_by = " ORDER BY a.created_at DESC"
            limit_sql = " LIMIT ?"
            params.append(limit)

            final_query = query + joins + where_sql + group_by + order_by + limit_sql

            artifacts_list = []
            cursor = await conn.execute(final_query, params)
            async for row in cursor:
                artifact = dict(row)
                if artifact.get("created_at"):
                    artifact["created_at"] = to_iso_z(artifact["created_at"])
                artifact["metadata"] = await MemoryUtils.deserialize(artifact.get("metadata"))
                artifact["is_output"] = bool(artifact["is_output"])
                artifact["tags"] = row["tags_str"].split(',') if row["tags_str"] else []
                artifact.pop("tags_str", None)
                if not include_content and "content" in artifact and artifact.get("content"): # Check key exists before accessing
                    if len(artifact["content"]) > 100:
                        artifact["content_preview"] = artifact["content"][:97] + "..."
                    if not include_content:
                        del artifact["content"] # Remove original content if not requested
                artifacts_list.append(artifact)
            await cursor.close()

            result = {
                "artifacts": artifacts_list,
                "workflow_id": workflow_id,
                "success": True
            }
            logger.info(f"Retrieved {len(artifacts_list)} artifacts for workflow {workflow_id}", emoji_key="open_file_folder")
            return result

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error getting artifacts: {e}", exc_info=True)
        raise ToolError(f"Failed to get artifacts: {str(e)}") from e

@with_tool_metrics
@with_error_handling
async def get_artifact_by_id(
    artifact_id: str,
    include_content: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves a specific artifact by its ID.
       Timestamps are returned as ISO 8601 strings.
    """
    if not artifact_id:
        raise ToolInputError("Artifact ID required.", param_name="artifact_id")
    try:
        async with DBConnection(db_path) as conn:
             select_cols = "a.*, GROUP_CONCAT(t.name) as tags_str"
             query = f"""
             SELECT {select_cols}
             FROM artifacts a
             LEFT JOIN artifact_tags att ON a.artifact_id = att.artifact_id
             LEFT JOIN tags t ON att.tag_id = t.tag_id
             WHERE a.artifact_id = ?
             GROUP BY a.artifact_id
             """
             cursor = await conn.execute(query, (artifact_id,))
             row = await cursor.fetchone()
             await cursor.close()
             if not row:
                  raise ToolInputError(f"Artifact {artifact_id} not found.", param_name="artifact_id")

             artifact = dict(row)
             if artifact.get("created_at"):
                 artifact["created_at"] = to_iso_z(artifact["created_at"])
             artifact["metadata"] = await MemoryUtils.deserialize(artifact.get("metadata"))
             artifact["is_output"] = bool(artifact["is_output"])
             artifact["tags"] = row["tags_str"].split(',') if row["tags_str"] else []
             artifact.pop("tags_str", None)

             if not include_content:
                  if "content" in artifact: # Check key exists before deleting
                      del artifact["content"]

             # Update access stats for related memory if possible
             mem_cursor = await conn.execute("SELECT memory_id, workflow_id FROM memories WHERE artifact_id = ?", (artifact_id,))
             mem_row = await mem_cursor.fetchone()
             await mem_cursor.close()
             if mem_row:
                 # Use workflow_id from memory for logging if artifact one missing? No, artifact should have one.
                 artifact_workflow_id = artifact.get("workflow_id") # Get workflow_id from artifact data
                 if artifact_workflow_id:
                     await MemoryUtils._update_memory_access(conn, mem_row["memory_id"])
                     await MemoryUtils._log_memory_operation(conn, artifact_workflow_id, "access_via_artifact", mem_row["memory_id"], None, {"artifact_id": artifact_id})
                     await conn.commit()
                 else:
                     logger.warning(f"Cannot log memory access via artifact {artifact_id} as workflow_id is missing from artifact record.")

             artifact["success"] = True
             logger.info(f"Retrieved artifact {artifact_id}", emoji_key="page_facing_up")
             return artifact

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error retrieving artifact {artifact_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to retrieve artifact: {str(e)}") from e

# --- 11. Thought Details ---
@with_tool_metrics
@with_error_handling
async def create_thought_chain(
    workflow_id: str,
    title: str,
    initial_thought: Optional[str] = None,
    initial_thought_type: str = "goal",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Creates a new reasoning chain for tracking related thoughts.

    This operation is atomic: the chain and optional initial thought are
    created within a single database transaction.

    Args:
        workflow_id: The ID of the workflow this chain belongs to.
        title: A descriptive title for the thought chain.
        initial_thought: (Optional) Content for the first thought in the chain.
        initial_thought_type: (Optional) Type for the initial thought. Default 'goal'.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing information about the created thought chain.
        Timestamps are returned as ISO 8601 strings.
        {
            "thought_chain_id": "uuid-string",
            "workflow_id": "workflow-uuid",
            "title": "Reasoning for X",
            "created_at": "iso-timestampZ",
            "initial_thought_id": "uuid-string" | None,
            "success": true
        }

    Raises:
        ToolInputError: If title is empty, type is invalid, or workflow not found.
        ToolError: If the database operation fails.
    """
    try:
        if not title:
            raise ToolInputError("Thought chain title required", param_name="title")
        initial_thought_type_enum = None
        if initial_thought:
            try:
                initial_thought_type_enum = ThoughtType(initial_thought_type.lower())
            except ValueError as e:
                valid_types = [t.value for t in ThoughtType]
                raise ToolInputError(f"Invalid initial_thought_type '{initial_thought_type}'. Must be one of: {', '.join(valid_types)}", param_name="initial_thought_type") from e

        thought_chain_id = MemoryUtils.generate_id()
        now_unix = int(time.time())
        thought_id = None # Initialize thought_id

        # Use the transaction manager for atomicity
        db_manager = DBConnection(db_path)
        async with db_manager.transaction() as conn:
            # Check workflow exists (using the transaction connection)
            cursor = await conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,))
            wf_exists = await cursor.fetchone()
            await cursor.close() # Close cursor after fetch
            if not wf_exists:
                raise ToolInputError(f"Workflow {workflow_id} not found", param_name="workflow_id")

            # Insert chain using Unix timestamp
            await conn.execute(
                "INSERT INTO thought_chains (thought_chain_id, workflow_id, title, created_at) VALUES (?, ?, ?, ?)",
                (thought_chain_id, workflow_id, title, now_unix)
            )
            # Update workflow using Unix timestamp
            await conn.execute(
                "UPDATE workflows SET updated_at = ?, last_active = ? WHERE workflow_id = ?",
                (now_unix, now_unix, workflow_id)
            )

            # Add initial thought if specified, passing the transaction connection
            if initial_thought and initial_thought_type_enum:
                thought_result = await record_thought(
                    workflow_id=workflow_id,
                    content=initial_thought,
                    thought_type=initial_thought_type_enum.value,
                    thought_chain_id=thought_chain_id,
                    db_path=db_path, # Still pass db_path in case record_thought needs it for other reasons
                    conn=conn # Pass the active transaction connection
                )
                thought_id = thought_result.get("thought_id")

        # Prepare result dictionary outside the transaction block
        result = {
            "thought_chain_id": thought_chain_id,
            "workflow_id": workflow_id,
            "title": title,
            "created_at": to_iso_z(now_unix), # Format the timestamp used for creation for output
            "initial_thought_id": thought_id, # Include if created
            "success": True
        }
        logger.info(f"Created thought chain '{title}' ({thought_chain_id}) in workflow {workflow_id}", emoji_key="thought_balloon")
        return result

    except ToolInputError:
        raise
    except Exception as e:
        # Log the error context
        logger.error(f"Error creating thought chain '{title}' for workflow {workflow_id}: {e}", exc_info=True)
        # Re-raise as a ToolError for consistent error handling
        raise ToolError(f"Failed to create thought chain: {str(e)}") from e

@with_tool_metrics
@with_error_handling
async def get_thought_chain(
    thought_chain_id: str,
    include_thoughts: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves a thought chain and optionally its thoughts.
    """
    if not thought_chain_id:
        raise ToolInputError("Thought chain ID required.", param_name="thought_chain_id")
    try:
        async with DBConnection(db_path) as conn:
             # Get chain info
             cursor = await conn.execute("SELECT * FROM thought_chains WHERE thought_chain_id = ?", (thought_chain_id,))
             chain_row = await cursor.fetchone()
             await cursor.close()
             if not chain_row:
                 raise ToolInputError(f"Thought chain {thought_chain_id} not found.", param_name="thought_chain_id")

             thought_chain_details = dict(chain_row)
             if thought_chain_details.get("created_at"):
                 thought_chain_details["created_at"] = to_iso_z(thought_chain_details["created_at"])

             # Get thoughts
             thought_chain_details["thoughts"] = []
             if include_thoughts:
                 thought_cursor = await conn.execute("SELECT * FROM thoughts WHERE thought_chain_id = ? ORDER BY sequence_number ASC", (thought_chain_id,))
                 async for row in thought_cursor:
                      thought = dict(row)
                      if thought.get("created_at"):
                          thought["created_at"] = to_iso_z(thought["created_at"])
                      thought_chain_details["thoughts"].append(thought)
                 await thought_cursor.close()

             thought_chain_details["success"] = True
             logger.info(f"Retrieved thought chain {thought_chain_id} with {len(thought_chain_details.get('thoughts',[]))} thoughts", emoji_key="left_speech_bubble")
             return thought_chain_details

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error getting thought chain {thought_chain_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to get thought chain: {str(e)}") from e

# ======================================================
# Helper Function for Working Memory Management (Adapted from cognitive_memory)
# ======================================================

async def _add_to_active_memories(conn: aiosqlite.Connection, context_id: str, memory_id: str) -> bool:
    """Adds a memory to the active memories list for a context, enforcing size limits.
       Internal helper function. Assumes context_id exists.

    Args:
        conn: Active database connection.
        context_id: Context ID.
        memory_id: Memory ID to add.

    Returns:
        True if successful (even if no change needed), False otherwise.
    """
    try:
        # Get current active memories and workflow_id
        async with conn.execute("SELECT workflow_id, active_memories FROM cognitive_states WHERE id = ?", (context_id,)) as cursor:
            row = await cursor.fetchone()
            if not row:
                 logger.warning(f"Context {context_id} not found while trying to add memory {memory_id}.")
                 return False # Context must exist
            workflow_id = row["workflow_id"]
            active_memories = await MemoryUtils.deserialize(row["active_memories"]) or []

        if memory_id in active_memories:
            return True # Already present

        # Check if memory exists before adding
        async with conn.execute("SELECT 1 FROM memories WHERE memory_id = ?", (memory_id,)) as cursor:
            if not await cursor.fetchone():
                 logger.warning(f"Memory {memory_id} not found, cannot add to active context {context_id}.")
                 return False

        removed_id = None
        if len(active_memories) >= MAX_WORKING_MEMORY_SIZE:
            if not active_memories: # Should not happen if size >= limit, but safeguard
                 logger.warning(f"Working memory limit reached ({MAX_WORKING_MEMORY_SIZE}) but list is empty for context {context_id}.")
                 # We can still add the new one if the list is empty
                 active_memories.append(memory_id)

            else:
                 # Fetch relevance scores for current working memories
                 placeholders = ', '.join(['?'] * len(active_memories))
                 query = f"""
                 SELECT memory_id,
                        compute_memory_relevance(importance, confidence, created_at, access_count, last_accessed) as relevance
                 FROM memories
                 WHERE memory_id IN ({placeholders})
                 """
                 relevance_scores = []
                 async with conn.execute(query, active_memories) as cursor:
                     async for row in cursor:
                         relevance_scores.append((row["memory_id"], row["relevance"]))

                 if relevance_scores:
                     # Sort by relevance (ascending) to find the least relevant
                     relevance_scores.sort(key=lambda x: x[1])
                     removed_id = relevance_scores[0][0]
                     try:
                          active_memories.remove(removed_id)
                          logger.debug(f"Removed least relevant memory {removed_id} from context {context_id} working memory.")
                     except ValueError:
                          logger.warning(f"Tried to remove {removed_id} from active memory {context_id}, but it was not found.")

                     # Log the removal
                     await MemoryUtils._log_memory_operation(conn, workflow_id, "remove_from_working", removed_id, None, {
                         "context_id": context_id, "reason": "working_memory_limit", "removed_relevance": relevance_scores[0][1]
                     })

                 # Add the new memory AFTER removing the old one
                 active_memories.append(memory_id)

        else:
            # Space available, just append
            active_memories.append(memory_id)

        # Update the cognitive state
        await conn.execute(
            "UPDATE cognitive_states SET active_memories = ?, last_active = ? WHERE id = ?",
            (await MemoryUtils.serialize(active_memories), int(time.time()), context_id) # Use Unix timestamp for last_active
        )
        logger.debug(f"Added memory {memory_id} to context {context_id} working memory. New count: {len(active_memories)}")
        # Log the addition
        await MemoryUtils._log_memory_operation(conn, workflow_id, "add_to_working", memory_id, None, {"context_id": context_id})

        return True

    except Exception as e:
        logger.error(f"Error adding memory {memory_id} to active context {context_id}: {e}", exc_info=True)
        return False

# --- 12. Working Memory Management (Adapted from cognitive_memory) ---

@with_tool_metrics
@with_error_handling
async def get_working_memory(
    context_id: str,
    include_content: bool = True,
    include_links: bool = True, # Default to True now for full feature parity
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves the current working memory (active memories) for a given context.

    Working memory holds the information the agent is actively using. This tool fetches
    the details of memories currently marked as active for a specific context ID,
    including their outgoing links if requested. Access statistics are updated.

    Args:
        context_id: The context identifier (e.g., conversation ID, session ID).
        include_content: (Optional) Whether to include the full content of memories. Default True.
        include_links: (Optional) Whether to include outgoing links from memories. Default True.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing the working memory details:
        {
            "context_id": "context-uuid",
            "workflow_id": "workflow-uuid",
            "focal_memory_id": "memory-uuid" | None,
            "working_memories": [
                {
                    "memory_id": "...",
                    ...,
                    "links": [ { "target_memory_id": "...", "link_type": "...", ... } ] # If include_links=True
                },
                ...
            ],
            "success": true,
            "processing_time": 0.123
        }

    Raises:
        ToolInputError: If the context ID is not provided or not found.
        ToolError: If the database operation fails.
    """
    if not context_id:
        raise ToolInputError("Context ID required.", param_name="context_id")
    start_time = time.time()

    try:
        async with DBConnection(db_path) as conn:
            # --- 1. Get Cognitive State & Active Memory IDs ---
            async with conn.execute("SELECT * FROM cognitive_states WHERE id = ?", (context_id,)) as cursor:
                state_row = await cursor.fetchone()
                if not state_row:
                    # Return empty if context doesn't exist yet
                    logger.warning(f"Cognitive state for context {context_id} not found. Returning empty working memory.")
                    return {
                        "context_id": context_id,
                        "workflow_id": None,
                        "focal_memory_id": None,
                        "working_memories": [],
                        "success": True,
                        "processing_time": time.time() - start_time
                    }

                state = dict(state_row)
                workflow_id = state.get("workflow_id") # Should exist if state exists
                focal_memory_id = state.get("focal_memory_id")
                active_memory_ids = await MemoryUtils.deserialize(state.get("active_memories")) or []

            working_memories_list = []
            if active_memory_ids:
                # --- 2. Fetch Memory Details ---
                placeholders = ', '.join(['?'] * len(active_memory_ids))
                select_cols_list = [
                    "memory_id", "workflow_id", "description", "memory_type",
                    "memory_level", "importance", "confidence", "created_at",
                    "tags", "action_id", "thought_id", "artifact_id",
                    "reasoning", "source", "context", "updated_at",
                    "last_accessed", "access_count", "ttl", "embedding_id"
                ]
                if include_content:
                    select_cols_list.append("content")
                select_cols = ", ".join(select_cols_list)

                memory_map = {} # Use a map for efficient link attachment later
                query = f"SELECT {select_cols} FROM memories WHERE memory_id IN ({placeholders})"
                async with conn.execute(query, active_memory_ids) as cursor:
                     rows = await cursor.fetchall()
                     for row in rows:
                          mem_dict = dict(row)
                          mem_dict["tags"] = await MemoryUtils.deserialize(mem_dict.get("tags"))
                          mem_dict["context"] = await MemoryUtils.deserialize(mem_dict.get("context")) # Also deserialize context
                          # Keep original unix timestamps, add suffix for clarity in result
                          mem_dict["created_at_unix"] = row["created_at"]
                          mem_dict["updated_at_unix"] = row["updated_at"]
                          mem_dict["last_accessed_unix"] = row["last_accessed"]
                          memory_map[row["memory_id"]] = mem_dict

                # --- 3. Fetch Links if Requested ---
                if include_links:
                    links_map = defaultdict(list)
                    # Select all relevant link fields
                    links_query = f"""
                        SELECT source_memory_id, target_memory_id, link_type, strength, description, link_id, created_at
                        FROM memory_links
                        WHERE source_memory_id IN ({placeholders})
                    """
                    async with conn.execute(links_query, active_memory_ids) as link_cursor:
                        async for link_row in link_cursor:
                            link_data = dict(link_row)
                            link_data["created_at_unix"] = link_row["created_at"] # Add unix timestamp consistency
                            links_map[link_row["source_memory_id"]].append(link_data)

                    # Attach links to memories
                    for mem_id in memory_map:
                        memory_map[mem_id]["links"] = links_map.get(mem_id, []) # Attach empty list if no links

                # --- 4. Reconstruct List & Update Access Stats ---
                for mem_id in active_memory_ids:
                    if mem_id in memory_map:
                        # Add the fully detailed memory to the list
                        working_memories_list.append(memory_map[mem_id])

                        # --- Update Access Time & Log Operation ---
                        # This is considered an access event
                        await MemoryUtils._update_memory_access(conn, mem_id)
                        await MemoryUtils._log_memory_operation(
                            conn,
                            workflow_id, # Use the workflow_id from the state
                            "access_working", # Specific operation type
                            mem_id,
                            None, # No specific action directly caused this retrieval
                            {"context_id": context_id}
                        )

                # --- 5. Commit Access Updates ---
                await conn.commit()

            # --- 6. Return Result ---
            processing_time = time.time() - start_time
            logger.info(f"Retrieved {len(working_memories_list)} working memories for context {context_id}", emoji_key="brain")
            return {
                "context_id": context_id,
                "workflow_id": workflow_id,
                "focal_memory_id": focal_memory_id,
                "working_memories": working_memories_list, # List contains full memory dicts
                "success": True,
                "processing_time": processing_time
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error getting working memory for {context_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to get working memory: {str(e)}") from e


@with_tool_metrics
@with_error_handling
async def focus_memory(
    memory_id: str,
    context_id: str,
    add_to_working: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Sets a specific memory as the current focus for a context.

    Marks a memory as the primary item of attention, which can influence subsequent
    retrieval or reasoning. Optionally adds the memory to the working set if not present.

    Args:
        memory_id: ID of the memory to focus on.
        context_id: The context identifier.
        add_to_working: (Optional) If True, add the memory to the working set if it's
                       not already there (may evict another memory). Default True.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary confirming the focus update.
        {
            "context_id": "context-uuid",
            "focused_memory_id": "memory-uuid",
            "workflow_id": "workflow-uuid",
            "added_to_working": true | false,
            "success": true,
            "processing_time": 0.05
        }

    Raises:
        ToolInputError: If memory or context doesn't exist.
        ToolError: If the database operation fails.
    """
    if not memory_id: 
        raise ToolInputError("Memory ID required.", param_name="memory_id")
    if not context_id: 
        raise ToolInputError("Context ID required.", param_name="context_id")
    start_time = time.time()

    try:
        async with DBConnection(db_path) as conn:
            # Check memory exists and get its workflow_id
            async with conn.execute("SELECT workflow_id FROM memories WHERE memory_id = ?", (memory_id,)) as cursor:
                 mem_row = await cursor.fetchone()
                 if not mem_row: 
                     raise ToolInputError(f"Memory {memory_id} not found.", param_name="memory_id")
                 mem_workflow_id = mem_row["workflow_id"]

            # Check context exists and belongs to the same workflow (or create if missing?)
            # Let's assume context must exist for focus. Agent should manage context creation.
            async with conn.execute("SELECT workflow_id FROM cognitive_states WHERE id = ?", (context_id,)) as cursor:
                 state_row = await cursor.fetchone()
                 if not state_row: 
                     raise ToolInputError(f"Context {context_id} not found.", param_name="context_id")
                 # Optional: Check if context workflow matches memory workflow
                 if state_row["workflow_id"] != mem_workflow_id:
                     raise ToolInputError(f"Memory {memory_id} (wf={mem_workflow_id}) does not belong to context {context_id}'s workflow ({state_row['workflow_id']})")

            # Add to working memory if requested
            added = False
            if add_to_working:
                added = await _add_to_active_memories(conn, context_id, memory_id)
                if not added:
                     # Log the failure but proceed to set focus anyway if memory exists
                     logger.warning(f"Failed to add memory {memory_id} to working set for context {context_id}, but proceeding to set focus.")
                     added = False # Ensure it's marked false in result

            # Update the focal memory in the cognitive state
            now_unix = int(time.time())
            await conn.execute(
                "UPDATE cognitive_states SET focal_memory_id = ?, last_active = ? WHERE id = ?",
                (memory_id, now_unix, context_id)
            )

            # Log focus operation
            await MemoryUtils._log_memory_operation(conn, mem_workflow_id, "focus", memory_id, None, {"context_id": context_id})

            await conn.commit()

            processing_time = time.time() - start_time
            logger.info(f"Set memory {memory_id} as focus for context {context_id}", emoji_key="target")
            return {
                "context_id": context_id,
                "focused_memory_id": memory_id,
                "workflow_id": mem_workflow_id, # Return the memory's workflow ID
                "added_to_working": added,
                "success": True,
                "processing_time": processing_time
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error focusing memory {memory_id} for context {context_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to focus memory: {str(e)}") from e

@with_tool_metrics
@with_error_handling
async def optimize_working_memory(
    context_id: str,
    target_size: int = MAX_WORKING_MEMORY_SIZE,
    strategy: str = "balanced", # Added 'diversity' strategy back
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Optimizes the working memory for a context by retaining the most relevant items.

    Reduces the number of active memories in a context down to the target size based on
    the chosen strategy (balancing importance/recency, prioritizing importance/recency,
    or maximizing diversity of memory types).

    Args:
        context_id: The context identifier.
        target_size: (Optional) The desired number of memories after optimization. Default MAX_WORKING_MEMORY_SIZE.
        strategy: (Optional) Optimization strategy: 'balanced', 'importance', 'recency', 'diversity'. Default 'balanced'.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary detailing the optimization results.
        {
            "context_id": "context-uuid",
            "workflow_id": "workflow-uuid",
            "strategy_used": "balanced",
            "target_size": 15,
            "before_count": 25,
            "after_count": 15,
            "removed_count": 10,
            "retained_memories": ["mem_id1", ...],
            "removed_memories": ["mem_id_removed1", ...],
            "success": true,
            "processing_time": 0.25
        }

    Raises:
        ToolInputError: If context not found or strategy is invalid.
        ToolError: If the database operation fails.
    """
    if not context_id:
        raise ToolInputError("Context ID required.", param_name="context_id")
    if target_size < 0:
        raise ToolInputError("Target size cannot be negative.", param_name="target_size")
    # Added 'diversity' back
    valid_strategies = ["balanced", "importance", "recency", "diversity"]
    if strategy not in valid_strategies:
        raise ToolInputError(f"Strategy must be one of: {', '.join(valid_strategies)}", param_name="strategy")
    start_time = time.time()

    try:
        async with DBConnection(db_path) as conn:
            # Get current context state
            state_cursor = await conn.execute("SELECT workflow_id, active_memories FROM cognitive_states WHERE id = ?", (context_id,))
            state_row = await state_cursor.fetchone()
            if not state_row:
                raise ToolInputError(f"Context {context_id} not found.", param_name="context_id")
            workflow_id = state_row["workflow_id"]
            current_memory_ids = await MemoryUtils.deserialize(state_row["active_memories"]) or []

            before_count = len(current_memory_ids)
            if before_count <= target_size:
                logger.info(f"Working memory for {context_id} already at or below target size ({before_count}/{target_size}). No optimization needed.")
                return {
                    "context_id": context_id, "workflow_id": workflow_id, "strategy_used": strategy,
                    "target_size": target_size, "before_count": before_count, "after_count": before_count,
                    "removed_count": 0, "retained_memories": current_memory_ids, "removed_memories": [],
                    "success": True, "processing_time": time.time() - start_time
                }

            # Fetch details needed for scoring/diversity
            memories_to_consider = []
            if current_memory_ids:
                placeholders = ', '.join(['?'] * len(current_memory_ids))
                query = f"""
                SELECT memory_id, memory_type, importance, confidence, created_at, last_accessed, access_count
                FROM memories WHERE memory_id IN ({placeholders})
                """
                mem_cursor = await conn.execute(query, current_memory_ids)
                async for row in mem_cursor:
                    memories_to_consider.append(dict(row))

            if not memories_to_consider: # Should not happen if before_count > 0, but safeguard
                 logger.warning(f"No memory details found for optimization despite non-empty ID list for context {context_id}.")
                 # Clear the working memory in the state as it seems invalid
                 await conn.execute("UPDATE cognitive_states SET active_memories = ?, last_active = ? WHERE id = ?", ('[]', int(time.time()), context_id))
                 await conn.commit()
                 return {
                     "context_id": context_id, "workflow_id": workflow_id, "strategy_used": strategy,
                     "target_size": target_size, "before_count": before_count, "after_count": 0,
                     "removed_count": before_count, "retained_memories": [], "removed_memories": current_memory_ids,
                     "success": True, "processing_time": time.time() - start_time
                 }

            # Score memories based on strategy
            scored_memories = []
            now = time.time()
            for memory in memories_to_consider:
                mem_id = memory["memory_id"]
                importance = memory["importance"]
                confidence = memory["confidence"]
                created_at = memory["created_at"]
                last_accessed = memory["last_accessed"]
                access_count = memory["access_count"]
                mem_type = memory["memory_type"] # Needed for diversity

                relevance = _compute_memory_relevance(importance, confidence, created_at, access_count, last_accessed)
                recency = 1.0 / (1.0 + (now - (last_accessed or created_at)) / 86400)

                score = 0.0
                if strategy == "balanced":
                    score = relevance
                elif strategy == "importance":
                    score = (importance * 0.7) + (confidence * 0.1) + (recency * 0.1) + (min(1.0, access_count / 5.0) * 0.1)
                elif strategy == "recency":
                    score = (recency * 0.6) + (importance * 0.2) + (min(1.0, access_count / 5.0) * 0.2)
                elif strategy == "diversity":
                    # For diversity, base score is relevance, selection logic handles diversity
                    score = relevance

                scored_memories.append({"id": mem_id, "score": score, "type": mem_type})

            # Select memories to keep
            retained_memory_ids = []
            if strategy == "diversity":
                # Group by type
                type_groups = defaultdict(list)
                for mem in scored_memories:
                    type_groups[mem["type"]].append(mem)

                # Sort each group by score (descending)
                for group in type_groups.values():
                    group.sort(key=lambda x: x["score"], reverse=True)

                # Select round-robin from types until target size is reached
                group_iters = {mem_type: iter(group) for mem_type, group in type_groups.items()}
                active_groups = list(group_iters.keys())

                while len(retained_memory_ids) < target_size and active_groups:
                    group_type_to_select = active_groups.pop(0) # Get next group type
                    try:
                        selected_mem = next(group_iters[group_type_to_select])
                        retained_memory_ids.append(selected_mem["id"])
                        # Add group back to end if it still has items
                        # Check if iterator has more items *without* consuming next one (tricky)
                        # Simplification: Assume we can just add it back for now.
                        # A more robust way would involve peeking or checking group length.
                        active_groups.append(group_type_to_select)
                    except StopIteration:
                        # This group is exhausted, don't add it back
                        pass
                    # Handle infinite loops if target_size > total items
                    if not active_groups and len(retained_memory_ids) < target_size:
                         break # Cannot add more

            else: # Balanced, Importance, Recency - sort by score
                scored_memories.sort(key=lambda x: x["score"], reverse=True)
                retained_memory_ids = [m["id"] for m in scored_memories[:target_size]]

            # Determine removed memories
            removed_memory_ids = list(set(current_memory_ids) - set(retained_memory_ids))
            after_count = len(retained_memory_ids)
            removed_count = len(removed_memory_ids)

            # Update cognitive state
            now_unix = int(time.time())
            await conn.execute(
                "UPDATE cognitive_states SET active_memories = ?, last_active = ? WHERE id = ?",
                (await MemoryUtils.serialize(retained_memory_ids), now_unix, context_id)
            )

            # Log operation
            await MemoryUtils._log_memory_operation(conn, workflow_id, "optimize_working_memory", None, None, {
                "context_id": context_id, "strategy": strategy, "target_size": target_size,
                "before_count": before_count, "after_count": after_count, "removed_count": removed_count
            })

            await conn.commit()

            processing_time = time.time() - start_time
            logger.info(f"Optimized working memory for {context_id} using '{strategy}'. Retained: {after_count}, Removed: {removed_count}", emoji_key="recycle")
            return {
                "context_id": context_id, "workflow_id": workflow_id, "strategy_used": strategy,
                "target_size": target_size, "before_count": before_count, "after_count": after_count,
                "removed_count": removed_count, "retained_memories": retained_memory_ids,
                "removed_memories": removed_memory_ids, "success": True, "processing_time": processing_time
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error optimizing working memory for {context_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to optimize working memory: {str(e)}") from e

# --- 13. Cognitive State Persistence (Ported from agent_memory - Corrected) ---
@with_tool_metrics
@with_error_handling
async def save_cognitive_state(
    workflow_id: str,
    title: str,
    # Accept lists of IDs
    working_memory_ids: List[str],
    focus_area_ids: Optional[List[str]] = None, # Assuming focus areas are primarily memory IDs now
    context_action_ids: Optional[List[str]] = None,
    current_goal_thought_ids: Optional[List[str]] = None, # Assuming goals are thought IDs
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Saves the agent's current cognitive state as a checkpoint for a workflow.

    Creates a snapshot containing active memory IDs, focus memory IDs, relevant action IDs,
    and current goal thought IDs. Marks previous states for the workflow as not the latest.
    Validates that all provided IDs exist within the current workflow context.

    Args:
        workflow_id: The ID of the workflow.
        title: A descriptive title for this state (e.g., "Before attempting API integration").
        working_memory_ids: List of memory IDs currently in active working memory.
        focus_area_ids: (Optional) List of memory IDs representing the agent's current focus.
        context_action_ids: (Optional) List of recent/relevant action IDs providing context.
        current_goal_thought_ids: (Optional) List of thought IDs representing current goals.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary confirming the saved state.
        {
            "state_id": "state-uuid",
            "workflow_id": "workflow-uuid",
            "title": "State title",
            "created_at": "iso-timestamp",
            "success": true,
            "processing_time": 0.08
        }

    Raises:
        ToolInputError: If workflow not found, required parameters missing, or any provided IDs do not exist or belong to the workflow.
        ToolError: If the database operation fails.
    """
    if not title:
        raise ToolInputError("State title required.", param_name="title")

    state_id = MemoryUtils.generate_id()
    now_unix = int(time.time())
    start_time = time.time()

    # Combine all IDs for validation efficiently
    all_memory_ids = set(working_memory_ids + (focus_area_ids or []))
    all_action_ids = set(context_action_ids or [])
    all_thought_ids = set(current_goal_thought_ids or [])

    try:
        async with DBConnection(db_path) as conn:
            # --- Validation Step ---
            # 1. Check workflow exists
            cursor = await conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,))
            wf_exists = await cursor.fetchone()
            await cursor.close()
            if not wf_exists: 
                raise ToolInputError(f"Workflow {workflow_id} not found.", param_name="workflow_id")

            # 2. Validate Memory IDs belong to this workflow
            if all_memory_ids:
                placeholders = ','.join('?' * len(all_memory_ids))
                query = f"SELECT memory_id FROM memories WHERE memory_id IN ({placeholders}) AND workflow_id = ?"
                params = list(all_memory_ids) + [workflow_id]
                cursor = await conn.execute(query, params)
                found_mem_ids = {row['memory_id'] for row in await cursor.fetchall()}
                await cursor.close()
                missing_mem_ids = all_memory_ids - found_mem_ids
                if missing_mem_ids:
                    raise ToolInputError(f"Memory IDs not found or not in workflow {workflow_id}: {missing_mem_ids}", param_name="working_memory_ids/focus_area_ids")

            # 3. Validate Action IDs belong to this workflow
            if all_action_ids:
                placeholders = ','.join('?' * len(all_action_ids))
                query = f"SELECT action_id FROM actions WHERE action_id IN ({placeholders}) AND workflow_id = ?"
                params = list(all_action_ids) + [workflow_id]
                cursor = await conn.execute(query, params)
                found_action_ids = {row['action_id'] for row in await cursor.fetchall()}
                await cursor.close()
                missing_action_ids = all_action_ids - found_action_ids
                if missing_action_ids:
                    raise ToolInputError(f"Action IDs not found or not in workflow {workflow_id}: {missing_action_ids}", param_name="context_action_ids")

            # 4. Validate Thought IDs belong to this workflow
            if all_thought_ids:
                placeholders = ','.join('?' * len(all_thought_ids))
                query = f"""
                    SELECT t.thought_id FROM thoughts t
                    JOIN thought_chains tc ON t.thought_chain_id = tc.thought_chain_id
                    WHERE t.thought_id IN ({placeholders}) AND tc.workflow_id = ?
                """
                params = list(all_thought_ids) + [workflow_id]
                cursor = await conn.execute(query, params)
                found_thought_ids = {row['thought_id'] for row in await cursor.fetchall()}
                await cursor.close()
                missing_thought_ids = all_thought_ids - found_thought_ids
                if missing_thought_ids:
                    raise ToolInputError(f"Thought IDs not found or not in workflow {workflow_id}: {missing_thought_ids}", param_name="current_goal_thought_ids")

            # --- Proceed with Saving State ---
            # Mark previous states as not latest
            await conn.execute("UPDATE cognitive_states SET is_latest = 0 WHERE workflow_id = ?", (workflow_id,))

            # Serialize state data (using the validated lists)
            working_mem_json = await MemoryUtils.serialize(working_memory_ids)
            focus_json = await MemoryUtils.serialize(focus_area_ids or [])
            context_actions_json = await MemoryUtils.serialize(context_action_ids or [])
            current_goals_json = await MemoryUtils.serialize(current_goal_thought_ids or [])

            # Insert new state
            await conn.execute(
                """
                INSERT INTO cognitive_states (state_id, workflow_id, title, working_memory, focus_areas,
                context_actions, current_goals, created_at, is_latest)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (state_id, workflow_id, title, working_mem_json, focus_json,
                 context_actions_json, current_goals_json, now_unix, True) # *** Use now_unix ***
            )

            # Update workflow timestamp
            await conn.execute("UPDATE workflows SET updated_at = ?, last_active = ? WHERE workflow_id = ?", (now_unix, now_unix, workflow_id)) # *** Use now_unix ***

            # Log operation
            log_data = {
                "state_id": state_id, "title": title,
                "working_memory_count": len(working_memory_ids),
                "focus_count": len(focus_area_ids or []),
                "action_context_count": len(context_action_ids or []),
                "goal_count": len(current_goal_thought_ids or [])
            }
            await MemoryUtils._log_memory_operation(conn, workflow_id, "save_state", None, None, log_data)

            await conn.commit()

            result = {
                "state_id": state_id,
                "workflow_id": workflow_id,
                "title": title,
                "created_at": to_iso_z(now_unix),
                "success": True,
                "processing_time": time.time() - start_time,
            }

            logger.info(
                f"Saved cognitive state '{title}' ({state_id}) for workflow {workflow_id}",
                emoji_key="save",
            )

            return result
    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error saving cognitive state: {e}", exc_info=True)
        raise ToolError(f"Failed to save cognitive state: {str(e)}") from e
    

@with_tool_metrics
@with_error_handling
async def load_cognitive_state(
    workflow_id: str,
    state_id: Optional[str] = None, # If None, load latest
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Loads a previously saved cognitive state for a workflow.

    Restores context by retrieving a saved snapshot of working memory, focus, etc.

    Args:
        workflow_id: The ID of the workflow.
        state_id: (Optional) The ID of the specific state to load. If None, loads the latest state.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing the loaded cognitive state data.
        {
            "state_id": "state-uuid",
            "workflow_id": "workflow-uuid",
            "title": "State title",
            "working_memory_ids": ["mem_id1", ...],
            "focus_areas": ["mem_id_focus", "topic description", ...],
            "context_action_ids": ["action_id1", ...],
            "current_goals": ["goal description", "goal_thought_id", ...],
            "created_at": "iso-timestamp",
            "success": true,
            "processing_time": 0.06
        }

    Raises:
        ToolInputError: If the workflow or specified state doesn't exist.
        ToolError: If the database operation fails.
    """
    if not workflow_id:
        raise ToolInputError("Workflow ID required.", param_name="workflow_id")
    start_time = time.time()

    try:
        async with DBConnection(db_path) as conn:
            # Check workflow exists
            cursor = await conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,))
            wf_exists = await cursor.fetchone()
            await cursor.close()
            if not wf_exists:
                raise ToolInputError(f"Workflow {workflow_id} not found.", param_name="workflow_id")

            # Build query
            query = "SELECT * FROM cognitive_states WHERE workflow_id = ?"
            params: List[Any] = [workflow_id]
            if state_id:
                query += " AND state_id = ?"
                params.append(state_id)
            else:
                query += " ORDER BY created_at DESC, is_latest DESC LIMIT 1" # Prefer is_latest if timestamps clash

            # Fetch state
            cursor = await conn.execute(query, params)
            row = await cursor.fetchone()
            await cursor.close()
            if not row:
                 err_msg = f"State {state_id} not found." if state_id else f"No states found for workflow {workflow_id}."
                 raise ToolInputError(err_msg, param_name="state_id" if state_id else "workflow_id")

            state = dict(row)

            # Deserialize data and format timestamp
            created_at_unix = state["created_at"] # Get the stored Unix timestamp
            result = {
                 "state_id": state["state_id"],
                 "workflow_id": state["workflow_id"],
                 "title": state["title"],
                 "working_memory_ids": await MemoryUtils.deserialize(state.get("working_memory")) or [],
                 "focus_areas": await MemoryUtils.deserialize(state.get("focus_areas")) or [],
                 "context_action_ids": await MemoryUtils.deserialize(state.get("context_actions")) or [],
                 "current_goals": await MemoryUtils.deserialize(state.get("current_goals")) or [],
                 "created_at": to_iso_z(created_at_unix),
                 "success": True,
                 "processing_time": time.time() - start_time
            }

            # Log operation
            await MemoryUtils._log_memory_operation(conn, workflow_id, "load_state", None, None, {
                "state_id": state["state_id"], "title": state["title"]
            })

            logger.info(f"Loaded cognitive state '{result['title']}' ({result['state_id']}) for workflow {workflow_id}", emoji_key="inbox_tray")
            return result

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error loading cognitive state: {e}", exc_info=True)
        raise ToolError(f"Failed to load cognitive state: {str(e)}") from e


# --- 14. Comprehensive Context Retrieval (Ported from agent_memory) ---
@with_tool_metrics
@with_error_handling
async def get_workflow_context(
    workflow_id: str,
    recent_actions_limit: int = 10, # Reduced default
    important_memories_limit: int = 5,
    key_thoughts_limit: int = 5,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves a comprehensive context summary for a workflow.

    Combines the latest cognitive state, recent actions, important memories, and key thoughts
    to provide a snapshot for resuming work or understanding the current situation.

    Args:
        workflow_id: The ID of the workflow.
        recent_actions_limit: (Optional) Max number of recent actions to include. Default 10.
        important_memories_limit: (Optional) Max number of important memories to include. Default 5.
        key_thoughts_limit: (Optional) Max number of key thoughts to include. Default 5.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing the combined workflow context.
        {
            "workflow_id": "workflow-uuid",
            "workflow_title": "Workflow Title",
            "workflow_status": "active",
            "workflow_goal": "Goal description",
            "latest_cognitive_state": { ... } | None,
            "recent_actions": [ { ...action_summary... }, ... ],
            "important_memories": [ { ...memory_summary... }, ... ],
            "key_thoughts": [ { ...thought_summary... }, ... ],
            "success": true,
            "processing_time": 0.35
        }

    Raises:
        ToolInputError: If the workflow doesn't exist.
        ToolError: If the database operation fails.
    """
    if not workflow_id: 
        raise ToolInputError("Workflow ID required.", param_name="workflow_id")
    start_time = time.time()

    try:
        async with DBConnection(db_path) as conn:
            # 1. Get basic workflow info
            async with conn.execute("SELECT title, goal, status FROM workflows WHERE workflow_id = ?", (workflow_id,)) as cursor:
                 wf_row = await cursor.fetchone()
                 if not wf_row: 
                     raise ToolInputError(f"Workflow {workflow_id} not found.", param_name="workflow_id")
                 context = {
                     "workflow_id": workflow_id,
                     "workflow_title": wf_row["title"],
                     "workflow_goal": wf_row["goal"],
                     "workflow_status": wf_row["status"]
                 }

            # 2. Get Latest Cognitive State (using load_cognitive_state logic)
            try:
                 # Reusing the logic by calling the tool function
                 context["latest_cognitive_state"] = await load_cognitive_state(workflow_id=workflow_id, state_id=None, db_path=db_path)
                 # Remove success/timing info from nested call result for cleaner context
                 context["latest_cognitive_state"].pop("success", None)
                 context["latest_cognitive_state"].pop("processing_time", None)
            except ToolInputError: # Catch if no state exists
                 context["latest_cognitive_state"] = None
                 logger.info(f"No cognitive state found for workflow {workflow_id} during context retrieval.")
            except Exception as e: # Catch other errors loading state
                 logger.warning(f"Could not load cognitive state for workflow {workflow_id} context: {e}")
                 context["latest_cognitive_state"] = {"error": f"Failed to load state: {e}"}


            # 3. Get Recent Actions (using get_recent_actions logic)
            try:
                actions_result = await get_recent_actions(
                    workflow_id=workflow_id, limit=recent_actions_limit,
                    include_reasoning=False, include_tool_results=False, # Keep context concise
                    db_path=db_path
                )
                context["recent_actions"] = actions_result.get("actions", [])
            except Exception as e:
                logger.warning(f"Could not load recent actions for workflow {workflow_id} context: {e}")
                context["recent_actions"] = [{"error": f"Failed to load actions: {e}"}]


            # 4. Get Important Memories (using query_memories logic)
            try:
                 memories_result = await query_memories(
                     workflow_id=workflow_id, limit=important_memories_limit,
                     sort_by="importance", sort_order="DESC",
                     include_content=False, # Exclude full content for context
                     db_path=db_path
                 )
                 # Extract relevant fields for context
                 context["important_memories"] = [
                     {"memory_id": m["memory_id"], "description": m.get("description"), "memory_type": m.get("memory_type"), "importance": m.get("importance")}
                     for m in memories_result.get("memories", [])
                 ]
            except Exception as e:
                 logger.warning(f"Could not load important memories for workflow {workflow_id} context: {e}")
                 context["important_memories"] = [{"error": f"Failed to load memories: {e}"}]

            # 5. Get Key Thoughts (e.g., latest goals, decisions, summaries from main chain)
            try:
                # Find main thought chain
                 async with conn.execute("SELECT thought_chain_id FROM thought_chains WHERE workflow_id = ? ORDER BY created_at ASC LIMIT 1", (workflow_id,)) as cursor:
                      chain_row = await cursor.fetchone()
                      if chain_row:
                          thought_chain_id = chain_row["thought_chain_id"]
                          # Fetch important thought types, most recent first
                          async with conn.execute(
                              """SELECT thought_type, content, sequence_number, created_at
                                 FROM thoughts WHERE thought_chain_id = ?
                                 AND thought_type IN (?, ?, ?, ?)
                                 ORDER BY sequence_number DESC LIMIT ?""",
                              (thought_chain_id, ThoughtType.GOAL.value, ThoughtType.DECISION.value, ThoughtType.SUMMARY.value, ThoughtType.REFLECTION.value, key_thoughts_limit)
                          ) as thought_cursor:
                               context["key_thoughts"] = [dict(row) for row in await thought_cursor.fetchall()]
                      else:
                          context["key_thoughts"] = []
            except Exception as e:
                 logger.warning(f"Could not load key thoughts for workflow {workflow_id} context: {e}")
                 context["key_thoughts"] = [{"error": f"Failed to load thoughts: {e}"}]


            context["success"] = True
            context["processing_time"] = time.time() - start_time
            logger.info(f"Retrieved context summary for workflow {workflow_id}", emoji_key="compass")
            return context

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error getting workflow context for {workflow_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to get workflow context: {str(e)}") from e

# --- Helper: Scoring for Focus ---
def _calculate_focus_score(memory: Dict, recent_action_ids: List[str], now_unix: int) -> float:
    """Calculates a score for prioritizing focus, based on memory attributes."""
    score = 0.0

    # Base relevance score (importance, confidence, recency, usage)
    relevance = _compute_memory_relevance(
        memory.get("importance", 5.0),
        memory.get("confidence", 1.0),
        memory.get("created_at", now_unix),
        memory.get("access_count", 0),
        memory.get("last_accessed", None)
    )
    score += relevance * 0.6 # Base relevance is weighted heavily

    # Boost for being linked to recent actions
    if memory.get("action_id") and memory["action_id"] in recent_action_ids:
        score += 3.0 # Significant boost if directly related to recent work

    # Boost for certain types often indicating current context
    if memory.get("memory_type") in [MemoryType.QUESTION.value, MemoryType.PLAN.value, MemoryType.INSIGHT.value]:
        score += 1.5

    # Slight boost for higher memory levels (semantic/procedural over episodic)
    if memory.get("memory_level") == MemoryLevel.SEMANTIC.value:
        score += 0.5
    elif memory.get("memory_level") == MemoryLevel.PROCEDURAL.value:
        score += 0.7

    # Ensure score is not negative
    return max(0.0, score)

# --- Tool: Auto Update Focus ---
@with_tool_metrics
@with_error_handling
async def auto_update_focus(
    context_id: str,
    recent_actions_count: int = 3, # How many recent actions to consider influential
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Automatically updates the focal memory for a context based on relevance and recent activity.

    Analyzes memories currently in the working set for the given context. It scores them based
    on importance, confidence, recency, usage, type, and linkage to recent actions.
    The memory with the highest score becomes the new focal memory.

    Args:
        context_id: The context identifier.
        recent_actions_count: (Optional) Number of most recent actions to consider for boosting relevance. Default 3.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary indicating the result of the focus update.
        {
            "context_id": "context-uuid",
            "workflow_id": "workflow-uuid",
            "previous_focal_memory_id": "old-focus-uuid" | None,
            "new_focal_memory_id": "new-focus-uuid" | None,
            "focus_changed": true | false,
            "reason": "Highest score based on relevance and recent activity." | "No suitable memory found." | "Focus unchanged.",
            "success": true,
            "processing_time": 0.1
        }

    Raises:
        ToolInputError: If context not found.
        ToolError: If the database operation fails.
    """
    if not context_id:
        raise ToolInputError("Context ID required.", param_name="context_id")
    if recent_actions_count < 0:
        raise ToolInputError("Recent actions count cannot be negative.", param_name="recent_actions_count")
    start_time = time.time()

    try:
        async with DBConnection(db_path) as conn:
            # --- 1. Get Current Context & Working Memory ---
            async with conn.execute("SELECT workflow_id, focal_memory_id, active_memories FROM cognitive_states WHERE id = ?", (context_id,)) as cursor:
                state_row = await cursor.fetchone()
                if not state_row:
                    raise ToolInputError(f"Context {context_id} not found.", param_name="context_id")
                workflow_id = state_row["workflow_id"]
                previous_focal_id = state_row["focal_memory_id"]
                working_memory_ids = await MemoryUtils.deserialize(state_row["active_memories"]) or []

            if not working_memory_ids:
                logger.info(f"Working memory for context {context_id} is empty. Cannot determine focus.")
                return {
                    "context_id": context_id, "workflow_id": workflow_id, "previous_focal_memory_id": previous_focal_id,
                    "new_focal_memory_id": None, "focus_changed": previous_focal_id is not None,
                    "reason": "Working memory is empty.", "success": True, "processing_time": time.time() - start_time
                }

            # --- 2. Get Details for Working Memories ---
            working_memories_details = []
            placeholders = ', '.join(['?'] * len(working_memory_ids))
            query = f"""
                SELECT memory_id, action_id, memory_type, memory_level, importance, confidence,
                       created_at, last_accessed, access_count
                FROM memories WHERE memory_id IN ({placeholders})
            """
            async with conn.execute(query, working_memory_ids) as cursor:
                 working_memories_details = [dict(row) for row in await cursor.fetchall()]

            # --- 3. Get Recent Action IDs ---
            recent_action_ids = []
            if recent_actions_count > 0:
                async with conn.execute(
                    "SELECT action_id FROM actions WHERE workflow_id = ? ORDER BY sequence_number DESC LIMIT ?",
                    (workflow_id, recent_actions_count)
                ) as cursor:
                     recent_action_ids = [row["action_id"] for row in await cursor.fetchall()]

            # --- 4. Score Memories & Find Best Candidate ---
            now_unix = int(time.time())
            best_candidate_id = None
            highest_score = -1.0

            for memory in working_memories_details:
                score = _calculate_focus_score(memory, recent_action_ids, now_unix)
                # logger.debug(f"Memory {memory['memory_id'][:8]} focus score: {score:.2f}") # Debug logging
                if score > highest_score:
                    highest_score = score
                    best_candidate_id = memory["memory_id"]

            # --- 5. Update Focus if Changed ---
            focus_changed = False
            reason = "Focus unchanged."
            if best_candidate_id and best_candidate_id != previous_focal_id:
                await conn.execute(
                    "UPDATE cognitive_states SET focal_memory_id = ?, last_active = ? WHERE id = ?",
                    (best_candidate_id, now_unix, context_id)
                )
                focus_changed = True
                reason = f"Memory {best_candidate_id[:8]}... has highest score ({highest_score:.2f}) based on relevance and recent activity."
                logger.info(f"Auto-shifting focus for context {context_id} to memory {best_candidate_id}. Previous: {previous_focal_id}", emoji_key="compass")
                # Log the operation
                await MemoryUtils._log_memory_operation(
                    conn, workflow_id, "auto_focus_shift", best_candidate_id, None,
                    {"context_id": context_id, "previous_focus": previous_focal_id, "score": highest_score}
                )
                await conn.commit() # Commit the change
            elif not best_candidate_id:
                 reason = "No suitable memory found in working set to focus on."
                 logger.info(f"Auto-focus update for context {context_id}: No suitable candidate found.")
            else:
                 logger.info(f"Auto-focus update for context {context_id}: Focus remains on {previous_focal_id}.")


            processing_time = time.time() - start_time
            return {
                "context_id": context_id,
                "workflow_id": workflow_id,
                "previous_focal_memory_id": previous_focal_id,
                "new_focal_memory_id": best_candidate_id,
                "focus_changed": focus_changed,
                "reason": reason,
                "success": True,
                "processing_time": processing_time
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error auto-updating focus for context {context_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to auto-update focus: {str(e)}") from e


# --- Tool: Promote Memory Level ---
@with_tool_metrics
@with_error_handling
async def promote_memory_level(
    memory_id: str,
    target_level: Optional[str] = None, # Explicit target level (e.g., 'semantic')
    min_access_count_episodic: int = 5, # Configurable thresholds
    min_confidence_episodic: float = 0.8,
    min_access_count_semantic: int = 10,
    min_confidence_semantic: float = 0.9,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Attempts to promote a memory to a higher cognitive level based on usage and confidence.

    Checks if a memory meets heuristic criteria for promotion (e.g., high access count,
    high confidence) to the next logical level (Episodic -> Semantic, Semantic -> Procedural).
    If criteria are met, the memory's level is updated.

    Args:
        memory_id: The ID of the memory to potentially promote.
        target_level: (Optional) Explicitly specify the level to promote TO. If provided,
                      checks if the memory meets criteria for *that specific level*.
                      If None, checks for promotion to the *next logical* level.
        min_access_count_episodic: (Optional) Min access count to promote Episodic->Semantic. Default 5.
        min_confidence_episodic: (Optional) Min confidence to promote Episodic->Semantic. Default 0.8.
        min_access_count_semantic: (Optional) Min access count to promote Semantic->Procedural. Default 10.
        min_confidence_semantic: (Optional) Min confidence to promote Semantic->Procedural. Default 0.9.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary indicating if promotion occurred and the result.
        {
            "memory_id": "memory-uuid",
            "promoted": true | false,
            "previous_level": "episodic",
            "new_level": "semantic" | None, # None if not promoted
            "reason": "Met criteria: access_count >= 5, confidence >= 0.8" | "Criteria not met." | "Already at highest/target level.",
            "success": true,
            "processing_time": 0.07
        }

    Raises:
        ToolInputError: If memory not found or target level is invalid.
        ToolError: If the database operation fails.
    """
    if not memory_id:
        raise ToolInputError("Memory ID required.", param_name="memory_id")

    target_level_enum = None
    if target_level:
        try:
            target_level_enum = MemoryLevel(target_level.lower())
        except ValueError as e:
            valid_levels = [ml.value for ml in MemoryLevel]
            raise ToolInputError(f"Invalid target_level. Use one of: {valid_levels}", param_name="target_level") from e

    start_time = time.time()

    try:
        async with DBConnection(db_path) as conn:
            # --- 1. Get Current Memory Details ---
            async with conn.execute(
                "SELECT workflow_id, memory_level, memory_type, access_count, confidence, importance FROM memories WHERE memory_id = ?", (memory_id,)
            ) as cursor:
                mem_row = await cursor.fetchone()
                if not mem_row:
                    raise ToolInputError(f"Memory {memory_id} not found.", param_name="memory_id")

                current_level = MemoryLevel(mem_row["memory_level"])
                mem_type = MemoryType(mem_row["memory_type"]) # Get type for procedural check
                access_count = mem_row["access_count"] or 0
                confidence = mem_row["confidence"] or 0.0
                workflow_id = mem_row["workflow_id"] # Needed for logging

            # --- 2. Determine Target Level and Check Criteria ---
            promoted = False
            new_level_enum = None
            reason = "Criteria not met or already at highest/target level."

            # Determine the *next* logical level if no explicit target is given
            potential_next_level = None
            if current_level == MemoryLevel.EPISODIC:
                 potential_next_level = MemoryLevel.SEMANTIC
            elif current_level == MemoryLevel.SEMANTIC and mem_type in [MemoryType.PROCEDURE, MemoryType.SKILL]:
                 # Only allow promotion to procedural if type is appropriate
                 potential_next_level = MemoryLevel.PROCEDURAL

            # Use explicit target level if provided, otherwise use the potential next level
            level_to_check_for = target_level_enum or potential_next_level

            # Check if promotion is possible and criteria are met
            if level_to_check_for and level_to_check_for.value > current_level.value: # Ensure target is actually higher
                criteria_met = False
                criteria_desc = ""

                if level_to_check_for == MemoryLevel.SEMANTIC and current_level == MemoryLevel.EPISODIC:
                    criteria_met = (access_count >= min_access_count_episodic and confidence >= min_confidence_episodic)
                    criteria_desc = f"Met criteria for Semantic: access_count >= {min_access_count_episodic} ({access_count}), confidence >= {min_confidence_episodic} ({confidence:.2f})"
                elif level_to_check_for == MemoryLevel.PROCEDURAL and current_level == MemoryLevel.SEMANTIC:
                    # Add stricter checks for procedural: must be procedure/skill type AND meet usage/confidence
                    if mem_type in [MemoryType.PROCEDURE, MemoryType.SKILL]:
                         criteria_met = (access_count >= min_access_count_semantic and confidence >= min_confidence_semantic)
                         criteria_desc = f"Met criteria for Procedural: type is '{mem_type.value}', access_count >= {min_access_count_semantic} ({access_count}), confidence >= {min_confidence_semantic} ({confidence:.2f})"
                    else:
                         criteria_desc = f"Criteria not met for Procedural: memory type '{mem_type.value}' is not procedure/skill."

                if criteria_met:
                    promoted = True
                    new_level_enum = level_to_check_for
                    reason = criteria_desc
                else:
                    reason = criteria_desc if criteria_desc else f"Criteria not met for promotion to {level_to_check_for.value}."

            elif level_to_check_for and level_to_check_for.value <= current_level.value:
                 reason = f"Memory is already at or above the target level '{level_to_check_for.value}'."
            elif not level_to_check_for:
                 reason = f"Memory is already at the highest promotable level ({current_level.value}) or not eligible for promotion (type: {mem_type.value})."


            # --- 3. Update Memory if Promoted ---
            if promoted and new_level_enum:
                now_unix = int(time.time())
                await conn.execute(
                    "UPDATE memories SET memory_level = ?, updated_at = ? WHERE memory_id = ?",
                    (new_level_enum.value, now_unix, memory_id)
                )
                # Log the promotion
                await MemoryUtils._log_memory_operation(conn, workflow_id, "promote_level", memory_id, None, {
                    "previous_level": current_level.value, "new_level": new_level_enum.value, "reason": reason
                })
                await conn.commit()
                logger.info(f"Promoted memory {memory_id} from {current_level.value} to {new_level_enum.value}", emoji_key="arrow_up")
            else:
                 logger.info(f"Memory {memory_id} not promoted. Reason: {reason}")


            # --- 4. Return Result ---
            processing_time = time.time() - start_time
            return {
                "memory_id": memory_id,
                "promoted": promoted,
                "previous_level": current_level.value,
                "new_level": new_level_enum.value if promoted else None,
                "reason": reason,
                "success": True,
                "processing_time": processing_time
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error promoting memory {memory_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to promote memory: {str(e)}") from e

# --- 15. Memory Update ---
@with_tool_metrics
@with_error_handling
async def update_memory(
    memory_id: str,
    content: Optional[str] = None,
    importance: Optional[float] = None,
    confidence: Optional[float] = None,
    description: Optional[str] = None,
    reasoning: Optional[str] = None,
    tags: Optional[List[str]] = None, # Replaces existing tags if provided
    ttl: Optional[int] = None,
    memory_level: Optional[str] = None,
    regenerate_embedding: bool = False, # Explicit flag to trigger re-embedding
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Updates fields of an existing memory entry.

    Allows modification of content, importance, confidence, tags, etc.
    If content or description is updated and regenerate_embedding=True,
    the embedding will be recalculated.

    Args:
        memory_id: ID of the memory to update.
        content: (Optional) New content for the memory.
        importance: (Optional) New importance score (1.0-10.0).
        confidence: (Optional) New confidence score (0.0-1.0).
        description: (Optional) New description.
        reasoning: (Optional) New reasoning.
        tags: (Optional) List of new tags (replaces existing tags).
        ttl: (Optional) New time-to-live in seconds (0 for permanent).
        memory_level: (Optional) New memory level.
        regenerate_embedding: (Optional) Force regeneration of embedding if content or description changes. Default False.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing the updated memory details (excluding full content unless changed).
        {
            "memory_id": "memory-uuid",
            "updated_fields": ["importance", "tags"],
            "embedding_regenerated": true | false,
            "updated_at_unix": 1678886400,
            "success": true,
            "processing_time": 0.15
        }

    Raises:
        ToolInputError: If memory not found or parameters are invalid.
        ToolError: If the database operation fails.
    """
    if not memory_id: 
        raise ToolInputError("Memory ID required.", param_name="memory_id")
    start_time = time.time()

    # Parameter validations
    if importance is not None and not 1.0 <= importance <= 10.0: 
        raise ToolInputError("Importance must be 1.0-10.0.", param_name="importance")
    if confidence is not None and not 0.0 <= confidence <= 1.0: 
        raise ToolInputError("Confidence must be 0.0-1.0.", param_name="confidence")
    final_tags_json = None
    if tags is not None:
        if not isinstance(tags, list): 
            raise ToolInputError("Tags must be a list.", param_name="tags")
        final_tags_json = json.dumps(list(set(str(t).lower() for t in tags)))
    if memory_level:
        try: 
            MemoryLevel(memory_level.lower())
        except ValueError as e: 
            raise ToolInputError("Invalid memory_level.", param_name="memory_level") from e

    update_clauses = []
    params = []
    updated_fields = []

    # Build dynamic SET clause
    if content is not None: 
        update_clauses.append("content = ?")
        params.append(content)
        updated_fields.append("content")
    if importance is not None: 
        update_clauses.append("importance = ?")
        params.append(importance)
        updated_fields.append("importance")
    if confidence is not None: 
        update_clauses.append("confidence = ?")
        params.append(confidence)
        updated_fields.append("confidence")
    if description is not None: 
        update_clauses.append("description = ?")
        params.append(description)
        updated_fields.append("description")
    if reasoning is not None: 
        update_clauses.append("reasoning = ?")
        params.append(reasoning)
        updated_fields.append("reasoning")
    if final_tags_json is not None: 
        update_clauses.append("tags = ?")
        params.append(final_tags_json)
        updated_fields.append("tags")
    if ttl is not None: 
        update_clauses.append("ttl = ?")
        params.append(ttl)
        updated_fields.append("ttl")
    if memory_level: 
        update_clauses.append("memory_level = ?")
        params.append(memory_level.lower())
        updated_fields.append("memory_level")

    if not update_clauses:
        raise ToolInputError("No fields provided for update.", param_name="content")

    # Always update timestamp
    now_unix = int(time.time())
    update_clauses.append("updated_at = ?")
    params.append(now_unix)
    params.append(memory_id) # For the WHERE clause

    try:
        async with DBConnection(db_path) as conn:
            # Check memory exists and get current description/content if needed for embedding
            current_desc = None
            current_content = None
            needs_embedding_check = regenerate_embedding and ('content' in updated_fields or 'description' in updated_fields)

            async with conn.execute("SELECT workflow_id, description, content FROM memories WHERE memory_id = ?", (memory_id,)) as cursor:
                 mem_row = await cursor.fetchone()
                 if not mem_row: 
                     raise ToolInputError(f"Memory {memory_id} not found.", param_name="memory_id")
                 workflow_id = mem_row["workflow_id"]
                 if needs_embedding_check:
                     current_desc = mem_row["description"]
                     current_content = mem_row["content"]

            # Execute update
            update_sql = f"UPDATE memories SET {', '.join(update_clauses)} WHERE memory_id = ?"
            await conn.execute(update_sql, params)

            # Regenerate embedding if requested and relevant fields changed
            embedding_regenerated = False
            new_embedding_db_id = None
            if needs_embedding_check:
                new_desc = description if 'description' in updated_fields else current_desc
                new_content = content if 'content' in updated_fields else current_content
                text_for_embedding = f"{new_desc}: {new_content}" if new_desc else new_content
                try:
                     new_embedding_db_id = await _store_embedding(conn, memory_id, text_for_embedding)
                     if new_embedding_db_id:
                         embedding_regenerated = True
                         logger.info(f"Regenerated embedding for updated memory {memory_id}", emoji_key="brain")
                     else:
                         logger.warning(f"Embedding regeneration failed for memory {memory_id}")
                except Exception as embed_err:
                     logger.error(f"Error during embedding regeneration for memory {memory_id}: {embed_err}", exc_info=True)

            # Log operation
            log_data = {"updated_fields": updated_fields}
            if embedding_regenerated: 
                log_data["embedding_regenerated"] = True
            await MemoryUtils._log_memory_operation(conn, workflow_id, "update", memory_id, None, log_data)

            await conn.commit()

            processing_time = time.time() - start_time
            logger.info(f"Updated memory {memory_id}. Fields: {', '.join(updated_fields)}.", emoji_key="pencil2")
            return {
                "memory_id": memory_id,
                "updated_fields": updated_fields,
                "embedding_regenerated": embedding_regenerated,
                "updated_at_unix": now_unix,
                "success": True,
                "processing_time": processing_time
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error updating memory {memory_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to update memory: {str(e)}") from e


# ======================================================
# Linked Memories Retrieval
# ======================================================

@with_tool_metrics
@with_error_handling
async def get_linked_memories(
    memory_id: str,
    direction: str = "both",  # "outgoing", "incoming", or "both"
    link_type: Optional[str] = None,  # Optional filter by link type
    limit: int = 10,
    include_memory_details: bool = True,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Retrieves memories linked to/from the specified memory.
    
    Fetches all memories linked to or from a given memory, optionally filtered by link type.
    Can include basic or detailed information about the linked memories.
    
    Args:
        memory_id: ID of the memory to get links for
        direction: Which links to include - "outgoing" (memory_id is the source),
                  "incoming" (memory_id is the target), or "both" (default)
        link_type: Optional filter for specific link types (e.g., "related", "supports")
        limit: Maximum number of links to return per direction (default 10)
        include_memory_details: Whether to include full details of the linked memories
        db_path: Path to the SQLite database file
        
    Returns:
        Dictionary containing the linked memories organized by direction:
        {
            "memory_id": "memory-uuid",
            "links": {
                "outgoing": [
                    {
                        "link_id": "link-uuid",
                        "source_memory_id": "memory-uuid", 
                        "target_memory_id": "linked-memory-uuid",
                        "link_type": "related",
                        "strength": 0.85,
                        "description": "Auto-link based on similarity",
                        "created_at_unix": 1678886400,
                        "target_memory": { # Only if include_memory_details=True
                            "memory_id": "linked-memory-uuid",
                            "description": "Memory description",
                            "memory_type": "observation",
                            ... other memory fields ...
                        }
                    },
                    ... more outgoing links ...
                ],
                "incoming": [
                    {
                        "link_id": "link-uuid",
                        "source_memory_id": "other-memory-uuid",
                        "target_memory_id": "memory-uuid",
                        "link_type": "supports",
                        "strength": 0.7,
                        "description": "Supporting evidence",
                        "created_at_unix": 1678885400,
                        "source_memory": { # Only if include_memory_details=True
                            "memory_id": "other-memory-uuid",
                            "description": "Memory description",
                            "memory_type": "fact",
                            ... other memory fields ...
                        }
                    },
                    ... more incoming links ...
                ]
            },
            "success": true,
            "processing_time": 0.123
        }
        
    Raises:
        ToolInputError: If memory_id is not provided or direction is invalid
        ToolError: If database operation fails
    """
    start_time = time.time()
    
    if not memory_id:
        raise ToolInputError("Memory ID is required", param_name="memory_id")
    
    valid_directions = ["outgoing", "incoming", "both"]
    direction = direction.lower()
    if direction not in valid_directions:
        raise ToolInputError(f"Direction must be one of: {', '.join(valid_directions)}", param_name="direction")
    
    if link_type:
        try:
            LinkType(link_type.lower())  # Validate enum
        except ValueError as e:
            valid_types = [lt.value for lt in LinkType]
            raise ToolInputError(f"Invalid link_type. Must be one of: {', '.join(valid_types)}", param_name="link_type") from e
    
    # Initialize result structure
    result = {
        "memory_id": memory_id,
        "links": {
            "outgoing": [],
            "incoming": []
        },
        "success": True,
        "processing_time": 0.0
    }
    
    try:
        async with DBConnection(db_path) as conn:
            # Check if memory exists
            async with conn.execute("SELECT 1 FROM memories WHERE memory_id = ?", (memory_id,)) as cursor:
                if not await cursor.fetchone():
                    raise ToolInputError(f"Memory {memory_id} not found", param_name="memory_id")
            
            # Process outgoing links (memory_id is the source)
            if direction in ["outgoing", "both"]:
                outgoing_query = """
                    SELECT ml.*, m.memory_type AS target_type, m.description AS target_description
                    FROM memory_links ml
                    JOIN memories m ON ml.target_memory_id = m.memory_id
                    WHERE ml.source_memory_id = ?
                """
                params = [memory_id]
                
                if link_type:
                    outgoing_query += " AND ml.link_type = ?"
                    params.append(link_type.lower())
                
                outgoing_query += " ORDER BY ml.created_at DESC LIMIT ?"
                params.append(limit)
                
                async with conn.execute(outgoing_query, params) as cursor:
                    rows = await cursor.fetchall()
                    for row in rows:
                        link_data = dict(row)
                        
                        # Add target memory details if requested
                        if include_memory_details:
                            target_memory_id = link_data["target_memory_id"]
                            async with conn.execute(
                                """
                                SELECT memory_id, memory_level, memory_type, importance, confidence, 
                                       description, created_at, updated_at, tags
                                FROM memories WHERE memory_id = ?
                                """, 
                                (target_memory_id,)
                            ) as mem_cursor:
                                target_memory = await mem_cursor.fetchone()
                                if target_memory:
                                    mem_dict = dict(target_memory)
                                    # Process fields
                                    mem_dict["created_at_unix"] = mem_dict["created_at"]
                                    mem_dict["updated_at_unix"] = mem_dict["updated_at"]
                                    mem_dict["tags"] = await MemoryUtils.deserialize(mem_dict.get("tags"))
                                    link_data["target_memory"] = mem_dict
                        
                        # Format link data
                        link_data["created_at_unix"] = link_data["created_at"]
                        result["links"]["outgoing"].append(link_data)
            
            # Process incoming links (memory_id is the target)
            if direction in ["incoming", "both"]:
                incoming_query = """
                    SELECT ml.*, m.memory_type AS source_type, m.description AS source_description
                    FROM memory_links ml
                    JOIN memories m ON ml.source_memory_id = m.memory_id
                    WHERE ml.target_memory_id = ?
                """
                params = [memory_id]
                
                if link_type:
                    incoming_query += " AND ml.link_type = ?"
                    params.append(link_type.lower())
                
                incoming_query += " ORDER BY ml.created_at DESC LIMIT ?"
                params.append(limit)
                
                async with conn.execute(incoming_query, params) as cursor:
                    rows = await cursor.fetchall()
                    for row in rows:
                        link_data = dict(row)
                        
                        # Add source memory details if requested
                        if include_memory_details:
                            source_memory_id = link_data["source_memory_id"]
                            async with conn.execute(
                                """
                                SELECT memory_id, memory_level, memory_type, importance, confidence, 
                                       description, created_at, updated_at, tags
                                FROM memories WHERE memory_id = ?
                                """, 
                                (source_memory_id,)
                            ) as mem_cursor:
                                source_memory = await mem_cursor.fetchone()
                                if source_memory:
                                    mem_dict = dict(source_memory)
                                    # Process fields
                                    mem_dict["created_at_unix"] = mem_dict["created_at"]
                                    mem_dict["updated_at_unix"] = mem_dict["updated_at"]
                                    mem_dict["tags"] = await MemoryUtils.deserialize(mem_dict.get("tags"))
                                    link_data["source_memory"] = mem_dict
                        
                        # Format link data
                        link_data["created_at_unix"] = link_data["created_at"]
                        result["links"]["incoming"].append(link_data)
            
            # Record access stats for the source memory
            await MemoryUtils._update_memory_access(conn, memory_id)
            await conn.commit()
            
            processing_time = time.time() - start_time
            logger.info(
                f"Retrieved {len(result['links']['outgoing'])} outgoing and {len(result['links']['incoming'])} incoming links for memory {memory_id}",
                emoji_key="link"
            )
            
            result["processing_time"] = processing_time
            return result
    
    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error retrieving linked memories: {e}", exc_info=True)
        raise ToolError(f"Failed to retrieve linked memories: {str(e)}") from e


# ======================================================
# Meta-Cognition Tools (Adapted from cognitive_memory)
# ======================================================

# --- Helper: Generate Consolidation Prompt (FULL INSTRUCTIONS) ---
def _generate_consolidation_prompt(memories: List[Dict], consolidation_type: str) -> str:
    """Generates a prompt for memory consolidation based on the type, with full instructions."""
    # Format memories as text (Limit input memories and content length for prompt size)
    memory_texts = []
    # Limit source memories included in prompt to avoid excessive length
    for i, memory in enumerate(memories[:20], 1):
        desc = memory.get("description") or ""
        # Limit content preview significantly to avoid overly long prompts
        content_preview = (memory.get("content", "") or "")[:300]
        mem_type = memory.get("memory_type", "N/A")
        importance = memory.get("importance", 5.0)
        confidence = memory.get("confidence", 1.0)
        created_ts = memory.get("created_at", 0)
        created_dt_str = datetime.fromtimestamp(created_ts).strftime('%Y-%m-%d %H:%M') if created_ts else "Unknown Date"
        mem_id_short = memory.get("memory_id", "UNKNOWN")[:8]

        formatted = f"--- MEMORY #{i} (ID: {mem_id_short}..., Type: {mem_type}, Importance: {importance:.1f}, Confidence: {confidence:.1f}, Date: {created_dt_str}) ---\n"
        if desc:
            formatted += f"Description: {desc}\n"
        formatted += f"Content Preview: {content_preview}"
        # Indicate truncation
        if len(memory.get("content", "")) > 300:
            formatted += "...\n"
        else:
            formatted += "\n"
        memory_texts.append(formatted)

    memories_str = "\n".join(memory_texts)

    # Base prompt template
    base_prompt = f"""You are an advanced cognitive system processing and consolidating memories for an AI agent. Below are {len(memories)} memory items containing information, observations, and insights relevant to a task. Your goal is to perform a specific type of consolidation: '{consolidation_type}'.

Analyze the following memories carefully:

{memories_str}
--- END OF MEMORIES ---

"""

    # Add specific instructions based on consolidation type (FULL INSTRUCTIONS)
    if consolidation_type == "summary":
        base_prompt += """TASK: Create a comprehensive and coherent summary that synthesizes the key information and context from ALL the provided memories. Your summary should:
1.  Distill the most critical facts, findings, and core ideas presented across the memories.
2.  Organize the information logically, perhaps chronologically or thematically, creating a clear narrative flow.
3.  Highlight significant connections, relationships, or developments revealed by considering the memories together.
4.  Eliminate redundancy while preserving essential details and nuances.
5.  Be objective and accurately reflect the content of the source memories.
6.  Be well-structured and easy to understand for someone reviewing the workflow's progress.

Generate ONLY the summary content based on the provided memories.

CONSOLIDATED SUMMARY:"""

    elif consolidation_type == "insight":
        base_prompt += """TASK: Generate high-level insights by identifying significant patterns, implications, conclusions, or discrepancies emerging from the provided memories. Your insights should:
1.  Go beyond simple summarization to reveal non-obvious patterns, trends, or relationships connecting different memories.
2.  Draw meaningful conclusions or formulate hypotheses that are supported by the collective information but may not be explicit in any single memory.
3.  Explicitly highlight any contradictions, tensions, or unresolved issues found between memories.
4.  Identify the broader significance, potential impact, or actionable implications of the combined information.
5.  Be stated clearly and concisely, using cautious language where certainty is limited (e.g., "It appears that...", "This might suggest...").
6.  Focus on the most impactful and novel understandings gained from analyzing these memories together.

Generate ONLY the list of insights based on the provided memories.

CONSOLIDATED INSIGHTS:"""

    elif consolidation_type == "procedural":
        base_prompt += """TASK: Formulate a generalized procedure, method, or set of steps based on the actions, outcomes, and observations described in the memories. Your procedure should:
1.  Identify recurring sequences of actions or steps that appear to lead to successful or notable outcomes.
2.  Generalize from specific instances described in the memories to create a potentially reusable approach or workflow pattern.
3.  Clearly outline the sequence of steps involved in the procedure.
4.  Note important conditions, prerequisites, inputs, outputs, or constraints associated with the procedure or its steps.
5.  Highlight decision points, potential variations, or common failure points if identifiable from the memories.
6.  Be structured as a clear set of instructions or a logical flow that could guide similar future situations.

Generate ONLY the procedure based on the provided memories.

CONSOLIDATED PROCEDURE:"""

    elif consolidation_type == "question":
        base_prompt += """TASK: Identify the most important and actionable questions that arise from analyzing these memories. Your questions should:
1.  Target significant gaps in knowledge, understanding, or information revealed by the memories.
2.  Highlight areas of uncertainty, ambiguity, or contradiction that require further investigation or clarification.
3.  Focus on issues that are critical for achieving the implied or stated goals related to these memories.
4.  Be specific and well-defined enough to guide further research, analysis, or action.
5.  Be prioritized, starting with the most critical or foundational questions.
6.  Avoid questions that are already answered within the provided memory content.

Generate ONLY the list of questions based on the provided memories.

CONSOLIDATED QUESTIONS:"""

    # The final marker like "CONSOLIDATED SUMMARY:" is added by the TASK instruction itself.
    return base_prompt


# Helper for reflection prompt (similar structure to consolidation)
def _generate_reflection_prompt(
    workflow_name: str,
    workflow_desc: Optional[str],
    operations: List[Dict],
    memories: Dict[str, Dict],
    reflection_type: str
) -> str:
    """Generates a prompt for reflective analysis based on the type, with detailed instructions."""

    # Format operations (limited for prompt size)
    op_texts = []
    for i, op_data in enumerate(operations[:30], 1): # Limit input operations
        op_ts_unix = op_data.get("timestamp", 0)
        op_ts_str = datetime.fromtimestamp(op_ts_unix).strftime('%Y-%m-%d %H:%M:%S') if op_ts_unix else "Unknown Time"
        op_type = op_data.get('operation', 'UNKNOWN').upper()
        mem_id = op_data.get('memory_id')
        action_id = op_data.get('action_id') # Get action_id if present

        # Extract relevant details from operation_data if present
        op_details_dict = {}
        op_data_raw = op_data.get('operation_data')
        if op_data_raw:
             try:
                  op_details_dict = json.loads(op_data_raw)
             except (json.JSONDecodeError, TypeError):
                  op_details_dict = {"raw_data": str(op_data_raw)[:50]} # Fallback

        # Build description string parts
        desc_parts = [f"OP #{i} ({op_ts_str})", f"Type: {op_type}"]
        if mem_id:
            mem_info = memories.get(mem_id)
            # Provide more context from memory if available
            mem_desc_text = f"Mem({mem_id[:6]}..)"
            if mem_info:
                 mem_desc_text += f" Desc: {mem_info.get('description', 'N/A')[:40]}"
                 if mem_info.get('memory_type'):
                      mem_desc_text += f" Type: {mem_info['memory_type']}"
            desc_parts.append(mem_desc_text)

        if action_id:
            desc_parts.append(f"Action({action_id[:6]}..)")

        # Add details from operation_data, excluding verbose fields
        detail_items = []
        for k, v in op_details_dict.items():
             if k not in ['content', 'description', 'embedding', 'prompt']: # Exclude common large fields
                  detail_items.append(f"{k}={str(v)[:30]}")
        if detail_items:
            desc_parts.append(f"Data({', '.join(detail_items)})")

        op_texts.append(" | ".join(desc_parts))

    operations_str = "\n".join(op_texts)

    # Base prompt template
    base_prompt = f"""You are an advanced meta-cognitive system analyzing an AI agent's workflow: "{workflow_name}".
Workflow Description: {workflow_desc or 'N/A'}
Your task is to perform a '{reflection_type}' reflection based on the recent memory operations listed below (newest first). Analyze these operations to understand the agent's process, progress, and knowledge state.

RECENT OPERATIONS (Up to 30):
{operations_str}

"""

    # --- Add specific instructions based on reflection type (FULL INSTRUCTIONS) ---
    if reflection_type == "summary":
        base_prompt += """TASK: Create a reflective summary of this workflow's progress and current state. Your summary should:
1. Trace the key developments, insights, and significant actions derived from the provided memory operations.
2. Identify the primary focus areas suggested by recent operations and the nature of memories being created or accessed.
3. Summarize the overall arc of the agent's thinking, knowledge acquisition, and task execution during this period.
4. Organize the summary logically (e.g., chronologically by operation, thematically by task).
5. Include a concise assessment of the current state of understanding or progress relative to the workflow's implied or stated goal.

REFLECTIVE SUMMARY:"""

    elif reflection_type == "progress":
        base_prompt += """TASK: Analyze the progress the agent has made toward its goals and understanding within this workflow, based *only* on the provided operations. Your analysis should:
1. Infer the likely immediate goals or sub-tasks the agent was pursuing during these operations.
2. Assess what tangible progress was made toward these inferred goals (e.g., information gathered, artifacts created, decisions made, errors overcome).
3. Highlight key milestones evident in the operations (e.g., successful tool use, creation of important memories, focus shifts).
4. Note operations that suggest stalled progress, repeated actions, failures, or areas needing more work.
5. If possible, suggest observable indicators or metrics from future operations that would signify further progress.

PROGRESS ANALYSIS:"""

    elif reflection_type == "gaps":
        base_prompt += """TASK: Identify gaps in knowledge, reasoning, or process suggested by these operations that should be addressed for this workflow. Your analysis should:
1. Pinpoint potential missing information or unanswered questions implied by the operations (e.g., failed actions, repeated queries, lack of evidence for inferences).
2. Identify possible logical inconsistencies, contradictions between memory operations, or areas where reasoning seems weak based on the sequence of operations.
3. Note operations indicating low confidence (if available in op_data) or areas where actions were taken without sufficient preceding evidence or planning evident in the log.
4. Formulate specific, actionable questions arising directly from analyzing these operations.
5. Recommend concrete next steps (e.g., specific tool use, memory queries, reasoning steps) suggested by the operations log to address these gaps.

KNOWLEDGE GAPS ANALYSIS:"""

    elif reflection_type == "strengths":
        base_prompt += """TASK: Identify what went well during the sequence of operations and what valuable knowledge or effective strategies were likely employed. Your analysis should focus on patterns *within these operations*:
1. Highlight sequences of operations suggesting successful reasoning patterns, problem-solving approaches, or effective decision-making (e.g., planning followed by successful execution).
2. Note operations that created potentially valuable insights, facts, or summaries (look for 'create' operations with relevant memory types).
3. Identify potentially reusable patterns or methods suggested by successful tool use sequences or memory linking operations.
4. Recognize potentially effective use of information sources or successful navigation of constraints if implied by the operation data.
5. Suggest ways the patterns observed in these successful operations could be leveraged or reinforced moving forward.

STRENGTHS ANALYSIS:"""

    elif reflection_type == "plan":
        base_prompt += """TASK: Based *only* on the provided recent operations and the workflow's current implied state, create a strategic plan for the immediate next steps. Your plan should:
1. Identify the most logical next actions based on the last few operations (e.g., follow up on a question, use results from a tool, consolidate findings).
2. Prioritize information gathering, analysis, or tool use based on the workflow's apparent trajectory revealed in the operations log.
3. Suggest specific, concrete actions, potentially including tool names or memory operations, for the very next phase.
4. Include brief considerations for potential challenges or alternative paths if suggested by recent failures or uncertainty in the log.
5. Define what a successful outcome for the immediate next 1-3 steps would look like, based on the context provided by the operations.

STRATEGIC PLAN:"""

    # Note: The final marker like "REFLECTIVE SUMMARY:" is added by the calling function `generate_reflection`.
    return base_prompt


# --- Tool: Consolidate Memories ---
@with_tool_metrics
@with_error_handling
async def consolidate_memories(
    workflow_id: Optional[str] = None,
    target_memories: Optional[List[str]] = None,
    consolidation_type: str = "summary",
    query_filter: Optional[Dict[str, Any]] = None,
    max_source_memories: int = 20,
    prompt_override: Optional[str] = None,
    provider: str = LLMGatewayProvider.OPENAI.value,
    model: Optional[str] = None,
    store_result: bool = True,
    store_as_level: str = MemoryLevel.SEMANTIC.value,
    store_as_type: Optional[str] = None,
    max_tokens: int = 1000,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Consolidates multiple memories using an LLM to generate summaries, insights, etc."""
    start_time = time.time()
    valid_types = ["summary", "insight", "procedural", "question"]
    if consolidation_type not in valid_types:
        raise ToolInputError(f"consolidation_type must be one of: {valid_types}", param_name="consolidation_type")

    source_memories_list = []
    source_memory_ids = []
    effective_workflow_id = workflow_id

    try:
        async with DBConnection(db_path) as conn:
            # --- 1. Select Source Memories (Full Logic) ---
            if target_memories:
                if len(target_memories) < 2:
                    raise ToolInputError("At least 2 target_memories required.", param_name="target_memories")
                placeholders = ', '.join(['?'] * len(target_memories))
                query = f"SELECT * FROM memories WHERE memory_id IN ({placeholders})"
                params: List[Any] = target_memories
                if effective_workflow_id:
                     query += " AND workflow_id = ?"
                     params.append(effective_workflow_id)

                async with conn.execute(query, params) as cursor:
                     rows = await cursor.fetchall()
                     found_ids = {r['memory_id'] for r in rows}
                     requested_ids = set(target_memories)
                     if found_ids != requested_ids:
                         missing = requested_ids - found_ids
                         err_msg = f"Target memories issue. Missing: {missing or 'None'}. Workflow mismatch or invalid IDs?"
                         raise ToolInputError(err_msg, param_name="target_memories")
                     source_memories_list = [dict(row) for row in rows]
                     source_memory_ids = target_memories
                     if not effective_workflow_id and source_memories_list:
                          effective_workflow_id = source_memories_list[0].get('workflow_id')

            elif query_filter:
                # Build filter query dynamically
                filter_where = ["1=1"]
                filter_params = []
                if effective_workflow_id:
                    filter_where.append("workflow_id = ?")
                    filter_params.append(effective_workflow_id)
                for key, value in query_filter.items():
                    if key in ["memory_level", "memory_type", "source"] and value:
                         filter_where.append(f"{key} = ?")
                         filter_params.append(value)
                    elif key == "min_importance" and value is not None:
                         filter_where.append("importance >= ?")
                         filter_params.append(float(value))
                    elif key == "min_confidence" and value is not None:
                         filter_where.append("confidence >= ?")
                         filter_params.append(float(value))
                    # Add more complex filters here if needed

                query = f"SELECT * FROM memories WHERE {' AND '.join(filter_where)} ORDER BY importance DESC, created_at DESC LIMIT ?"
                filter_params.append(max_source_memories)

                async with conn.execute(query, filter_params) as cursor:
                     source_memories_list = [dict(row) for row in await cursor.fetchall()]
                     source_memory_ids = [m["memory_id"] for m in source_memories_list]
                     if not effective_workflow_id and source_memories_list:
                          effective_workflow_id = source_memories_list[0].get('workflow_id')
            else:
                # Default: Get recent, important memories from the specified workflow
                if not effective_workflow_id:
                    raise ToolInputError("workflow_id is required if not using target_memories or query_filter.", param_name="workflow_id")
                query = "SELECT * FROM memories WHERE workflow_id = ? ORDER BY importance DESC, created_at DESC LIMIT ?"
                async with conn.execute(query, [effective_workflow_id, max_source_memories]) as cursor:
                     source_memories_list = [dict(row) for row in await cursor.fetchall()]
                     source_memory_ids = [m["memory_id"] for m in source_memories_list]

            if len(source_memories_list) < 2:
                 raise ToolError(f"Insufficient source memories found ({len(source_memories_list)}) for consolidation based on criteria.")

            if not effective_workflow_id:
                raise ToolError("Could not determine a workflow ID for consolidation.")

            # --- 2. Generate Consolidation Prompt (using the full helper) ---
            prompt = prompt_override or _generate_consolidation_prompt(source_memories_list, consolidation_type)

            # --- 3. Call LLM via Gateway ---
            consolidated_content = ""
            try:
                 provider_instance = await get_provider(provider)
                 # Use a reasonable default model if none provided for the provider
                 default_models = {"openai": "gpt-4.1-mini", "anthropic": "claude-3-5-haiku-20241022"}
                 final_model = model or default_models.get(provider)
                 if not final_model:
                      logger.warning(f"No specific or default model known for provider '{provider}'. Attempting provider default.")
                 
                 llm_result = await provider_instance.generate_completion(
                     prompt=prompt, model=final_model, max_tokens=max_tokens, temperature=0.6
                 )
                 consolidated_content = llm_result.text.strip()
                 if not consolidated_content:
                     logger.warning("LLM returned empty content for consolidation. Cannot store result.")
                     consolidated_content = ""
                 else:
                     logger.debug(f"LLM consolidation successful. Content length: {len(consolidated_content)}")
            except Exception as llm_err:
                 logger.error(f"LLM call failed during consolidation: {llm_err}", exc_info=True)
                 raise ToolError(f"Consolidation failed due to LLM error: {llm_err}") from llm_err

            # --- 4. Store Result ---
            stored_memory_id = None
            if store_result and consolidated_content:
                result_type_val = store_as_type or {
                    "summary": MemoryType.SUMMARY.value, "insight": MemoryType.INSIGHT.value,
                    "procedural": MemoryType.PROCEDURE.value, "question": MemoryType.QUESTION.value
                }.get(consolidation_type, MemoryType.INSIGHT.value)
                try: 
                    result_type = MemoryType(result_type_val.lower())
                except ValueError: 
                    result_type = MemoryType(MemoryType.INSIGHT.value)

                try: 
                    result_level = MemoryLevel(store_as_level.lower())
                except ValueError: 
                    result_level = MemoryLevel.SEMANTIC

                result_desc = f"Consolidated {consolidation_type} from {len(source_memory_ids)} memories."
                result_tags = ["consolidated", consolidation_type]
                result_context = {"source_memories": source_memory_ids}

                # --- Calculate derived importance and confidence ---
                derived_importance = 5.0 # Default
                derived_confidence = 0.75 # Default
                if source_memories_list:
                    source_importances = [m.get("importance", 5.0) for m in source_memories_list]
                    source_confidences = [m.get("confidence", 0.5) for m in source_memories_list]

                    # Importance: Max source importance + small boost, capped at 10
                    derived_importance = min(max(source_importances) + 0.5, 10.0)

                    # Confidence: Average source confidence, capped at 1
                    derived_confidence = min(sum(source_confidences) / len(source_confidences), 1.0)
                    # Add a slight penalty based on number of sources (more sources -> slightly less confidence in exact consolidation?)
                    derived_confidence = max(0.1, derived_confidence * (1.0 - min(0.2, (len(source_memories_list) - 1) * 0.02)))

                logger.debug(f"Derived Importance: {derived_importance:.2f}, Confidence: {derived_confidence:.2f}")
                # --- End calculation ---

                try:
                    # Use store_memory tool function with derived values
                    store_result_dict = await store_memory(
                        workflow_id=effective_workflow_id,
                        content=consolidated_content,
                        memory_type=result_type.value,
                        memory_level=result_level.value,
                        importance=round(derived_importance, 2), # Use derived importance
                        confidence=round(derived_confidence, 3), # Use derived confidence
                        description=result_desc,
                        source=f"consolidation_{consolidation_type}",
                        tags=result_tags, context_data=result_context,
                        generate_embedding=True, db_path=db_path
                    )
                    stored_memory_id = store_result_dict.get("memory_id")
                except Exception as store_err:
                    logger.error(f"Failed to store consolidated memory result: {store_err}", exc_info=True)

                # --- 5. Link Result to Sources ---
                if stored_memory_id:
                    link_tasks = []
                    for source_id in source_memory_ids:
                         link_task = create_memory_link(
                             source_memory_id=stored_memory_id,
                             target_memory_id=source_id,
                             link_type=LinkType.GENERALIZES.value,
                             description=f"Source for consolidated {consolidation_type}",
                             db_path=db_path
                         )
                         link_tasks.append(link_task)
                    link_results = await asyncio.gather(*link_tasks, return_exceptions=True)
                    failed_links = [res for res in link_results if isinstance(res, Exception)]
                    if failed_links:
                         logger.warning(f"Failed to create {len(failed_links)} links from consolidated memory {stored_memory_id} to sources.")

            # --- 6. Log Consolidation Operation ---
            log_data = {
                 "consolidation_type": consolidation_type, "source_count": len(source_memory_ids),
                 "llm_provider": provider, "llm_model": model or "default",
                 "stored": bool(stored_memory_id), "stored_memory_id": stored_memory_id
            }
            await MemoryUtils._log_memory_operation(conn, effective_workflow_id, "consolidate", None, None, log_data)

            await conn.commit() # Commit logging operation

            processing_time = time.time() - start_time
            logger.info(f"Consolidated {len(source_memory_ids)} memories ({consolidation_type}). Stored as: {stored_memory_id}", emoji_key="sparkles", time=processing_time)
            return {
                "consolidated_content": consolidated_content or "Consolidation failed or produced no content.",
                "consolidation_type": consolidation_type,
                "source_memory_ids": source_memory_ids,
                "workflow_id": effective_workflow_id,
                "stored_memory_id": stored_memory_id,
                "success": True, # Success of the operation itself
                "processing_time": processing_time
            }

    except (ToolInputError, ToolError):
        raise
    except Exception as e:
        logger.error(f"Failed to consolidate memories: {str(e)}", exc_info=True)
        raise ToolError(f"Failed to consolidate memories: {str(e)}") from e

    
@with_tool_metrics
@with_error_handling
async def generate_reflection(
    workflow_id: str,
    reflection_type: str = "summary", # summary, progress, gaps, strengths, plan
    recent_ops_limit: int = 30,
    provider: str = LLMGatewayProvider.OPENAI.value,
    model: Optional[str] = None,
    max_tokens: int = 1000,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Generates a reflective analysis of a workflow using an LLM.

    Args:
        workflow_id: ID of the workflow to reflect on.
        reflection_type: Type of reflection ('summary', 'progress', 'gaps', 'strengths', 'plan').
        recent_ops_limit: (Optional) Number of recent operations to analyze. Default 30.
        provider: (Optional) LLM provider name.
        model: (Optional) Specific LLM model name.
        max_tokens: (Optional) Max tokens for LLM response. Default 1000.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing the reflection details.
    """
    start_time = time.time()
    valid_types = ["summary", "progress", "gaps", "strengths", "plan"]
    if reflection_type not in valid_types: 
        raise ToolInputError("Invalid reflection_type.", param_name="reflection_type")

    try:
        async with DBConnection(db_path) as conn:
            # --- 1. Fetch Workflow Info & Recent Operations ---
            async with conn.execute("SELECT title, description FROM workflows WHERE workflow_id = ?", (workflow_id,)) as cursor:
                 wf_row = await cursor.fetchone()
                 if not wf_row: 
                     raise ToolInputError(f"Workflow {workflow_id} not found.", param_name="workflow_id")
                 workflow_name = wf_row["title"]
                 workflow_desc = wf_row["description"]

            operations = []
            async with conn.execute("SELECT * FROM memory_operations WHERE workflow_id = ? ORDER BY timestamp DESC LIMIT ?", (workflow_id, recent_ops_limit)) as cursor:
                 operations = [dict(row) for row in await cursor.fetchall()]

            if not operations: 
                raise ToolError("No operations found for reflection.")

            # --- 2. Fetch Details of Referenced Memories ---
            mem_ids = {op['memory_id'] for op in operations if op.get('memory_id')}
            memories_details = {}
            if mem_ids:
                placeholders = ','.join('?'*len(mem_ids))
                async with conn.execute(f"SELECT memory_id, description FROM memories WHERE memory_id IN ({placeholders})", list(mem_ids)) as cursor:
                    async for row in cursor: 
                        memories_details[row["memory_id"]] = dict(row)

            # --- 3. Generate Reflection Prompt ---
            prompt = _generate_reflection_prompt(workflow_name, workflow_desc, operations, memories_details, reflection_type)

            # --- 4. Call LLM via Gateway ---
            try:
                 provider_instance = await get_provider(provider)
                 llm_result = await provider_instance.generate_completion(
                     prompt=prompt, model=model, max_tokens=max_tokens, temperature=0.7
                 )
                 reflection_content = llm_result.text.strip()
                 if not reflection_content: 
                     raise ToolError("LLM returned empty reflection.")
            except Exception as llm_err:
                 logger.error(f"LLM call failed during reflection: {llm_err}", exc_info=True)
                 raise ToolError(f"Reflection failed due to LLM error: {llm_err}") from llm_err

            # --- 5. Store Reflection ---
            reflection_id = MemoryUtils.generate_id()
            now_unix = int(time.time())
            # Extract title from content (simple approach)
            title = reflection_content.split('\n', 1)[0].strip('# ')[:100] or f"{reflection_type.capitalize()} Reflection"
            referenced_memory_ids = list(mem_ids) # Store IDs of memories involved in operations

            await conn.execute(
                 """INSERT INTO reflections
                    (reflection_id, workflow_id, title, content, reflection_type, created_at, referenced_memories)
                    VALUES (?, ?, ?, ?, ?, ?, ?)""",
                 (reflection_id, workflow_id, title, reflection_content, reflection_type, now_unix, json.dumps(referenced_memory_ids))
            )

            # Log operation
            await MemoryUtils._log_memory_operation(conn, workflow_id, "reflect", None, None, {
                "reflection_id": reflection_id, "reflection_type": reflection_type, "ops_analyzed": len(operations)
            })

            await conn.commit()

            processing_time = time.time() - start_time
            logger.info(f"Generated reflection '{title}' ({reflection_id}) for workflow {workflow_id}", emoji_key="mirror")
            return {
                "reflection_id": reflection_id,
                "reflection_type": reflection_type,
                "title": title,
                "content": reflection_content, # Return full content
                "workflow_id": workflow_id,
                "operations_analyzed": len(operations),
                "success": True,
                "processing_time": processing_time
            }

    except (ToolInputError, ToolError):
        raise
    except Exception as e:
        logger.error(f"Failed to generate reflection: {str(e)}", exc_info=True)
        raise ToolError(f"Failed to generate reflection: {str(e)}") from e


# ======================================================
# Text Summarization (using LLM)
# ======================================================

@with_tool_metrics
@with_error_handling
async def summarize_text(
    text_to_summarize: str,
    target_tokens: int = 500,
    prompt_template: Optional[str] = None,
    provider: str = "openai",
    model: Optional[str] = None,
    workflow_id: Optional[str] = None,
    record_summary: bool = False,
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Summarizes text content using an LLM to generate a concise summary.
    
    Uses the configured LLM provider to generate a summary of the provided text,
    optimizing for the requested token length. Optionally stores the summary
    as a memory in the specified workflow.
    
    Args:
        text_to_summarize: Text content to summarize
        target_tokens: Approximate desired length of summary (default 500)
        prompt_template: Optional custom prompt template for summarization
        provider: LLM provider to use for summarization (default "openai")
        model: Specific model to use, or None for provider default
        workflow_id: Optional workflow ID to store the summary in
        record_summary: Whether to store the summary as a memory
        db_path: Path to the SQLite database file
        
    Returns:
        Dictionary containing the generated summary:
        {
            "summary": "Concise summary text...",
            "original_length": 2500,  # Approximate character count
            "summary_length": 350,    # Approximate character count
            "stored_memory_id": "memory-uuid" | None,  # Only if record_summary=True
            "success": true,
            "processing_time": 0.123
        }
        
    Raises:
        ToolInputError: If text_to_summarize is empty or provider/model is invalid
        ToolError: If summarization fails or database operation fails
    """
    start_time = time.time()
    
    if not text_to_summarize:
        raise ToolInputError("Text to summarize cannot be empty", param_name="text_to_summarize")
    
    if record_summary and not workflow_id:
        raise ToolInputError("Workflow ID is required when record_summary=True", param_name="workflow_id")
    
    # Ensure target_tokens is reasonable
    target_tokens = max(50, min(2000, target_tokens))
    
    # Use default models for common providers if none specified
    default_models = {
        "openai": "gpt-4.1-mini",
        "anthropic": "claude-3-5-haiku-20241022"
    }
    model_to_use = model or default_models.get(provider)
    
    # Default prompt template if none provided
    if not prompt_template:
        prompt_template = """
You are an expert summarizer. Your task is to create a concise and accurate summary of the following text.
The summary should be approximately {target_tokens} tokens long.
Focus on the main points, key information, and essential details.
Maintain the tone and factual accuracy of the original text.

TEXT TO SUMMARIZE:
{text_to_summarize}

CONCISE SUMMARY:
"""
    
    try:
        # Get provider instance from ultimate
        provider_instance = await get_provider(provider)
        if not provider_instance:
            raise ToolError(f"Failed to initialize provider '{provider}'. Check configuration.")
        
        # Prepare prompt by filling in the template
        prompt = prompt_template.format(
            text_to_summarize=text_to_summarize,
            target_tokens=target_tokens
        )
        
        # Generate summary using LLM provider
        generation_result = await provider_instance.generate_completion(
            prompt=prompt,
            model=model_to_use,
            max_tokens=target_tokens + 100,  # Add some buffer for prompt tokens
            temperature=0.3  # Lower temperature for more deterministic summaries
        )
        
        summary_text = generation_result.text.strip()
        if not summary_text:
            raise ToolError("LLM returned empty summary.")
        
        # Optional: Store summary as a memory
        stored_memory_id = None
        if record_summary and workflow_id:
            # Validate workflow exists
            async with DBConnection(db_path) as conn:
                async with conn.execute("SELECT 1 FROM workflows WHERE workflow_id = ?", (workflow_id,)) as cursor:
                    if not await cursor.fetchone():
                        raise ToolInputError(f"Workflow {workflow_id} not found", param_name="workflow_id")
                
                # Create new memory entry for the summary
                memory_id = MemoryUtils.generate_id()
                now_unix = int(time.time())
                
                description = f"Summary of text ({len(text_to_summarize)} chars)"
                tags = json.dumps(["summary", "automated", "text_summary"])
                
                await conn.execute(
                    """
                    INSERT INTO memories (memory_id, workflow_id, content, memory_level, memory_type, 
                                        importance, confidence, description, source, tags, 
                                        created_at, updated_at, access_count)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (memory_id, workflow_id, summary_text, MemoryLevel.SEMANTIC.value, MemoryType.SUMMARY.value,
                     6.0, 0.85, description, "summarize_text", tags,
                     now_unix, now_unix, 0)
                )
                
                # Log the operation
                await MemoryUtils._log_memory_operation(
                    conn, workflow_id, "create_summary", memory_id, None, 
                    {"original_length": len(text_to_summarize), "summary_length": len(summary_text)}
                )
                
                await conn.commit()
                stored_memory_id = memory_id
        
        processing_time = time.time() - start_time
        logger.info(
            f"Generated summary of {len(text_to_summarize)} chars text to {len(summary_text)} chars",
            emoji_key="scissors", time=processing_time
        )
        
        return {
            "summary": summary_text,
            "original_length": len(text_to_summarize),
            "summary_length": len(summary_text),
            "stored_memory_id": stored_memory_id,
            "success": True,
            "processing_time": processing_time
        }
    
    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error summarizing text: {e}", exc_info=True)
        raise ToolError(f"Failed to summarize text: {str(e)}") from e


# --- 17. Maintenance (Adapted from cognitive_memory) ---
@with_tool_metrics
@with_error_handling
async def delete_expired_memories(db_path: str = DEFAULT_DB_PATH) -> Dict[str, Any]:
    """Deletes memories that have exceeded their time-to-live (TTL)."""
    start_time = time.time()
    deleted_count = 0
    workflows_affected = set()

    try:
        async with DBConnection(db_path) as conn:
            now_unix = int(time.time())
            # Find expired memory IDs and their workflows
            expired_memories = []
            async with conn.execute(
                "SELECT memory_id, workflow_id FROM memories WHERE ttl > 0 AND created_at + ttl < ?",
                (now_unix,)
            ) as cursor:
                 expired_memories = await cursor.fetchall()

            if not expired_memories:
                 logger.info("No expired memories found to delete.")
                 return { "deleted_count": 0, "workflows_affected": [], "success": True, "processing_time": time.time() - start_time }

            expired_ids = [row["memory_id"] for row in expired_memories]
            workflows_affected = {row["workflow_id"] for row in expired_memories}
            deleted_count = len(expired_ids)

            # Delete in batches to avoid issues with too many placeholders
            batch_size = 500
            for i in range(0, deleted_count, batch_size):
                 batch_ids = expired_ids[i:i+batch_size]
                 placeholders = ', '.join(['?'] * len(batch_ids))
                 # Delete from memories table (FK constraints handle related embeddings/links)
                 await conn.execute(f"DELETE FROM memories WHERE memory_id IN ({placeholders})", batch_ids)
                 logger.debug(f"Deleted batch of {len(batch_ids)} expired memories.")

            # Log the operation for each affected workflow
            for wf_id in workflows_affected:
                await MemoryUtils._log_memory_operation(conn, wf_id, "expire_batch", None, None, {"expired_count_in_workflow": sum(1 for mid, wid in expired_memories if wid == wf_id)})

            await conn.commit()

            processing_time = time.time() - start_time
            logger.success(f"Deleted {deleted_count} expired memories across {len(workflows_affected)} workflows.", emoji_key="wastebasket", time=processing_time)
            return {
                "deleted_count": deleted_count,
                "workflows_affected": list(workflows_affected),
                "success": True,
                "processing_time": processing_time
            }

    except Exception as e:
        logger.error(f"Failed to delete expired memories: {str(e)}", exc_info=True)
        raise ToolError(f"Failed to delete expired memories: {str(e)}") from e


# --- 18. Statistics (Adapted from cognitive_memory) ---
@with_tool_metrics
@with_error_handling
async def compute_memory_statistics(
    workflow_id: Optional[str] = None, # Optional: If None, compute global stats
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Computes statistics about memories, optionally filtered by workflow."""
    start_time = time.time()
    stats: Dict[str, Any] = {"scope": workflow_id or "global"}

    try:
        async with DBConnection(db_path) as conn:
            # Base WHERE clause and params
            where_clause = "WHERE workflow_id = ?" if workflow_id else ""
            params = [workflow_id] if workflow_id else []

            # Total Memories
            async with conn.execute(f"SELECT COUNT(*) FROM memories {where_clause}", params) as cursor:
                 stats["total_memories"] = (await cursor.fetchone())[0]

            if stats["total_memories"] == 0:
                 stats.update({"success": True, "processing_time": time.time() - start_time})
                 logger.info(f"No memories found for statistics in scope: {stats['scope']}")
                 return stats

            # By Level
            async with conn.execute(f"SELECT memory_level, COUNT(*) FROM memories {where_clause} GROUP BY memory_level", params) as cursor:
                 stats["by_level"] = {row["memory_level"]: row[1] for row in await cursor.fetchall()}
            # By Type
            async with conn.execute(f"SELECT memory_type, COUNT(*) FROM memories {where_clause} GROUP BY memory_type", params) as cursor:
                 stats["by_type"] = {row["memory_type"]: row[1] for row in await cursor.fetchall()}

            # Confidence & Importance Aggregates
            async with conn.execute(f"SELECT AVG(confidence), AVG(importance) FROM memories {where_clause}", params) as cursor:
                 row = await cursor.fetchone()
                 stats["confidence_avg"] = round(row[0], 3) if row[0] is not None else None
                 stats["importance_avg"] = round(row[1], 2) if row[1] is not None else None

            # Temporal Stats
            async with conn.execute(f"SELECT MAX(created_at), MIN(created_at) FROM memories {where_clause}", params) as cursor:
                 row = await cursor.fetchone()
                 stats["newest_memory_unix"] = row[0]
                 stats["oldest_memory_unix"] = row[1]

            # Link Stats
            link_where = "WHERE m.workflow_id = ?" if workflow_id else ""
            link_params = params # Reuse params
            async with conn.execute(f"SELECT COUNT(*) FROM memory_links ml JOIN memories m ON ml.source_memory_id = m.memory_id {link_where}", link_params) as cursor:
                 stats["total_links"] = (await cursor.fetchone())[0]
            async with conn.execute(f"SELECT ml.link_type, COUNT(*) FROM memory_links ml JOIN memories m ON ml.source_memory_id = m.memory_id {link_where} GROUP BY ml.link_type", link_params) as cursor:
                 stats["links_by_type"] = {row["link_type"]: row[1] for row in await cursor.fetchall()}

            # Tag Stats (Top 5)
            tag_where = "WHERE wt.workflow_id = ?" if workflow_id else "" # Filter by workflow if needed
            tag_params = params # Reuse params
            async with conn.execute(f"""SELECT t.name, COUNT(wt.workflow_id) as count
                                         FROM tags t JOIN workflow_tags wt ON t.tag_id = wt.tag_id {tag_where}
                                         GROUP BY t.tag_id ORDER BY count DESC LIMIT 5""", tag_params) as cursor:
                  stats["top_workflow_tags"] = {row["name"]: row["count"] for row in await cursor.fetchall()}

            # Workflow Stats (if global)
            if not workflow_id:
                async with conn.execute("SELECT status, COUNT(*) FROM workflows GROUP BY status") as cursor:
                     stats["workflows_by_status"] = {row["status"]: row[1] for row in await cursor.fetchall()}

            stats["success"] = True
            stats["processing_time"] = time.time() - start_time
            logger.info(f"Computed memory statistics for scope: {stats['scope']}", emoji_key="bar_chart")
            return stats

    except Exception as e:
        logger.error(f"Failed to compute statistics: {str(e)}", exc_info=True)
        raise ToolError(f"Failed to compute statistics: {str(e)}") from e

def _mermaid_escape(text: str) -> str:
    """Escapes characters problematic for Mermaid node labels."""
    if not isinstance(text, str):
        text = str(text)
    # Replace quotes first, then other potentially problematic characters
    text = text.replace('"', '#quot;')
    text = text.replace('(', '#40;')
    text = text.replace(')', '#41;')
    text = text.replace('[', '#91;')
    text = text.replace(']', '#93;')
    text = text.replace('{', '#123;')
    text = text.replace('}', '#125;')
    text = text.replace(':', '#58;')
    text = text.replace(';', '#59;')
    text = text.replace('<', '#lt;')
    text = text.replace('>', '#gt;')
    # Replace newline with <br> for multiline labels if needed, or just space
    text = text.replace('\n', '<br>')
    return text

async def _generate_mermaid_diagram(workflow: Dict[str, Any]) -> str:
    """Generates a detailed Mermaid flowchart representation of the workflow."""

    def sanitize_mermaid_id(uuid_str: Optional[str], prefix: str) -> str:
        """Creates a valid Mermaid node ID from a UUID, handling None."""
        if not uuid_str:
             # Generate a unique fallback for missing IDs to avoid collisions
             return f"{prefix}_MISSING_{MemoryUtils.generate_id().replace('-', '_')}"
        # Replace hyphens which are problematic in unquoted Mermaid node IDs
        sanitized = uuid_str.replace("-", "_")
        return f"{prefix}_{sanitized}"

    diagram = ["```mermaid", "flowchart TD"] # Top-Down flowchart

    # --- Workflow Node ---
    wf_node_id = sanitize_mermaid_id(workflow.get('workflow_id'), "W") # Use sanitized full ID
    wf_title = _mermaid_escape(workflow.get('title', 'Workflow'))
    wf_status_class = f":::{workflow.get('status', 'active')}" # Style based on status
    diagram.append(f'    {wf_node_id}("{wf_title}"){wf_status_class}')
    diagram.append("") # Spacer

    # --- Action Nodes & Links ---
    action_nodes = {} # Map action_id to mermaid_node_id
    parent_links = {} # Map child_action_id to parent_action_id
    sequential_links = {} # Map sequence_number to action_id for sequential linking if no parent

    actions = sorted(workflow.get("actions", []), key=lambda a: a.get("sequence_number", 0))

    for i, action in enumerate(actions):
        action_id = action.get("action_id")
        if not action_id: 
            continue # Skip actions somehow missing an ID

        node_id = sanitize_mermaid_id(action_id, "A") # Use sanitized full ID
        action_nodes[action_id] = node_id
        sequence_number = action.get("sequence_number", i) # Use sequence number if available

        # Label: Include type, title, and potentially tool name
        action_type = action.get('action_type', 'Action').capitalize()
        action_title = _mermaid_escape(action.get('title', action_type))
        label = f"<b>{action_type} #{sequence_number}</b><br/>{action_title}"
        if action.get('tool_name'):
            label += f"<br/><i>Tool: {_mermaid_escape(action['tool_name'])}</i>"

        # Node shape/style based on status
        status = action.get('status', ActionStatus.PLANNED.value)
        node_style = f":::{status}" # Use status directly for class name

        # Node Definition
        diagram.append(f'    {node_id}["{label}"]{node_style}')

        # Store parent relationship
        parent_action_id = action.get("parent_action_id")
        if parent_action_id:
            parent_links[action_id] = parent_action_id
        else:
            sequential_links[sequence_number] = action_id

    diagram.append("") # Spacer

    # Draw Links: Parent/Child first, then sequential for roots
    linked_actions = set()
    # Parent->Child links
    for child_id, parent_id in parent_links.items():
        if child_id in action_nodes and parent_id in action_nodes:
            child_node = action_nodes[child_id]
            parent_node = action_nodes[parent_id]
            diagram.append(f"    {parent_node} --> {child_node}")
            linked_actions.add(child_id) # Mark child as linked

    # Sequential links for actions without explicit parents
    last_sequential_node = wf_node_id # Start sequence from workflow node
    sorted_sequences = sorted(sequential_links.keys())
    for seq_num in sorted_sequences:
        action_id = sequential_links[seq_num]
        if action_id in action_nodes: # Ensure action node exists
             node_id = action_nodes[action_id]
             diagram.append(f"    {last_sequential_node} --> {node_id}")
             last_sequential_node = node_id # Chain sequential actions
             linked_actions.add(action_id) # Mark root as linked

    # Link any remaining unlinked actions (e.g., if parents were missing/invalid) sequentially
    for action in actions:
         action_id = action.get("action_id")
         if action_id and action_id not in linked_actions and action_id in action_nodes:
              node_id = action_nodes[action_id]
              # Link from workflow if no other link established
              diagram.append(f"    {wf_node_id} -.-> {node_id} :::orphanLink")
              logger.debug(f"Linking orphan action {action_id} to workflow.")

    diagram.append("") # Spacer

    # --- Artifact Nodes & Links ---
    artifacts = workflow.get("artifacts", [])
    if artifacts:
        for artifact in artifacts:
            artifact_id = artifact.get("artifact_id")
            if not artifact_id: 
                continue # Skip artifacts missing ID

            node_id = sanitize_mermaid_id(artifact_id, "F") # Use sanitized full ID
            artifact_name = _mermaid_escape(artifact.get('name', 'Artifact'))
            artifact_type = _mermaid_escape(artifact.get('artifact_type', 'file'))
            label = f"üìÑ<br/><b>{artifact_name}</b><br/>({artifact_type})"

            # Node shape/style based on type/output status
            node_shape_start, node_shape_end = "[(", ")]" # Default: capsule for artifacts
            node_style = ":::artifact"
            if artifact.get('is_output'):
                node_style = ":::artifact_output" # Style final outputs differently

            diagram.append(f'    {node_id}{node_shape_start}"{label}"{node_shape_end}{node_style}')

            # Link from creating action or workflow
            creator_action_id = artifact.get("action_id")
            if creator_action_id and creator_action_id in action_nodes:
                creator_node = action_nodes[creator_action_id]
                diagram.append(f"    {creator_node} -- Creates --> {node_id}")
            else:
                # Link artifact to workflow if no specific action created it
                diagram.append(f"    {wf_node_id} -.-> {node_id}")

    # --- Class Definitions (Full Set) ---
    diagram.append("\n    %% Stylesheets")
    diagram.append("    classDef workflow fill:#e7f0fd,stroke:#0056b3,stroke-width:2px,color:#000")
    # Action Statuses
    diagram.append("    classDef completed fill:#d4edda,stroke:#155724,stroke-width:1px,color:#155724")
    diagram.append("    classDef failed fill:#f8d7da,stroke:#721c24,stroke-width:1px,color:#721c24")
    diagram.append("    classDef skipped fill:#e2e3e5,stroke:#383d41,stroke-width:1px,color:#383d41")
    diagram.append("    classDef in_progress fill:#fff3cd,stroke:#856404,stroke-width:1px,color:#856404")
    diagram.append("    classDef planned fill:#fefefe,stroke:#6c757d,stroke-width:1px,color:#343a40,stroke-dasharray: 3 3")
    # Artifacts
    diagram.append("    classDef artifact fill:#fdfae7,stroke:#b3a160,stroke-width:1px,color:#333")
    diagram.append("    classDef artifact_output fill:#e7fdf4,stroke:#2e855d,stroke-width:2px,color:#000")
    diagram.append("    classDef orphanLink stroke:#ccc,stroke-dasharray: 2 2")

    diagram.append("```")
    return "\n".join(diagram)

async def _generate_thought_chain_mermaid(thought_chain: Dict[str, Any]) -> str:
    """Generates a detailed Mermaid flowchart of a thought chain."""

    def sanitize_mermaid_id(uuid_str: Optional[str], prefix: str) -> str:
        """Creates a valid Mermaid node ID from a UUID, handling None."""
        if not uuid_str:
             return f"{prefix}_MISSING_{MemoryUtils.generate_id().replace('-', '_')}"
        sanitized = uuid_str.replace("-", "_")
        return f"{prefix}_{sanitized}"

    diagram = ["```mermaid", "graph TD"] # Top-Down graph

    # --- Header Node ---
    chain_node_id = sanitize_mermaid_id(thought_chain.get('thought_chain_id'), "TC") # Use sanitized full ID
    chain_title = _mermaid_escape(thought_chain.get('title', 'Thought Chain'))
    diagram.append(f'    {chain_node_id}("{chain_title}"):::header')
    diagram.append("")

    # --- Thought Nodes ---
    thoughts = sorted(thought_chain.get("thoughts", []), key=lambda t: t.get("sequence_number", 0))
    thought_nodes = {} # Map thought_id to mermaid_node_id
    parent_links = {} # Map child_thought_id to parent_thought_id

    if thoughts:
        for thought in thoughts:
            thought_id = thought.get("thought_id")
            if not thought_id: 
                continue

            node_id = sanitize_mermaid_id(thought_id, "T") # Use sanitized full ID
            thought_nodes[thought_id] = node_id
            thought_type = thought.get('thought_type', 'thought').lower()

            # Node shape and style based on thought type
            shapes = {
                "goal": ("([", "])"), "question": ("{{", "}}"), "decision": ("[/", "\\]"),
                "summary": ("[(", ")]"), "constraint": ("[[", "]]"), "hypothesis": ("( ", " )")
            }
            node_shape_start, node_shape_end = shapes.get(thought_type, ("[", "]")) # Default rectangle
            node_style = f":::type{thought_type}"

            # Label content
            content = _mermaid_escape(thought.get('content', '...'))
            label = f"<b>{thought_type.capitalize()} #{thought.get('sequence_number')}</b><br/>{content}"

            diagram.append(f'    {node_id}{node_shape_start}"{label}"{node_shape_end}{node_style}')

            # Store parent relationship
            parent_id = thought.get("parent_thought_id")
            if parent_id:
                parent_links[thought_id] = parent_id

    diagram.append("")

    # --- Draw Links ---
    linked_thoughts = set()
    # Parent -> Child links
    for child_id, parent_id in parent_links.items():
        if child_id in thought_nodes and parent_id in thought_nodes:
            child_node = thought_nodes[child_id]
            parent_node = thought_nodes[parent_id]
            diagram.append(f"    {parent_node} --> {child_node}")
            linked_thoughts.add(child_id)

    # Link root thoughts (no parent or parent not found) sequentially from the header
    last_root_node = chain_node_id # Use sanitized chain node ID
    for thought in thoughts:
        thought_id = thought.get("thought_id")
        if thought_id and thought_id not in linked_thoughts and thought_id in thought_nodes:
            # Check if its parent exists in the fetched thoughts; if not, treat as root for linking
            parent_id = parent_links.get(thought_id)
            if not parent_id or parent_id not in thought_nodes:
                node_id = thought_nodes[thought_id]
                diagram.append(f"    {last_root_node} --> {node_id}")
                last_root_node = node_id # Chain subsequent roots
                linked_thoughts.add(thought_id)

    # --- External Links (Actions/Artifacts/Memories) ---
    if thoughts:
        diagram.append("")
        for thought in thoughts:
            thought_id = thought.get("thought_id")
            if not thought_id or thought_id not in thought_nodes:
                continue # Skip if thought or its node wasn't created
            node_id = thought_nodes[thought_id]

            # Link to relevant action
            rel_action_id = thought.get("relevant_action_id")
            if rel_action_id:
                ext_node_id = sanitize_mermaid_id(rel_action_id, "ExtA") # Sanitize external ID
                diagram.append(f'    {ext_node_id}["Action: {rel_action_id[:8]}..."]:::action')
                diagram.append(f"    {node_id} -.-> {ext_node_id}")

            # Link to relevant artifact
            rel_artifact_id = thought.get("relevant_artifact_id")
            if rel_artifact_id:
                ext_node_id = sanitize_mermaid_id(rel_artifact_id, "ExtF") # Sanitize external ID
                diagram.append(f'    {ext_node_id}[("Artifact: {rel_artifact_id[:8]}...")]:::artifact')
                diagram.append(f"    {node_id} -.-> {ext_node_id}")

            # Link to relevant memory
            rel_memory_id = thought.get("relevant_memory_id")
            if rel_memory_id:
                ext_node_id = sanitize_mermaid_id(rel_memory_id, "ExtM") # Sanitize external ID
                diagram.append(f'    {ext_node_id}("Memory: {rel_memory_id[:8]}..."):::memory')
                diagram.append(f"    {node_id} -.-> {ext_node_id}")

    # --- Class Definitions (Full Set) ---
    diagram.append("\n    %% Stylesheets")
    diagram.append("    classDef header fill:#666,stroke:#333,color:#fff,stroke-width:2px,font-weight:bold;")
    # Thought Types
    diagram.append("    classDef typegoal fill:#d4edda,stroke:#155724,color:#155724;")
    diagram.append("    classDef typequestion fill:#cce5ff,stroke:#004085,color:#004085;")
    diagram.append("    classDef typehypothesis fill:#e2e3e5,stroke:#383d41,color:#383d41;")
    diagram.append("    classDef typeinference fill:#fff3cd,stroke:#856404,color:#856404;")
    diagram.append("    classDef typeevidence fill:#d1ecf1,stroke:#0c5460,color:#0c5460;")
    diagram.append("    classDef typeconstraint fill:#f8d7da,stroke:#721c24,color:#721c24;")
    diagram.append("    classDef typeplan fill:#d6d8f8,stroke:#3f4d9a,color:#3f4d9a;")
    diagram.append("    classDef typedecision fill:#ffe6f5,stroke:#97114c,color:#97114c,font-weight:bold;")
    diagram.append("    classDef typereflection fill:#f5f5f5,stroke:#5a5a5a,color:#5a5a5a;")
    diagram.append("    classDef typecritique fill:#feeed8,stroke:#a34e00,color:#a34e00;")
    diagram.append("    classDef typesummary fill:#cfe2ff,stroke:#0a3492,color:#0a3492;")
    # External Links
    diagram.append("    classDef action fill:#f9f2f4,stroke:#c7254e,color:#c7254e,stroke-dasharray: 5 5;")
    diagram.append("    classDef artifact fill:#f3f6f9,stroke:#367fa9,color:#367fa9,stroke-dasharray: 5 5;")
    diagram.append("    classDef memory fill:#f0f0f0,stroke:#777,color:#333,stroke-dasharray: 2 2;")

    diagram.append("```")
    return "\n".join(diagram)

# --- 19. Reporting (Corrected Port from agent_memory) ---
@with_tool_metrics
@with_error_handling
async def generate_workflow_report(
    workflow_id: str,
    report_format: str = "markdown", # markdown, html, json, mermaid
    include_details: bool = True,
    include_thoughts: bool = True,
    include_artifacts: bool = True,
    style: Optional[str] = "professional",
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Generates a comprehensive report for a specific workflow in various formats and styles.

    Creates a human-readable summary or detailed log of a workflow's progress, actions,
    artifacts, and reasoning. For HTML reports, includes CSS for code syntax highlighting
    if the `pygments` library is installed.

    Args:
        workflow_id: The ID of the workflow to report on.
        report_format: (Optional) Output format: 'markdown', 'html', 'json', or 'mermaid'. Default 'markdown'.
        include_details: (Optional) Whether to include detailed sections like reasoning, arguments, results. Default True.
        include_thoughts: (Optional) Whether to include thought chain details. Default True.
        include_artifacts: (Optional) Whether to include artifact details. Default True.
        style: (Optional) Reporting style for Markdown/HTML: 'professional', 'concise',
               'narrative', or 'technical'. Default 'professional'. Ignored for JSON/Mermaid.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        A dictionary containing the generated report and metadata.
        {
            "workflow_id": "workflow-uuid",
            "title": "Workflow Title",
            "report": "Generated report content...",
            "format": "markdown",
            "style_used": "professional", # Included for clarity
            "generated_at": "iso-timestampZ",
            "success": true,
            "processing_time": 0.45
        }

    Raises:
        ToolInputError: If workflow not found or parameters are invalid.
        ToolError: If report generation fails.
    """
    # --- Input Validation ---
    if not workflow_id:
        raise ToolInputError("Workflow ID required.", param_name="workflow_id")

    valid_formats = ["markdown", "html", "json", "mermaid"]
    report_format_lower = report_format.lower()
    if report_format_lower not in valid_formats:
        raise ToolInputError(f"Invalid format '{report_format}'. Must be one of: {valid_formats}", param_name="report_format")

    valid_styles = ["professional", "concise", "narrative", "technical"]
    style_lower = (style or "professional").lower() # Default to professional if None
    if style_lower not in valid_styles:
        raise ToolInputError(f"Invalid style '{style}'. Must be one of: {valid_styles}", param_name="style")

    start_time = time.time()

    try:
        # --- Fetch Workflow Data ---
        # Fetch all potentially needed data, filtering happens during report generation
        workflow_data = await get_workflow_details(
            workflow_id=workflow_id,
            include_actions=True, # Fetch actions for all report types/styles
            include_artifacts=include_artifacts, # Fetch if requested
            include_thoughts=include_thoughts,   # Fetch if requested
            include_memories=False, # Keep reports focused on tracked items for now
            db_path=db_path
        )
        # get_workflow_details should raise ToolInputError if workflow not found
        if not workflow_data.get("success"): # Check just in case error handling fails
             raise ToolError(f"Failed to retrieve workflow details for report generation (ID: {workflow_id}).")

        # --- Generate Report Content ---
        report_content = None
        markdown_report_content = None # To store markdown for HTML conversion

        if report_format_lower == "markdown" or report_format_lower == "html":
            # Select the appropriate Markdown generation helper based on style
            if style_lower == "concise":
                markdown_report_content = await _generate_concise_report(workflow_data, include_details)
            elif style_lower == "narrative":
                markdown_report_content = await _generate_narrative_report(workflow_data, include_details)
            elif style_lower == "technical":
                markdown_report_content = await _generate_technical_report(workflow_data, include_details)
            else: # Default to professional
                markdown_report_content = await _generate_professional_report(workflow_data, include_details)

            if report_format_lower == "markdown":
                report_content = markdown_report_content
            else: # HTML format
                try:
                    # Convert markdown to HTML body content
                    html_body = markdown.markdown(
                        markdown_report_content,
                        extensions=['tables', 'fenced_code', 'codehilite']
                    )

                    # --- FIX: Generate and prepend CSS for code highlighting ---
                    pygments_css = ""
                    try:
                        # Use the default pygments CSS style
                        formatter = HtmlFormatter(style='default')
                        pygments_css = f"<style>{formatter.get_style_defs('.codehilite')}</style>"
                    except Exception as css_err:
                        logger.warning(f"Failed to generate Pygments CSS: {css_err}")
                        pygments_css = "<!-- Pygments CSS generation failed -->"

                    # Construct the full HTML document
                    report_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Workflow Report: {workflow_data.get('title', 'Untitled')}</title>
    {pygments_css}
    <style>
        body {{ font-family: sans-serif; line-height: 1.6; padding: 20px; }}
        table {{ border-collapse: collapse; width: 100%; margin-bottom: 1em; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        code {{ background-color: #f5f5f5; padding: 2px 4px; border-radius: 3px; }}
        pre code {{ display: block; padding: 10px; background-color: #f5f5f5; border: 1px solid #ddd; border-radius: 4px; }}
        /* Basic codehilite styles if pygments fails */
        .codehilite pre {{ white-space: pre-wrap; }}
    </style>
</head>
<body>
{html_body}
</body>
</html>"""
                except Exception as md_err:
                    logger.error(f"Markdown to HTML conversion failed: {md_err}", exc_info=True)
                    raise ToolError(f"Failed to convert report to HTML: {md_err}") from md_err

        elif report_format_lower == "json":
            # Return the structured data directly, cleaned up
            clean_data = {k: v for k, v in workflow_data.items() if k not in ['success', 'processing_time']} # Remove tool metadata
            try:
                report_content = json.dumps(clean_data, indent=2, ensure_ascii=False)
            except Exception as json_err:
                 logger.error(f"JSON serialization failed for report: {json_err}", exc_info=True)
                 raise ToolError(f"Failed to serialize workflow data to JSON: {json_err}") from json_err

        elif report_format_lower == "mermaid":
            # Generate the Mermaid diagram string
            report_content = await _generate_mermaid_diagram(workflow_data)

        # Final check if content generation failed unexpectedly
        if report_content is None:
            raise ToolError(f"Report content generation failed unexpectedly for format '{report_format_lower}' and style '{style_lower}'.")

        # --- Prepare Result ---
        result = {
            "workflow_id": workflow_id,
            "title": workflow_data.get("title", "Workflow Report"),
            "report": report_content,
            "format": report_format_lower,
            "style_used": style_lower if report_format_lower in ["markdown", "html"] else None, # Indicate style used
            "generated_at": to_iso_z(datetime.now(timezone.utc).timestamp()), # Use helper for consistency
            "success": True,
            "processing_time": time.time() - start_time
        }
        logger.info(f"Generated {report_format_lower} report (style: {style_lower if report_format_lower in ['markdown','html'] else 'N/A'}) for workflow {workflow_id}", emoji_key="newspaper")
        return result

    except (ToolInputError, ToolError):
        raise # Re-raise specific handled errors
    except Exception as e:
        logger.error(f"Unexpected error generating report for {workflow_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to generate workflow report due to an unexpected error: {str(e)}") from e

# --- 20. Visualization (Ported from agent_memory) ---
@with_tool_metrics
@with_error_handling
async def visualize_reasoning_chain(
    thought_chain_id: str,
    output_format: str = "mermaid", # mermaid, json
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Generates a visualization of a specific thought chain."""
    if not thought_chain_id: 
        raise ToolInputError("Thought chain ID required.", param_name="thought_chain_id")
    valid_formats = ["mermaid", "json"]
    if output_format.lower() not in valid_formats:
         raise ToolInputError(f"Invalid format. Use one of: {valid_formats}", param_name="output_format")
    output_format = output_format.lower()
    start_time = time.time()

    try:
        # Fetch the thought chain data
        thought_chain_data = await get_thought_chain(thought_chain_id, db_path=db_path)
        if not thought_chain_data.get("success"):
             raise ToolError(f"Failed to retrieve thought chain {thought_chain_id} for visualization.")

        visualization_content = None
        # Generate visualization content
        if output_format == "mermaid":
            visualization_content = await _generate_thought_chain_mermaid(thought_chain_data)
        elif output_format == "json":
            # Create hierarchical JSON structure
            structured_chain = {k: v for k, v in thought_chain_data.items() if k not in ['success', 'thoughts']}
            child_map = defaultdict(list)
            all_thoughts = thought_chain_data.get("thoughts", [])
            for thought in all_thoughts: 
                child_map[thought.get("parent_thought_id")].append(thought)

            def build_tree(thought_list):
                tree = []
                for thought in thought_list:
                    node = dict(thought)
                    children = child_map.get(thought["thought_id"])
                    if children:
                        node["children"] = build_tree(children)
                    tree.append(node)
                return tree

            structured_chain["thought_tree"] = build_tree(child_map.get(None, [])) # Start with roots (None parent)
            visualization_content = json.dumps(structured_chain, indent=2)

        if visualization_content is None:
            raise ToolError(f"Failed to generate visualization content for format '{output_format}'.")

        result = {
            "thought_chain_id": thought_chain_id,
            "title": thought_chain_data.get("title", "Thought Chain"),
            "visualization": visualization_content,
            "format": output_format,
            "success": True,
            "processing_time": time.time() - start_time
        }
        logger.info(f"Generated {output_format} visualization for thought chain {thought_chain_id}", emoji_key="projector")
        return result

    except (ToolInputError, ToolError):
        raise
    except Exception as e:
        logger.error(f"Error visualizing thought chain {thought_chain_id}: {e}", exc_info=True)
        raise ToolError(f"Failed to visualize thought chain: {str(e)}") from e


async def _generate_professional_report(workflow: Dict[str, Any], include_details: bool) -> str:
    """Generates a professional-style report with formal structure and comprehensive details."""
    report_lines = [f"# Workflow Report: {workflow.get('title', 'Untitled Workflow')}"]

    # --- Executive Summary ---
    report_lines.append("\n## Executive Summary\n")
    report_lines.append(f"**Status:** {workflow.get('status', 'N/A').capitalize()}")
    if workflow.get("goal"): 
        report_lines.append(f"**Goal:** {workflow['goal']}")
    if workflow.get("description"):
        report_lines.append(f"\n{workflow['description']}")
    report_lines.append(f"\n**Created:** {to_iso_z(workflow.get('created_at_unix'))}") # Assuming get_details returns unix ts
    report_lines.append(f"**Last Updated:** {to_iso_z(workflow.get('updated_at_unix'))}")
    if workflow.get("completed_at_unix"): 
        report_lines.append(f"**Completed:** {to_iso_z(workflow.get('completed_at_unix'))}")
    if workflow.get("tags"): 
        report_lines.append(f"**Tags:** {', '.join(workflow['tags'])}")

    # --- Progress Overview ---
    actions = workflow.get("actions", [])
    if actions:
        total_actions = len(actions)
        completed_actions_count = sum(1 for a in actions if a.get("status") == ActionStatus.COMPLETED.value)
        completion_percentage = int((completed_actions_count / total_actions) * 100) if total_actions > 0 else 0
        report_lines.append("\n## Progress Overview\n")
        report_lines.append(f"Overall completion: **{completion_percentage}%** ({completed_actions_count}/{total_actions} actions completed)")
        bar_filled = '#' * (completion_percentage // 5)
        bar_empty = ' ' * (20 - (completion_percentage // 5))
        report_lines.append(f"\n```\n[{bar_filled}{bar_empty}] {completion_percentage}%\n```")

    # --- Key Actions and Steps ---
    if actions and include_details:
        report_lines.append("\n## Key Actions and Steps\n")
        sorted_actions = sorted(actions, key=lambda a: a.get("sequence_number", 0))
        for i, action in enumerate(sorted_actions):
            status_emoji = {"completed": "‚úÖ", "failed": "‚ùå", "skipped": "‚è≠Ô∏è", "in_progress": "‚è≥", "planned": "üóìÔ∏è"}.get(action.get('status'), '‚ùì')
            title = action.get('title', action.get('action_type', 'Action')).strip()
            report_lines.append(f"### {i+1}. {status_emoji} {title}\n")
            report_lines.append(f"**Action ID:** `{action.get('action_id')}`")
            report_lines.append(f"**Type:** {action.get('action_type', 'N/A').capitalize()}")
            report_lines.append(f"**Status:** {action.get('status', 'N/A').capitalize()}")
            report_lines.append(f"**Started:** {action.get('started_at', 'N/A')}") # Assumes get_details already formatted these
            if action.get("completed_at"): 
                report_lines.append(f"**Completed:** {action['completed_at']}")

            if action.get("reasoning"): 
                report_lines.append(f"\n**Reasoning:**\n```\n{action['reasoning']}\n```")
            if action.get("tool_name"):
                report_lines.append(f"\n**Tool Used:** `{action['tool_name']}`")
                if action.get("tool_args"):
                    try: 
                        args_str = json.dumps(action['tool_args'], indent=2)
                    except TypeError: 
                        args_str = str(action['tool_args'])
                    report_lines.append(f"**Arguments:**\n```json\n{args_str}\n```")
                if action.get("tool_result"):
                    result_repr = action['tool_result']
                    try:
                        result_str = json.dumps(result_repr, indent=2)
                        lang = "json"
                    except TypeError:
                        result_str = str(result_repr)
                        lang = ""
                    if len(result_str) > 500: 
                        result_str = result_str[:497] + "..."
                    report_lines.append(f"**Result Preview:**\n```{lang}\n{result_str}\n```")

            if action.get("tags"): 
                report_lines.append(f"**Tags:** {', '.join(action['tags'])}")
            report_lines.append("\n---") # Separator

    # --- Key Findings & Insights (from Thoughts) ---
    thought_chains = workflow.get("thought_chains", [])
    if thought_chains and include_details:
        report_lines.append("\n## Key Findings & Insights (from Reasoning)\n")
        for i, chain in enumerate(thought_chains):
            report_lines.append(f"### Reasoning Chain {i+1}: {chain.get('title', 'Untitled')}\n")
            thoughts = sorted(chain.get("thoughts", []), key=lambda t: t.get("sequence_number", 0))
            if not thoughts: 
                report_lines.append("_No thoughts recorded in this chain._")
            else:
                for thought in thoughts:
                    is_key_thought = thought.get('thought_type') in ['goal', 'decision', 'summary', 'hypothesis', 'inference', 'reflection', 'critique']
                    prefix = "**" if is_key_thought else ""
                    suffix = "**" if is_key_thought else ""
                    type_label = thought.get('thought_type', 'Thought').capitalize()
                    thought_time = thought.get('created_at', 'N/A') # Assumes get_details already formatted
                    report_lines.append(f"- {prefix}{type_label}{suffix} ({thought_time}): {thought.get('content', '')}")
                    links = []
                    if thought.get('relevant_action_id'):
                        links.append(f"Action `{thought['relevant_action_id'][:8]}`")
                    if thought.get('relevant_artifact_id'): 
                        links.append(f"Artifact `{thought['relevant_artifact_id'][:8]}`")
                    if thought.get('relevant_memory_id'):
                        links.append(f"Memory `{thought['relevant_memory_id'][:8]}`")
                    if links:
                        report_lines.append(f"  *Related to:* {', '.join(links)}")
            report_lines.append("")

    # --- Artifacts & Outputs ---
    artifacts = workflow.get("artifacts", [])
    if artifacts and include_details:
        report_lines.append("\n## Artifacts & Outputs\n")
        report_lines.append("| Name | Type | Description | Path/Preview | Created | Tags | Output? |")
        report_lines.append("| ---- | ---- | ----------- | ------------ | ------- | ---- | ------- |")
        for artifact in artifacts:
            name = artifact.get('name', 'N/A')
            atype = artifact.get('artifact_type', 'N/A')
            desc = (artifact.get('description', '') or '')[:50]
            path_or_preview = artifact.get('path', '') or (artifact.get('content_preview', '') or '')
            path_or_preview = f"`{path_or_preview}`" if artifact.get('path') else path_or_preview[:60]
            created_time = artifact.get('created_at', 'N/A') # Assumes get_details already formatted
            tags = ', '.join(artifact.get('tags', []))
            is_output = "Yes" if artifact.get('is_output') else "No"
            report_lines.append(f"| {name} | {atype} | {desc} | {path_or_preview} | {created_time} | {tags} | {is_output} |")

    # --- Conclusion / Next Steps ---
    report_lines.append("\n## Conclusion & Next Steps\n")
    status = workflow.get("status", "N/A")
    # ... (rest of conclusion logic remains the same) ...
    if status == WorkflowStatus.COMPLETED.value:
        report_lines.append("Workflow marked as **Completed**.")
    elif status == WorkflowStatus.FAILED.value:
        report_lines.append("Workflow marked as **Failed**.")
    elif status == WorkflowStatus.ABANDONED.value: 
        report_lines.append("Workflow marked as **Abandoned**.")
    elif status == WorkflowStatus.PAUSED.value:
        report_lines.append("Workflow is currently **Paused**.")
    else: # Active
        report_lines.append("Workflow is **Active**. Potential next steps include:")
        last_action = sorted(actions, key=lambda a: a.get("sequence_number", 0))[-1] if actions else None
        if last_action and last_action.get("status") == ActionStatus.IN_PROGRESS.value:
            report_lines.append(f"- Completing action: '{last_action.get('title', 'Last Action')}'")
        elif last_action:
            report_lines.append(f"- Planning the next action after '{last_action.get('title', 'Last Action')}'")
        else: 
            report_lines.append("- Defining the initial actions for the workflow goal.")

    # Footer
    report_lines.append("\n---\n*Report generated on " + to_iso_z(datetime.now(timezone.utc).timestamp()) + "Z*")
    return "\n".join(report_lines)

# --- Concise, Narrative, Technical Reports (Keep these as separate functions for clarity) ---

async def _generate_concise_report(workflow: Dict[str, Any], include_details: bool) -> str:
    """Generates a concise report focusing on key information."""
    report_lines = [f"# {workflow.get('title', 'Untitled Workflow')} (`{workflow.get('workflow_id', '')[:8]}`)"]
    report_lines.append(f"**Status:** {workflow.get('status', 'N/A').capitalize()}")
    if workflow.get("goal"): 
        report_lines.append(f"**Goal:** {workflow.get('goal', '')[:100]}...")

    actions = workflow.get("actions", [])
    if actions:
        total = len(actions)
        completed = sum(1 for a in actions if a.get("status") == ActionStatus.COMPLETED.value)
        perc = int((completed / total) * 100) if total > 0 else 0
        report_lines.append(f"**Progress:** {perc}% ({completed}/{total} actions)")

    # Recent/Current Actions
    if actions:
         report_lines.append("\n**Recent Activity:**")
         sorted_actions = sorted(actions, key=lambda a: a.get("sequence_number", 0), reverse=True)
         for action in sorted_actions[:3]: # Show top 3 recent
              status_emoji = {"completed": "‚úÖ", "failed": "‚ùå", "skipped": "‚è≠Ô∏è", "in_progress": "‚è≥", "planned": "üóìÔ∏è"}.get(action.get('status'), '‚ùì')
              report_lines.append(f"- {status_emoji} {action.get('title', 'Action')[:50]}")

    # Outputs
    artifacts = workflow.get("artifacts", [])
    outputs = [a for a in artifacts if a.get('is_output')]
    if outputs:
        report_lines.append("\n**Outputs:**")
        for output in outputs[:5]: # Limit outputs listed
            report_lines.append(f"- {output.get('name', 'N/A')} (`{output.get('artifact_type', 'N/A')}`)")

    return "\n".join(report_lines)

async def _generate_narrative_report(workflow: Dict[str, Any], include_details: bool) -> str:
    """Generates a narrative-style report as a story."""
    report_lines = [f"# The Journey of: {workflow.get('title', 'Untitled Workflow')}"]

    # Helper to format timestamp or return 'N/A'
    def format_ts(unix_ts_or_iso_str):
        if isinstance(unix_ts_or_iso_str, int):
             try: 
                 return to_iso_z(unix_ts_or_iso_str)
             except (TypeError, ValueError): 
                 return 'an unknown time'
        elif isinstance(unix_ts_or_iso_str, str):
             try: 
                 return to_iso_z(unix_ts_or_iso_str)
             except (TypeError, ValueError): 
                 return 'an unknown time'
        return 'an unknown time'

    # Introduction
    report_lines.append("\n## Our Quest Begins\n")
    start_time = format_ts(workflow.get('created_at')) # Assumes get_details returned ISO string
    if workflow.get("goal"):
        report_lines.append(f"We embarked on a mission around {start_time}: **{workflow['goal']}**.")
    else:
        report_lines.append(f"Our story started on {start_time}, aiming to understand or create '{workflow.get('title', 'something interesting')}'")
    if workflow.get("description"): 
        report_lines.append(f"> {workflow['description']}\n")

    # The Path
    actions = workflow.get("actions", [])
    if actions:
        report_lines.append("## The Path Unfolds\n")
        sorted_actions = sorted(actions, key=lambda a: a.get("sequence_number", 0))
        for action in sorted_actions:
            title = action.get('title', action.get('action_type', 'A step'))
            start_time_action = format_ts(action.get('started_at')) # Assumes get_details returned ISO string
            if action.get("status") == ActionStatus.COMPLETED.value:
                 report_lines.append(f"Then, around {start_time_action}, we successfully **{title}**.")
                 if include_details and action.get('reasoning'): 
                     report_lines.append(f"  *Our reasoning was: {action['reasoning'][:150]}...*")
            elif action.get("status") == ActionStatus.FAILED.value:
                 report_lines.append(f"Around {start_time_action}, we encountered trouble when trying to **{title}**.")
            elif action.get("status") == ActionStatus.IN_PROGRESS.value:
                 report_lines.append(f"Starting around {start_time_action}, we are working on **{title}**.")
            report_lines.append("")

    # Discoveries
    thoughts = [t for chain in workflow.get("thought_chains", []) for t in chain.get("thoughts", [])]
    key_thoughts = [t for t in thoughts if t.get('thought_type') in ['decision', 'insight', 'hypothesis', 'summary', 'reflection']] # Added insight here
    if key_thoughts and include_details:
         report_lines.append("## Moments of Clarity\n")
         sorted_thoughts = sorted(key_thoughts, key=lambda t: t.get("sequence_number", 0))
         for thought in sorted_thoughts[:7]:
              thought_time = format_ts(thought.get('created_at')) # Assumes get_details returned ISO string
              report_lines.append(f"- Around {thought_time}, a key **{thought.get('thought_type')}** emerged: *{thought.get('content', '')[:150]}...*")

    # Treasures
    artifacts = workflow.get("artifacts", [])
    if artifacts and include_details:
         report_lines.append("\n## Treasures Found\n")
         outputs = [a for a in artifacts if a.get('is_output')]
         other_artifacts = [a for a in artifacts if not a.get('is_output')]
         display_artifacts = outputs[:3] + other_artifacts[:(5-len(outputs))]
         for artifact in display_artifacts:
              marker = "üèÜ Final Result:" if artifact.get('is_output') else "üìå Item Created:"
              artifact_time = format_ts(artifact.get('created_at')) # Assumes get_details returned ISO string
              report_lines.append(f"- {marker} Around {artifact_time}, **{artifact.get('name')}** ({artifact.get('artifact_type')}) was produced.")
              if artifact.get('description'):
                  report_lines.append(f"  *{artifact['description'][:100]}...*")

    # Current Status/Ending
    status = workflow.get("status", "active")
    report_lines.append(f"\n## {'Journey\'s End' if status == 'completed' else 'The Story So Far...'}\n")
    if status == WorkflowStatus.COMPLETED.value:
        report_lines.append("Our quest is complete! We achieved our objectives.")
    elif status == WorkflowStatus.FAILED.value:
        report_lines.append("Alas, this chapter ends here, marked by challenges we could not overcome.")
    elif status == WorkflowStatus.ABANDONED.value:
        report_lines.append("We chose to leave this path, perhaps to return another day.")
    elif status == WorkflowStatus.PAUSED.value: 
        report_lines.append("We pause here, taking stock before continuing the adventure.")
    else: 
        report_lines.append("The journey continues...")


    report_lines.append("\n---\n*Narrative recorded on " + to_iso_z(datetime.now()))
    return "\n".join(report_lines)

async def _generate_technical_report(workflow: Dict[str, Any], include_details: bool) -> str:
    """Generates a technical report with data-oriented structure."""
    report_lines = [f"# Technical Report: {workflow.get('title', 'Untitled Workflow')}"]

    # --- Metadata ---
    report_lines.append("\n## Workflow Metadata\n```yaml")
    report_lines.append(f"workflow_id: {workflow.get('workflow_id')}")
    report_lines.append(f"title: {workflow.get('title')}")
    report_lines.append(f"status: {workflow.get('status')}")
    report_lines.append(f"goal: {workflow.get('goal') or 'N/A'}")
    report_lines.append(f"created_at: {to_iso_z(workflow.get('created_at'))}")
    report_lines.append(f"updated_at: {to_iso_z(workflow.get('updated_at'))}")
    if workflow.get("completed_at"): 
        report_lines.append(f"completed_at: {to_iso_z(workflow.get('completed_at'))}")
    if workflow.get("tags"):
        report_lines.append(f"tags: {workflow['tags']}")
    report_lines.append("```")

    # --- Metrics ---
    actions = workflow.get("actions", [])
    if actions:
        report_lines.append("\n## Execution Metrics\n")
        total = len(actions)
        counts = defaultdict(int)
        for a in actions: 
            counts[a.get("status", "unknown")] += 1
        report_lines.append("**Action Status Counts:**")
        for status, count in counts.items(): 
            report_lines.append(f"- {status.capitalize()}: {count} ({int(count/total*100)}%)")
        type_counts = defaultdict(int)
        for a in actions: 
            type_counts[a.get("action_type", "unknown")] += 1
        report_lines.append("\n**Action Type Counts:**")
        for atype, count in type_counts.items(): 
            report_lines.append(f"- {atype.capitalize()}: {count} ({int(count/total*100)}%)")

    # --- Action Log ---
    if actions and include_details:
        report_lines.append("\n## Action Log\n")
        sorted_actions = sorted(actions, key=lambda a: a.get("sequence_number", 0))
        for action in sorted_actions:
            report_lines.append(f"### Action Sequence: {action.get('sequence_number')}\n```yaml")
            report_lines.append(f"action_id: {action.get('action_id')}")
            report_lines.append(f"title: {action.get('title')}")
            report_lines.append(f"type: {action.get('action_type')}")
            report_lines.append(f"status: {action.get('status')}")
            report_lines.append(f"started_at: {to_iso_z(action.get('started_at'))}")
            if action.get("completed_at"): 
                report_lines.append(f"completed_at: {to_iso_z(action.get('completed_at'))}")
            if action.get("tool_name"): 
                report_lines.append(f"tool_name: {action['tool_name']}")
            if action.get("tool_args"): 
                report_lines.append(f"tool_args_preview: {str(action['tool_args'])[:100]}...")
            if action.get("tool_result"):
                report_lines.append(f"tool_result_preview: {str(action['tool_result'])[:100]}...")
            report_lines.append("```")
            if action.get("reasoning"):
                report_lines.append(f"**Reasoning:**\n```\n{action['reasoning']}\n```")

    # --- Artifacts ---
    artifacts = workflow.get("artifacts", [])
    if artifacts and include_details:
        report_lines.append("\n## Artifacts\n```json")
        artifact_list_repr = []
        for artifact in artifacts:
            repr_dict = {k: artifact.get(k) for k in ["artifact_id", "name", "artifact_type", "description", "path", "is_output", "tags", "created_at"]}
            if "created_at" in repr_dict:
                repr_dict["created_at"] = to_iso_z(repr_dict["created_at"])
            artifact_list_repr.append(repr_dict)
        report_lines.append(json.dumps(artifact_list_repr, indent=2))
        report_lines.append("```")

    # --- Thoughts ---
    thought_chains = workflow.get("thought_chains", [])
    if thought_chains and include_details:
        report_lines.append("\n## Thought Chains\n")
        for chain in thought_chains:
             report_lines.append(f"### Chain: {chain.get('title')} (`{chain.get('thought_chain_id')}`)\n```json")
             thoughts = sorted(chain.get("thoughts", []), key=lambda t: t.get("sequence_number", 0))
             formatted_thoughts = []
             for thought in thoughts:
                 fmt_thought = dict(thought)
                 if fmt_thought.get("created_at"):
                     fmt_thought["created_at"] = to_iso_z(fmt_thought["created_at"])
                 formatted_thoughts.append(fmt_thought)
             report_lines.append(json.dumps(formatted_thoughts, indent=2))
             report_lines.append("```")

    return "\n".join(report_lines)

async def _generate_memory_network_mermaid(memories: List[Dict], links: List[Dict], center_memory_id: Optional[str] = None) -> str:
    """Helper function to generate Mermaid graph syntax for a memory network."""

    def sanitize_mermaid_id(uuid_str: Optional[str], prefix: str) -> str:
        """Creates a valid Mermaid node ID from a UUID, handling None."""
        if not uuid_str:
             return f"{prefix}_MISSING_{MemoryUtils.generate_id().replace('-', '_')}"
        sanitized = uuid_str.replace("-", "_")
        return f"{prefix}_{sanitized}"

    diagram = ["```mermaid", "graph TD"] # Top-Down graph direction

    # Node Definitions
    diagram.append("\n    %% Memory Nodes")
    memory_id_to_node_id = {} # Map full memory ID to sanitized Mermaid node ID
    for memory in memories:
        mem_id = memory.get("memory_id")
        if not mem_id: 
            continue

        node_id = sanitize_mermaid_id(mem_id, "M") # Use sanitized full ID
        memory_id_to_node_id[mem_id] = node_id # Store mapping

        # Label content: Type, Description (truncated), Importance
        mem_type = memory.get("memory_type", "memory").capitalize()
        desc = _mermaid_escape(memory.get("description", mem_id)) # Use full ID if no desc
        if len(desc) > 40:
            desc = desc[:37] + "..."
        importance = memory.get('importance', 5.0)
        label = f"<b>{mem_type}</b><br/>{desc}<br/><i>(I: {importance:.1f})</i>"

        # Node shape/style based on level (e.g., Semantic=rectangle, Episodic=rounded, Procedural=subroutine)
        level = memory.get("memory_level", MemoryLevel.EPISODIC.value)
        shape_start, shape_end = "[", "]" # Default rectangle (Semantic)
        if level == MemoryLevel.EPISODIC.value:
            shape_start, shape_end = "(", ")" # Rounded rectangle
        elif level == MemoryLevel.PROCEDURAL.value:
            shape_start, shape_end = "[[", "]]" # Subroutine shape
        elif level == MemoryLevel.WORKING.value:
             shape_start, shape_end = "([", "])" # Capsule shape for working memory?

        # Style based on level + highlight center node
        node_style = f":::level{level}"
        if mem_id == center_memory_id:
            node_style += " :::centerNode" # Add specific style for center

        diagram.append(f'    {node_id}{shape_start}"{label}"{shape_end}{node_style}')

    # Edge Definitions
    diagram.append("\n    %% Memory Links")
    for link in links:
        source_mem_id = link.get("source_memory_id")
        target_mem_id = link.get("target_memory_id")
        link_type = link.get("link_type", "related")

        # Only draw links where both source and target are in the visualized node set
        if source_mem_id in memory_id_to_node_id and target_mem_id in memory_id_to_node_id:
            source_node = memory_id_to_node_id[source_mem_id]
            target_node = memory_id_to_node_id[target_mem_id]
            # Add link type as label, style based on strength? (Keep simple for now)
            diagram.append(f"    {source_node} -- {link_type} --> {target_node}")

    # Class Definitions for Styling
    diagram.append("\n    %% Stylesheets")
    diagram.append("    classDef levelworking fill:#e3f2fd,stroke:#2196f3,color:#1e88e5,stroke-width:1px;") # Light blue
    diagram.append("    classDef levelepisodic fill:#e8f5e9,stroke:#4caf50,color:#388e3c,stroke-width:1px;") # Light green
    diagram.append("    classDef levelsemantic fill:#fffde7,stroke:#ffc107,color:#ffa000,stroke-width:1px;") # Light yellow
    diagram.append("    classDef levelprocedural fill:#fce4ec,stroke:#e91e63,color:#c2185b,stroke-width:1px;") # Light pink
    diagram.append("    classDef centerNode stroke-width:3px,stroke:#0d47a1,font-weight:bold;") # Darker blue border for center

    diagram.append("```")
    return "\n".join(diagram)


@with_tool_metrics
@with_error_handling
async def visualize_memory_network(
    workflow_id: Optional[str] = None,
    center_memory_id: Optional[str] = None,
    depth: int = 1, # How many steps away from the center node to include
    max_nodes: int = 30, # Max total memory nodes to display
    memory_level: Optional[str] = None, # Filter nodes by level
    memory_type: Optional[str] = None, # Filter nodes by type
    output_format: str = "mermaid", # Only mermaid for now
    db_path: str = DEFAULT_DB_PATH
) -> Dict[str, Any]:
    """Generates a visualization of the memory network for a workflow or around a specific memory.

    Creates a Mermaid graph showing memories as nodes and links between them as edges.
    Can be focused around a central memory or show the most relevant memories in a workflow.

    Args:
        workflow_id: (Optional) ID of the workflow to visualize. Required if center_memory_id is not provided.
        center_memory_id: (Optional) ID of the memory to center the visualization around.
        depth: (Optional) If center_memory_id is provided, how many link steps away to include (e.g., 1=immediate neighbors). Default 1.
        max_nodes: (Optional) Maximum total number of memory nodes to include in the graph. Default 30.
        memory_level: (Optional) Filter included memories by level (applied after neighbor selection if centered).
        memory_type: (Optional) Filter included memories by type (applied after neighbor selection if centered).
        output_format: (Optional) Currently only supports 'mermaid'. Default 'mermaid'.
        db_path: (Optional) Path to the SQLite database file.

    Returns:
        Dictionary containing the Mermaid visualization string.
        {
            "workflow_id": "workflow-uuid" | None,
            "center_memory_id": "center-uuid" | None,
            "visualization": "```mermaid\ngraph TD\n...",
            "node_count": 25,
            "link_count": 35,
            "format": "mermaid",
            "success": true,
            "processing_time": 0.25
        }

    Raises:
        ToolInputError: If required parameters are missing, invalid, or entities not found.
        ToolError: If database or visualization generation fails.
    """
    if not workflow_id and not center_memory_id:
        raise ToolInputError("Either workflow_id or center_memory_id must be provided.", param_name="workflow_id")
    if output_format.lower() != "mermaid":
        raise ToolInputError("Currently only 'mermaid' format is supported.", param_name="output_format")
    if depth < 0:
        raise ToolInputError("Depth cannot be negative.", param_name="depth")
    if max_nodes <= 0:
        raise ToolInputError("Max nodes must be positive.", param_name="max_nodes")

    start_time = time.time()
    selected_memory_ids = set() # Keep track of memory IDs to include
    memories_data = {} # Store fetched memory details {mem_id: data}
    links_data = [] # Store fetched links between selected memories

    try:
        async with DBConnection(db_path) as conn:
            # --- 1. Fetch Initial Set of Memory IDs ---
            target_workflow_id = workflow_id # Use workflow_id if provided

            if center_memory_id:
                # Fetch center memory to get its workflow_id if not provided
                async with conn.execute("SELECT workflow_id FROM memories WHERE memory_id = ?", (center_memory_id,)) as cursor:
                    center_row = await cursor.fetchone()
                    if not center_row:
                         raise ToolInputError(f"Center memory {center_memory_id} not found.", param_name="center_memory_id")
                    if not target_workflow_id:
                         target_workflow_id = center_row["workflow_id"]
                    elif target_workflow_id != center_row["workflow_id"]:
                         raise ToolInputError(f"Center memory {center_memory_id} does not belong to workflow {target_workflow_id}.", param_name="center_memory_id")

                # BFS-like approach to get neighbors up to depth
                queue = {center_memory_id}
                visited = set()
                for current_depth in range(depth + 1): # Iterate depth + 1 times (depth 0 is the center node itself)
                    if len(selected_memory_ids) >= max_nodes: 
                        break # Stop if max nodes reached
                    
                    next_queue = set()
                    ids_to_query = list(queue - visited) # Process only new nodes at this level
                    if not ids_to_query: 
                        break # No new nodes to explore

                    visited.update(ids_to_query)
                    
                    # Add nodes found at this level to the final selection
                    for node_id in ids_to_query:
                        if len(selected_memory_ids) < max_nodes:
                            selected_memory_ids.add(node_id)
                        else:
                            break # Hit max nodes limit

                    if current_depth < depth and len(selected_memory_ids) < max_nodes: # Don't fetch neighbors on the last level or if max nodes hit
                        # Fetch direct neighbors (both incoming and outgoing) for the next level
                        placeholders = ', '.join(['?'] * len(ids_to_query))
                        neighbor_query = f"""
                            SELECT target_memory_id as neighbor_id FROM memory_links WHERE source_memory_id IN ({placeholders})
                            UNION
                            SELECT source_memory_id as neighbor_id FROM memory_links WHERE target_memory_id IN ({placeholders})
                        """
                        async with conn.execute(neighbor_query, ids_to_query * 2) as cursor:
                            async for row in cursor:
                                if row["neighbor_id"] not in visited:
                                     next_queue.add(row["neighbor_id"])

                    queue = next_queue # Move to the next level

            else: # No center node, get top memories based on filters/sorting
                if not target_workflow_id: # Should have been caught earlier, but safeguard
                     raise ToolInputError("Workflow ID is required when not specifying a center memory.", param_name="workflow_id")

                filter_where = ["workflow_id = ?"]
                filter_params: List[Any] = [target_workflow_id]
                if memory_level:
                    filter_where.append("memory_level = ?")
                    filter_params.append(memory_level.lower())
                if memory_type:
                    filter_where.append("memory_type = ?")
                    filter_params.append(memory_type.lower())

                # Add TTL check
                now_unix = int(time.time())
                filter_where.append("(ttl = 0 OR created_at + ttl > ?)")
                filter_params.append(now_unix)

                where_sql = " AND ".join(filter_where)
                # Fetch most important/relevant first
                query = f"""
                    SELECT memory_id
                    FROM memories
                    WHERE {where_sql}
                    ORDER BY compute_memory_relevance(importance, confidence, created_at, access_count, last_accessed) DESC
                    LIMIT ?
                """
                filter_params.append(max_nodes)
                async with conn.execute(query, filter_params) as cursor:
                    selected_memory_ids = {row["memory_id"] for row in await cursor.fetchall()}

            if not selected_memory_ids:
                logger.info("No memories selected for visualization based on criteria.")
                return { # Return empty graph structure
                     "workflow_id": target_workflow_id, "center_memory_id": center_memory_id,
                     "visualization": "```mermaid\ngraph TD\n    NoNodes[No memories found]\n```",
                     "node_count": 0, "link_count": 0, "format": "mermaid", "success": True,
                     "processing_time": time.time() - start_time
                }

            # --- 2. Fetch Details for Selected Memories ---
            # Apply level/type filters *now* if a center_memory_id was used
            # (We needed neighbors first, now filter the selected set)
            fetch_ids = list(selected_memory_ids)
            placeholders = ', '.join(['?'] * len(fetch_ids))
            details_query = f"SELECT * FROM memories WHERE memory_id IN ({placeholders})"
            details_params = fetch_ids
            
            final_selected_ids = set() # Rebuild the set after filtering

            async with conn.execute(details_query, details_params) as cursor:
                 async for row in cursor:
                      mem_data = dict(row)
                      # Apply post-filters if centering was used
                      if center_memory_id:
                           passes_filter = True
                           if memory_level and mem_data.get("memory_level") != memory_level.lower():
                                passes_filter = False
                           if memory_type and mem_data.get("memory_type") != memory_type.lower():
                                passes_filter = False
                           
                           # Keep the center node even if it doesn't match filters
                           if passes_filter or mem_data["memory_id"] == center_memory_id:
                                memories_data[mem_data["memory_id"]] = mem_data
                                final_selected_ids.add(mem_data["memory_id"])
                      else: # If not centering, filters were applied in the initial query
                           memories_data[mem_data["memory_id"]] = mem_data
                           final_selected_ids.add(mem_data["memory_id"])
            
            # Ensure center node is included if specified and exists, even if filters applied
            if center_memory_id and center_memory_id not in final_selected_ids:
                 if center_memory_id in memories_data: # Check if fetched but filtered out
                      final_selected_ids.add(center_memory_id)
                 # else: it wasn't fetched initially, likely doesn't exist or error occurred

            # Check again if filtering removed all nodes
            if not final_selected_ids:
                logger.info("No memories remained after applying filters for visualization.")
                # Return empty graph
                return {
                     "workflow_id": target_workflow_id, "center_memory_id": center_memory_id,
                     "visualization": "```mermaid\ngraph TD\n    NoNodes[No memories match filters]\n```",
                     "node_count": 0, "link_count": 0, "format": "mermaid", "success": True,
                     "processing_time": time.time() - start_time
                }


            # --- 3. Fetch Links BETWEEN Selected Memories ---
            final_ids_list = list(final_selected_ids)
            placeholders = ', '.join(['?'] * len(final_ids_list))
            links_query = f"""
                SELECT * FROM memory_links
                WHERE source_memory_id IN ({placeholders}) AND target_memory_id IN ({placeholders})
            """
            async with conn.execute(links_query, final_ids_list * 2) as cursor:
                 links_data = [dict(row) for row in await cursor.fetchall()]

            # --- 4. Generate Mermaid Diagram ---
            # Use the final filtered memories_data values
            mermaid_string = await _generate_memory_network_mermaid(list(memories_data.values()), links_data, center_memory_id)

            # --- 5. Return Result ---
            processing_time = time.time() - start_time
            node_count = len(memories_data)
            link_count = len(links_data)
            logger.info(f"Generated memory network visualization ({node_count} nodes, {link_count} links) for workflow {target_workflow_id}", emoji_key="network")
            return {
                "workflow_id": target_workflow_id,
                "center_memory_id": center_memory_id,
                "visualization": mermaid_string,
                "node_count": node_count,
                "link_count": link_count,
                "format": "mermaid",
                "success": True,
                "processing_time": processing_time
            }

    except ToolInputError:
        raise
    except Exception as e:
        logger.error(f"Error visualizing memory network: {e}", exc_info=True)
        raise ToolError(f"Failed to visualize memory network: {str(e)}") from e

# ======================================================
# Exports
# ======================================================

__all__ = [
    # Initialization
    "initialize_memory_system",
    # Workflow
    "create_workflow", "update_workflow_status", "list_workflows", "get_workflow_details",
    # Actions
    "record_action_start", "record_action_completion", "get_recent_actions", "get_action_details", 
    # Action Dependency Tools
    "add_action_dependency", "get_action_dependencies",
    # Artifacts
    "record_artifact", "get_artifacts", "get_artifact_by_id",
    # Thoughts
    "record_thought", "create_thought_chain", "get_thought_chain",
    # Core Memory
    "store_memory", "get_memory_by_id", "create_memory_link", "search_semantic_memories", "query_memories", "hybrid_search_memories", "update_memory", "get_linked_memories", 
    # Context & State
    "get_working_memory", "focus_memory", "optimize_working_memory", "save_cognitive_state", "load_cognitive_state", "get_workflow_context",
    # Automated Cognitive Management
    "auto_update_focus", "promote_memory_level",
    # Meta-Cognition & Maintenance
    "consolidate_memories", "generate_reflection", "summarize_text", "delete_expired_memories", "compute_memory_statistics", "compute_memory_statistics",
    # Reporting & Visualization
    "generate_workflow_report", "visualize_reasoning_chain", "visualize_memory_network",
]


# Example Usage (for testing)
async def _example():
    db = "test_unified_memory.db"
    if os.path.exists(db):
        os.remove(db)

    init_result = await initialize_memory_system(db_path=db)
    print("Init Result:", init_result)

    wf_result = await create_workflow(title="Test Analysis Workflow", goal="Analyze test data", tags=["testing", "example"], db_path=db)
    wf_id = wf_result["workflow_id"]
    print("\nWorkflow Created:", wf_result)

    thought1 = await record_thought(workflow_id=wf_id, content="Need to load the data first.", thought_type="plan", db_path=db)
    print("\nThought Recorded:", thought1)

    action1_start = await record_action_start(workflow_id=wf_id, action_type="tool_use", reasoning="Load data from file.", tool_name="load_data", tool_args={"file": "data.csv"}, title="Load Data", tags=["io"], related_thought_id=thought1["thought_id"], db_path=db)
    action1_id = action1_start["action_id"]
    print("\nAction Started:", action1_start)

    # Simulate tool execution
    await asyncio.sleep(0.1)
    tool_output = {"rows_loaded": 100, "columns": ["A", "B"]}

    action1_end = await record_action_completion(action_id=action1_id, tool_result=tool_output, summary="Data loaded successfully.", db_path=db)
    print("\nAction Completed:", action1_end)

    artifact1 = await record_artifact(workflow_id=wf_id, action_id=action1_id, name="Loaded Data Sample", artifact_type="json", content=json.dumps(tool_output), description="Sample of loaded data structure", tags=["data"], db_path=db)
    print("\nArtifact Recorded:", artifact1)

    mem1 = await store_memory(workflow_id=wf_id, content="Column A seems to be numerical.", memory_type="observation", importance=6.0, action_id=action1_id, db_path=db)
    print("\nMemory Stored:", mem1)
    mem2 = await store_memory(workflow_id=wf_id, content="Column B looks categorical.", memory_type="observation", importance=6.0, action_id=action1_id, db_path=db)
    print("\nMemory Stored:", mem2)

    link1 = await create_memory_link(source_memory_id=mem1["memory_id"], target_memory_id=mem2["memory_id"], link_type="related", db_path=db)
    print("\nMemory Link Created:", link1)

    mem_get = await get_memory_by_id(memory_id=mem1["memory_id"], include_links=True, db_path=db)
    print("\nGet Memory By ID:", mem_get)

    # Close the connection on app shutdown
    await DBConnection.close_connection()

# if __name__ == "__main__":
#     asyncio.run(_example())
\end{minted}
\end{pageablecode}


\section{Appendix D: Agent Master Loop (AML) Code Listing}

% Add a non-floating caption for the code listing
\captionof{listing}{Complete Code for Agent Master Loop (agent\_master\_loop.py)}
\label{lst:aml-code} % Add a label

% Use the pageablecode environment
\begin{pageablecode}
\begin{minted}[
  escapeinside=||,
  fontsize=\scriptsize,
  breaklines=true,
  breakanywhere=true,
  breaksymbolleft={},
  breaksymbolright={},
  tabsize=2,
  autogobble
]{python}
"""
"""
EideticEngine Agent Master Loop (AML) - v4.1 P1 - GOAL STACK + MOMENTUM
======================================================================

This module implements the core orchestration logic for the EideticEngine
AI agent. It manages the primary think-act cycle, interacts with the
Unified Memory System (UMS) via MCPClient, leverages an LLM (Anthropic Claude)
for decision-making and planning, and incorporates several cognitive functions
inspired by human memory and reasoning.

** V4.1 P1 implements Phase 1 improvements: refined context, adaptive
thresholds, plan validation/repair, structured error handling, robust
background task management, AND adds explicit Goal Stack management and
a "Mental Momentum" bias. **

Key Functionalities:
--------------------
*   **Workflow & Context Management:**
    - Creates, manages, and tracks progress within structured workflows.
    - Supports sub-workflow execution via a workflow stack.
    - **Manages an explicit Goal Stack for hierarchical task decomposition.**
    - Gathers rich, multi-faceted context for the LLM decision-making process, including:
        *   **Current Goal Stack information.**
        *   Current working memory and focal points **(Prioritized)**.
        *   Core workflow context (recent actions, important memories, key thoughts).
        *   Proactively searched memories relevant to the current goal/plan step **(Limited Fetch)**.
        *   Relevant procedural memories (how-to knowledge) **(Limited Fetch)**.
        *   Summaries of memories linked to the current focus **(Limited Fetch)**.
        *   **Freshness indicators** for context components.
    - Implements structure-aware context truncation and optional LLM-based compression.

*   **Planning & Execution:**
    - Maintains an explicit, modifiable plan consisting of sequential steps with dependencies.
    - Allows the LLM to propose plan updates via a dedicated tool or text parsing.
    - Includes a heuristic fallback mechanism to update plan steps based on action outcomes if the LLM doesn't explicitly replan.
    - **Validates plan steps and detects dependency cycles.**
    - Checks action prerequisites (dependencies) before execution.
    - Executes tools via the MCPClient, handling server lookup and argument injection.
    - Records detailed action history (start, completion, arguments, results, dependencies).

*   **LLM Interaction & Reasoning:**
    - Constructs detailed prompts for the LLM, providing comprehensive context, tool schemas, and cognitive instructions.
    - **Prompts explicitly guide analysis of working memory, goal stack, and provide error recovery strategies.**
    - Parses LLM responses to identify tool calls, textual reasoning (recorded as thoughts), or goal completion signals.
    - Manages dedicated thought chains for recording the agent's reasoning process.

*   **Cognitive & Meta-Cognitive Processes:**
    - **Memory Interaction:** Stores, updates, searches (semantic/hybrid/keyword), and links memories in the UMS.
    - **Working Memory Management:** Retrieves, optimizes (based on relevance/diversity), and automatically focuses working memory via UMS tools.
    - **Goal Management:** Uses UMS tools to push new sub-goals onto the stack and mark goals as completed/failed.
    - **Background Cognitive Tasks:** Initiates asynchronous tasks **with timeouts and concurrency limits (semaphore)** for:
        *   Automatic semantic linking of newly created/updated memories.
        *   Checking and potentially promoting memories to higher cognitive levels (e.g., Episodic -> Semantic) based on usage/confidence.
    - **Periodic Meta-cognition:** Runs scheduled tasks based on loop intervals or success counters:
        *   **Reflection:** Generates analysis of progress, gaps, strengths, or plans using an LLM.
        *   **Consolidation:** Synthesizes information from multiple memories into summaries, insights, or procedures using an LLM.
        *   **Adaptive Thresholds:** Dynamically adjusts the frequency of reflection/consolidation based on agent performance (e.g., error rates, **memory statistics, trends, goal progress stability**) **with enhanced heuristics and dampening**. **Includes "Mental Momentum" bias.**
    - **Maintenance:** Periodically deletes expired memories.

*   **State & Error Handling:**
    - Persists the complete agent runtime state (workflow, **goal stack**, plan, counters, thresholds) atomically to a JSON file for resumption.
    - Implements retry logic with backoff for potentially transient tool failures (especially for idempotent operations).
    - Tracks consecutive errors and halts execution if a limit is reached.
    - Provides detailed, **categorized** error information back to the LLM for recovery attempts.
    - Handles graceful shutdown via system signals (SIGINT, SIGTERM).

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
"""
from __future__ import annotations

import asyncio
import copy
import dataclasses
import json
import logging
import math
import os
import random
import signal
import sys
import time
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import aiofiles
from anthropic import APIConnectionError, APIStatusError, AsyncAnthropic, RateLimitError  # Correct imports
from pydantic import BaseModel, Field, ValidationError

try:
    # Note: Import all potentially used enums/classes from MCPClient for clarity
    from mcp_client import (
        ActionStatus,
        ActionType,
        LinkType,
        MCPClient,
        MemoryLevel,
        MemoryType,
        MemoryUtils,
        ThoughtType,
        ToolError,
        ToolInputError,
        WorkflowStatus,
    )

    MCP_CLIENT_AVAILABLE = True
    log = logging.getLogger("AgentMasterLoop")
    # Bootstrap logger if MCPClient didn't configure it
    if not logging.root.handlers and not log.handlers:
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )
        log = logging.getLogger("AgentMasterLoop")
        log.warning("MCPClient did not configure logger ‚Äì falling back.")
    log.info("Successfully imported MCPClient and required components.")
except ImportError as import_err:
    # Critical error if MCPClient cannot be imported
    print(f"‚ùå CRITICAL ERROR: Could not import MCPClient: {import_err}")
    sys.exit(1)

# --------------------------------------------------------------------------
# Runtime log‚Äëlevel can be raised via AGENT_LOOP_LOG_LEVEL=DEBUG, etc.
# --------------------------------------------------------------------------
LOG_LEVEL_ENV = os.environ.get("AGENT_LOOP_LOG_LEVEL", "INFO").upper()
log.setLevel(getattr(logging, LOG_LEVEL_ENV, logging.INFO))
if log.level <= logging.DEBUG:
    log.info("Verbose logging enabled for Agent loop.")

# ==========================================================================
# CONSTANTS
# ==========================================================================
# File for saving/loading agent state, versioned for this implementation phase
AGENT_STATE_FILE = "agent_loop_state_v4.1_p1_goalstack_momentum.json" # Updated filename
# Agent identifier used in prompts and logging
AGENT_NAME = "EidenticEngine4.1-P1-GoalStackMomentum" # Updated agent name
# Default LLM model string (can be overridden by environment or config)
MASTER_LEVEL_AGENT_LLM_MODEL_STRING = "claude-3-7-sonnet-20250219" # Use the confirmed model

# ---------------- meta‚Äëcognition thresholds ----------------
# Base thresholds for triggering reflection and consolidation, adjustable via environment
BASE_REFLECTION_THRESHOLD = int(os.environ.get("BASE_REFLECTION_THRESHOLD", "7"))
BASE_CONSOLIDATION_THRESHOLD = int(os.environ.get("BASE_CONSOLIDATION_THRESHOLD", "12"))
# Minimum and maximum bounds for adaptive thresholds to prevent extreme values
MIN_REFLECTION_THRESHOLD = 3
MAX_REFLECTION_THRESHOLD = 15
MIN_CONSOLIDATION_THRESHOLD = 5
MAX_CONSOLIDATION_THRESHOLD = 25
# Dampening factor for threshold adjustments (e.g., 0.75 means apply 75% of calculated change)
THRESHOLD_ADAPTATION_DAMPENING = float(os.environ.get("THRESHOLD_DAMPENING", "0.75"))
# Positive bias added to thresholds during "Mental Momentum" (low errors)
MOMENTUM_THRESHOLD_BIAS_FACTOR = 1.2 # e.g., multiply base adjustment by this if momentum is high

# ---------------- interval constants (in loop iterations) ----------------
# How often to run working memory optimization and auto-focus checks
OPTIMIZATION_LOOP_INTERVAL = int(os.environ.get("OPTIMIZATION_INTERVAL", "8"))
# How often to check recently accessed memories for potential level promotion
MEMORY_PROMOTION_LOOP_INTERVAL = int(os.environ.get("PROMOTION_INTERVAL", "15"))
# How often to compute memory statistics and adapt thresholds
STATS_ADAPTATION_INTERVAL = int(os.environ.get("STATS_ADAPTATION_INTERVAL", "10"))
# How often to run maintenance tasks like deleting expired memories
MAINTENANCE_INTERVAL = int(os.environ.get("MAINTENANCE_INTERVAL", "50"))

# ---------------- context / token sizing ----------------
# Delay range (seconds) before running background auto-linking task
AUTO_LINKING_DELAY_SECS: Tuple[float, float] = (1.5, 3.0)
# Default description for the initial plan step if none exists
DEFAULT_PLAN_STEP = "Assess goal, gather context, formulate initial plan."

# Limits for various context components included in the prompt (PRE-FETCH LIMITS)
CONTEXT_RECENT_ACTIONS_FETCH_LIMIT = 10 # Fetch slightly more than shown
CONTEXT_IMPORTANT_MEMORIES_FETCH_LIMIT = 7
CONTEXT_KEY_THOUGHTS_FETCH_LIMIT = 7
CONTEXT_PROCEDURAL_MEMORIES_FETCH_LIMIT = 3 # Fetch limit for procedural
CONTEXT_PROACTIVE_MEMORIES_FETCH_LIMIT = 5 # Fetch limit for proactive goal-relevant
CONTEXT_LINK_TRAVERSAL_FETCH_LIMIT = 5 # Fetch limit for link traversal per direction
CONTEXT_GOAL_DETAILS_FETCH_LIMIT = 3 # How many parent goals to fetch details for in context

# Limits for items SHOWN in final prompt context (after potential truncation/summarization)
CONTEXT_RECENT_ACTIONS_SHOW_LIMIT = 7
CONTEXT_IMPORTANT_MEMORIES_SHOW_LIMIT = 5
CONTEXT_KEY_THOUGHTS_SHOW_LIMIT = 5
CONTEXT_PROCEDURAL_MEMORIES_SHOW_LIMIT = 2 # Limit procedural memories included
CONTEXT_PROACTIVE_MEMORIES_SHOW_LIMIT = 3 # Limit goal-relevant memories included
CONTEXT_WORKING_MEMORY_SHOW_LIMIT = 10 # Max working memory items shown in context
CONTEXT_LINK_TRAVERSAL_SHOW_LIMIT = 3 # Max links shown per direction in link summary
CONTEXT_GOAL_STACK_SHOW_LIMIT = 5 # Max goals shown from the stack in context

# Token limits triggering context compression
CONTEXT_MAX_TOKENS_COMPRESS_THRESHOLD = 15_000
CONTEXT_COMPRESSION_TARGET_TOKENS = 5_000 # Target size after compression

# Maximum number of consecutive tool execution errors before aborting
MAX_CONSECUTIVE_ERRORS = 3

# ---------------- unified‚Äëmemory tool constants ----------------
# Define constants for all UMS tool names for consistency and easy updates
TOOL_GET_WORKFLOW_DETAILS = "unified_memory:get_workflow_details"
TOOL_GET_CONTEXT = "unified_memory:get_workflow_context" # Core context retrieval tool
TOOL_CREATE_WORKFLOW = "unified_memory:create_workflow"
TOOL_UPDATE_WORKFLOW_STATUS = "unified_memory:update_workflow_status"
TOOL_RECORD_ACTION_START = "unified_memory:record_action_start"
TOOL_RECORD_ACTION_COMPLETION = "unified_memory:record_action_completion"
TOOL_GET_ACTION_DETAILS = "unified_memory:get_action_details"
TOOL_ADD_ACTION_DEPENDENCY = "unified_memory:add_action_dependency"
TOOL_GET_ACTION_DEPENDENCIES = "unified_memory:get_action_dependencies"
TOOL_RECORD_ARTIFACT = "unified_memory:record_artifact"
TOOL_GET_ARTIFACTS = "unified_memory:get_artifacts"
TOOL_GET_ARTIFACT_BY_ID = "unified_memory:get_artifact_by_id"
TOOL_HYBRID_SEARCH = "unified_memory:hybrid_search_memories"
TOOL_STORE_MEMORY = "unified_memory:store_memory"
TOOL_UPDATE_MEMORY = "unified_memory:update_memory"
TOOL_GET_WORKING_MEMORY = "unified_memory:get_working_memory"
TOOL_SEMANTIC_SEARCH = "unified_memory:search_semantic_memories"
TOOL_CREATE_THOUGHT_CHAIN = "unified_memory:create_thought_chain"
TOOL_GET_THOUGHT_CHAIN = "unified_memory:get_thought_chain"
TOOL_DELETE_EXPIRED_MEMORIES = "unified_memory:delete_expired_memories"
TOOL_COMPUTE_STATS = "unified_memory:compute_memory_statistics"
TOOL_RECORD_THOUGHT = "unified_memory:record_thought"
TOOL_REFLECTION = "unified_memory:generate_reflection"
TOOL_CONSOLIDATION = "unified_memory:consolidate_memories"
TOOL_OPTIMIZE_WM = "unified_memory:optimize_working_memory"
TOOL_AUTO_FOCUS = "unified_memory:auto_update_focus"
TOOL_PROMOTE_MEM = "unified_memory:promote_memory_level"
TOOL_QUERY_MEMORIES = "unified_memory:query_memories" # Keyword/filter-based search
TOOL_CREATE_LINK = "unified_memory:create_memory_link"
TOOL_GET_MEMORY_BY_ID = "unified_memory:get_memory_by_id"
TOOL_GET_LINKED_MEMORIES = "unified_memory:get_linked_memories"
TOOL_LIST_WORKFLOWS = "unified_memory:list_workflows"
TOOL_GENERATE_REPORT = "unified_memory:generate_workflow_report"
TOOL_SUMMARIZE_TEXT = "unified_memory:summarize_text"
# --- New UMS Tool Constants for Goal Stack ---
TOOL_PUSH_SUB_GOAL = "unified_memory:push_sub_goal" # Assumed UMS tool
TOOL_MARK_GOAL_STATUS = "unified_memory:mark_goal_status" # Assumed UMS tool
TOOL_GET_GOAL_DETAILS = "unified_memory:get_goal_details" # Assumed UMS tool
# --- Agent-internal tool name constant ---
AGENT_TOOL_UPDATE_PLAN = "agent:update_plan"

# --- Background Task Management ---
BACKGROUND_TASK_TIMEOUT_SECONDS = 60.0 # Timeout for individual background tasks
MAX_CONCURRENT_BG_TASKS = 10 # Limit concurrent background tasks (linking, promotion)

# ==========================================================================
# Utility helpers
# ==========================================================================

def _fmt_id(val: Any, length: int = 8) -> str:
    """Return a short id string safe for logs."""
    s = str(val) if val is not None else "?"
    return s[:length] if len(s) >= length else s


def _utf8_safe_slice(s: str, max_len: int) -> str:
    """Return a UTF‚Äë8 boundary‚Äësafe slice ‚â§ max_len bytes."""
    # Ensure input is a string
    if not isinstance(s, str):
        s = str(s)
    return s.encode("utf‚Äë8")[:max_len].decode("utf‚Äë8", "ignore")


def _truncate_context(context: Dict[str, Any], max_len: int = 25_000) -> str:
    """
    Structure‚Äëaware context truncation with UTF‚Äë8 safe fallback.

    Attempts to intelligently reduce context size while preserving structure
    and indicating where truncation occurred. Honors CONTEXT_*_SHOW_LIMIT constants.

    1. Serialise full context ‚Äì if within limit, return.
    2. Iteratively truncate known large lists (e.g., recent actions, memories, goal stack)
       based on SHOW_LIMIT constants, adding a note about omissions.
    3. Remove lowest‚Äëpriority top‚Äëlevel keys (e.g., procedures, links)
       until size fits.
    4. Final fallback: utf‚Äë8 safe byte slice of the full JSON, attempting to
       make it syntactically valid-ish and adding a clear truncation marker.
    """
    try:
        # Serialize the context to JSON. Use default=str for non-serializable types like datetime.
        full = json.dumps(context, indent=2, default=str, ensure_ascii=False)
    except TypeError:
        # Handle potential non-serializable types even before dumping if necessary
        context = json.loads(json.dumps(context, default=str)) # Pre-process
        full = json.dumps(context, indent=2, default=str, ensure_ascii=False)

    # If already within the limit, return the full JSON string
    if len(full) <= max_len:
        return full

    # Log that truncation is starting
    log.debug(f"Context length {len(full)} exceeds max {max_len}. Applying structured truncation.")
    ctx_copy = copy.deepcopy(context) # Work on a copy
    ctx_copy["_truncation_applied"] = "structure‚Äëaware" # Add metadata about truncation
    original_length = len(full)

    # Define paths to lists that can be truncated and the number of items to keep (using SHOW_LIMIT constants)
    list_paths_to_truncate = [ # (parent_key or None, key_of_list, items_to_keep_constant)
        ("core_context", "recent_actions", CONTEXT_RECENT_ACTIONS_SHOW_LIMIT),
        ("core_context", "important_memories", CONTEXT_IMPORTANT_MEMORIES_SHOW_LIMIT),
        ("core_context", "key_thoughts", CONTEXT_KEY_THOUGHTS_SHOW_LIMIT),
        (None, "proactive_memories", CONTEXT_PROACTIVE_MEMORIES_SHOW_LIMIT),
        # Apply limit to the list *within* the working memory dict
        ("current_working_memory", "working_memories", CONTEXT_WORKING_MEMORY_SHOW_LIMIT),
        (None, "relevant_procedures", CONTEXT_PROCEDURAL_MEMORIES_SHOW_LIMIT),
        # --- Add goal stack truncation ---
        ("current_goal_context", "goal_stack_summary", CONTEXT_GOAL_STACK_SHOW_LIMIT),
    ]
    # Define keys to remove entirely if context is still too large, in order of least importance
    keys_to_remove_low_priority = [
        "relevant_procedures",
        "proactive_memories",
        "contextual_links", # Link summary is lower priority than core items
        # Removing items from core_context is more impactful
        ("core_context", "key_thoughts"), # Check nested key first
        ("core_context", "important_memories"),
        ("core_context", "recent_actions"),
        "core_context", # Remove the entire core context dict last
        "current_working_memory", # Remove working memory context before byte slice
        "current_goal_context", # Remove goal context before byte slice (lower priority than plan?)
    ]

    # 1. Truncate specified lists based on SHOW_LIMIT constants
    for parent, key, keep_count in list_paths_to_truncate:
        try:
            container = ctx_copy
            # Navigate to the parent dictionary if specified
            if parent:
                if parent not in container or not isinstance(container[parent], dict):
                    continue # Parent doesn't exist or isn't a dict
                container = container[parent]

            # Check if the key exists, is a list, and needs truncation
            if key in container and isinstance(container[key], list) and len(container[key]) > keep_count:
                original_count = len(container[key])
                # Add a note indicating truncation within the list itself
                note = {"truncated_note": f"{original_count - keep_count} items omitted from '{key}'"}
                # Slice to keep the desired number of items
                container[key] = container[key][:keep_count]
                # Append the note if there's space or if it's crucial context
                if keep_count > 0: # Only add note if we kept some items
                    container[key].append(note)
                log.debug(f"Truncated list '{key}' (under '{parent or 'root'}') to {keep_count} items (+ note).")

            # Check size after each list truncation
            serial = json.dumps(ctx_copy, indent=2, default=str, ensure_ascii=False)
            if len(serial) <= max_len:
                log.info(f"Context truncated successfully after list reduction (Length: {len(serial)}).")
                return serial
        except (KeyError, TypeError, IndexError) as e:
            # Log errors during this specific truncation but continue with others
            log.warning(f"Error during list truncation for key '{key}' (under '{parent}'): {e}")
            continue

    # 2. Remove low-priority keys if still too large
    for key_info in keys_to_remove_low_priority:
        removed = False
        key_to_remove = key_info
        parent_key_to_remove = None
        if isinstance(key_info, tuple): # Handle nested keys like ("core_context", "key_thoughts")
            parent_key_to_remove, key_to_remove = key_info

        # Try removing from the parent or root level
        container_to_remove = ctx_copy
        if parent_key_to_remove:
            if parent_key_to_remove in container_to_remove and isinstance(container_to_remove[parent_key_to_remove], dict):
                container_to_remove = container_to_remove[parent_key_to_remove]
            else:
                continue # Skip if parent doesn't exist or isn't a dict

        # Remove the key if it exists in the target container
        if key_to_remove in container_to_remove:
            container_to_remove.pop(key_to_remove)
            removed = True
            log.debug(f"Removed low-priority key '{key_to_remove}' from {'root' if parent_key_to_remove is None else parent_key_to_remove} for truncation.")

        # Check size after each key removal
        if removed:
            serial = json.dumps(ctx_copy, indent=2, default=str, ensure_ascii=False)
            if len(serial) <= max_len:
                log.info(f"Context truncated successfully after key removal (Length: {len(serial)}).")
                return serial

    # 3. Ultimate fallback: UTF-8 safe byte slice
    log.warning(f"Structured truncation insufficient (Length still {len(serial)}). Applying final byte-slice.")
    # Slice the original full JSON string, leaving some buffer for the truncation note/closing chars
    clipped_json_str = _utf8_safe_slice(full, max_len - 50)
    # Attempt to make it look somewhat like valid JSON by finding the last closing brace/bracket
    try:
        last_brace = clipped_json_str.rfind('}')
        last_bracket = clipped_json_str.rfind(']')
        cutoff = max(last_brace, last_bracket)
        if cutoff > 0:
            # Truncate after the last complete element and add a note + closing brace
            final_str = clipped_json_str[:cutoff+1] + '\n// ... (CONTEXT TRUNCATED BY BYTE LIMIT) ...\n}'
        else:
            # If no closing elements found, just add a basic note
            final_str = clipped_json_str + '... (CONTEXT TRUNCATED)'
    except Exception:
        # Fallback if string manipulation fails
        final_str = clipped_json_str + '... (CONTEXT TRUNCATED)'

    log.error(f"Context severely truncated from {original_length} to {len(final_str)} bytes using fallback.")
    return final_str

# ==========================================================================
# Dataclass & pydantic models
# ==========================================================================

class PlanStep(BaseModel):
    """Represents a single step in the agent's plan."""
    id: str = Field(default_factory=lambda: f"step-{MemoryUtils.generate_id()[:8]}", description="Unique identifier for the plan step.")
    description: str = Field(..., description="Clear, concise description of the action or goal for this step.")
    status: str = Field(
        default="planned",
        description="Current status of the step: planned, in_progress, completed, failed, skipped.",
    )
    depends_on: List[str] = Field(default_factory=list, description="List of other plan step IDs that must be completed before this step can start.")
    assigned_tool: Optional[str] = Field(default=None, description="Specific tool designated for executing this step, if applicable.")
    tool_args: Optional[Dict[str, Any]] = Field(default=None, description="Arguments to be passed to the assigned tool.")
    result_summary: Optional[str] = Field(default=None, description="A brief summary of the outcome after the step is completed or failed.")
    is_parallel_group: Optional[str] = Field(default=None, description="Optional tag to group steps that can potentially run in parallel.")


def _default_tool_stats() -> Dict[str, Dict[str, Union[int, float]]]:
    """Factory function for initializing tool usage statistics dictionary."""
    # Use defaultdict for convenience: accessing a non-existent tool key will create its default stats entry
    return defaultdict(lambda: {"success": 0, "failure": 0, "latency_ms_total": 0.0})


@dataclass
class AgentState:
    """
    Represents the complete persisted runtime state of the Agent Master Loop.

    This dataclass holds all information necessary to resume the agent's operation,
    including workflow context, **goal hierarchy**, planning state, error tracking,
    meta-cognition metrics, and adaptive thresholds.

    Attributes:
        workflow_id: The ID of the primary workflow the agent is currently focused on.
        context_id: The specific context ID for memory operations (often matches workflow_id).
        workflow_stack: A list maintaining the hierarchy of active workflows (e.g., for sub-workflows).
        goal_stack: List of goal dictionaries tracking hierarchical goals. **(NEW)**
        current_goal_id: ID of the goal at the top of the `goal_stack` (the agent's current focus). **(RENAMED/REPURPOSED)**
        current_plan: A list of `PlanStep` objects representing the agent's current plan.
        current_thought_chain_id: ID of the active thought chain for recording reasoning.
        last_action_summary: A brief string summarizing the outcome of the last action taken.
        current_loop: The current iteration number of the main agent loop.
        goal_achieved_flag: Boolean flag indicating if the *overall root goal* has been marked as achieved.
        consecutive_error_count: Counter for consecutive failed actions, used for error limiting.
        needs_replan: Boolean flag indicating if the agent needs to revise its plan in the next cycle.
        last_error_details: A dictionary holding structured information about the last error encountered **(Enhanced with category)**.
        successful_actions_since_reflection: Counter for successful *agent-level* actions since the last reflection.
        successful_actions_since_consolidation: Counter for successful *agent-level* actions since the last consolidation.
        loops_since_optimization: Counter for loops since the last working memory optimization/focus update.
        loops_since_promotion_check: Counter for loops since the last memory promotion check cycle.
        loops_since_stats_adaptation: Counter for loops since the last statistics check and threshold adaptation.
        loops_since_maintenance: Counter for loops since the last maintenance task (e.g., deleting expired memories).
        reflection_cycle_index: Index to cycle through different reflection types.
        last_meta_feedback: Stores the summary of the last reflection/consolidation output for the next prompt.
        current_reflection_threshold: The current dynamic threshold for triggering reflection **(Adaptive + Momentum)**.
        current_consolidation_threshold: The current dynamic threshold for triggering consolidation **(Adaptive + Momentum)**.
        tool_usage_stats: Dictionary tracking success/failure counts and latency for each tool used.
        background_tasks: (Transient) Set holding currently running asyncio background tasks (not saved to state file).
    """

    # --- workflow stack ---
    workflow_id: Optional[str] = None
    context_id: Optional[str] = None
    workflow_stack: List[str] = field(default_factory=list)

    # --- Goal Management --- (NEW/MODIFIED)
    goal_stack: List[Dict[str, Any]] = field(default_factory=list) # Stores {goal_id, description, status, ...}
    current_goal_id: Optional[str] = None # ID of the active goal (top of the stack)

    # --- planning & reasoning ---
    current_plan: List[PlanStep] = field(
        default_factory=lambda: [PlanStep(description=DEFAULT_PLAN_STEP)]
    )
    current_thought_chain_id: Optional[str] = None # Tracks the current reasoning thread
    last_action_summary: str = "Loop initialized."
    current_loop: int = 0
    goal_achieved_flag: bool = False # Flag to signal loop termination (based on root goal)

    # --- error/replan ---
    consecutive_error_count: int = 0
    needs_replan: bool = False # Flag to force replanning cycle
    last_error_details: Optional[Dict[str, Any]] = None # Stores info about the last error **(Enhanced)**

    # --- meta‚Äëcognition metrics ---
    # Counters reset when corresponding meta-task runs or on error
    successful_actions_since_reflection: float = 0.0 # Use float for potential fractional counting
    successful_actions_since_consolidation: float = 0.0
    # Loop counters reset when corresponding periodic task runs
    loops_since_optimization: int = 0
    loops_since_promotion_check: int = 0
    loops_since_stats_adaptation: int = 0
    loops_since_maintenance: int = 0
    reflection_cycle_index: int = 0 # Used to cycle through reflection types
    last_meta_feedback: Optional[str] = None # Feedback from last meta-task for next prompt

    # adaptive thresholds (dynamic) - Initialized from constants, adapted based on stats and momentum
    current_reflection_threshold: int = BASE_REFLECTION_THRESHOLD
    current_consolidation_threshold: int = BASE_CONSOLIDATION_THRESHOLD

    # tool statistics - tracks usage counts and latency
    tool_usage_stats: Dict[str, Dict[str, Union[int, float]]] = field(
        default_factory=_default_tool_stats
    )

    # background tasks (transient) - Not saved/loaded, managed at runtime
    background_tasks: Set[asyncio.Task] = field(
        default_factory=set, init=False, repr=False
    )

# =====================================================================
# Agent Master Loop
# =====================================================================
class AgentMasterLoop:
    """
    Agent Master Loop Orchestrator.

    This class orchestrates the primary think-act cycle of the AI agent.
    It manages state, interacts with the Unified Memory System via MCPClient,
    calls the LLM for decision-making, handles plan execution, manages goal hierarchies,
    and runs periodic meta-cognitive tasks (reflection, consolidation, optimization).
    This version integrates rich context gathering, goal stack management, mental momentum bias,
    and detailed prompting as defined in Phase 1 of the v4.1 plan.
    """

    # Set of tool names considered internal or meta-cognitive,
    # which typically shouldn't be recorded as primary agent actions.
    _INTERNAL_OR_META_TOOLS: Set[str] = {
        # Action recording itself is meta
        TOOL_RECORD_ACTION_START,
        TOOL_RECORD_ACTION_COMPLETION,
        # Information retrieval is usually part of the agent's thought process, not a world-altering action
        TOOL_GET_CONTEXT,
        TOOL_GET_WORKING_MEMORY,
        TOOL_SEMANTIC_SEARCH,
        TOOL_HYBRID_SEARCH,
        TOOL_QUERY_MEMORIES,
        TOOL_GET_MEMORY_BY_ID,
        TOOL_GET_LINKED_MEMORIES,
        TOOL_GET_ACTION_DETAILS,
        TOOL_GET_ARTIFACTS,
        TOOL_GET_ARTIFACT_BY_ID,
        TOOL_GET_ACTION_DEPENDENCIES,
        TOOL_GET_THOUGHT_CHAIN,
        TOOL_GET_WORKFLOW_DETAILS, # Getting details is informational
        TOOL_GET_GOAL_DETAILS, # Getting goal details is informational (NEW)
        # Managing relationships is meta
        TOOL_ADD_ACTION_DEPENDENCY,
        TOOL_CREATE_LINK,
        # Goal stack management is meta
        TOOL_PUSH_SUB_GOAL, # (NEW)
        TOOL_MARK_GOAL_STATUS, # (NEW)
        # Admin/Utility tasks are not primary actions
        TOOL_LIST_WORKFLOWS,
        TOOL_COMPUTE_STATS,
        TOOL_SUMMARIZE_TEXT, # Summarization is a utility
        # Periodic cognitive maintenance and enhancement tasks
        TOOL_OPTIMIZE_WM,
        TOOL_AUTO_FOCUS,
        TOOL_PROMOTE_MEM,
        TOOL_REFLECTION,
        TOOL_CONSOLIDATION,
        TOOL_DELETE_EXPIRED_MEMORIES,
        # Agent's internal mechanism for plan updates
        AGENT_TOOL_UPDATE_PLAN,
    }

    # --------------------------------------------------------------- ctor --
    def __init__(
        self, mcp_client_instance: MCPClient, agent_state_file: str = AGENT_STATE_FILE
    ):
        """
        Initializes the AgentMasterLoop.

        Args:
            mcp_client_instance: An initialized instance of the MCPClient.
            agent_state_file: Path to the file for saving/loading agent state.
        """
        # Ensure MCPClient dependency is met
        if not MCP_CLIENT_AVAILABLE:
            raise RuntimeError("MCPClient unavailable. Cannot initialize AgentMasterLoop.")

        self.mcp_client = mcp_client_instance
        # Ensure the Anthropic client is available via MCPClient
        if not hasattr(mcp_client_instance, 'anthropic') or not isinstance(mcp_client_instance.anthropic, AsyncAnthropic):
             self.logger.critical("Anthropic client not found within provided MCPClient instance.")
             raise ValueError("Anthropic client required via MCPClient.")
        self.anthropic_client: AsyncAnthropic = self.mcp_client.anthropic # type: ignore
        self.logger = log
        self.agent_state_file = Path(agent_state_file)

        # Configuration parameters for cognitive processes
        self.consolidation_memory_level = MemoryLevel.EPISODIC.value
        self.consolidation_max_sources = 10 # Max memories to feed into consolidation
        self.auto_linking_threshold = 0.7 # Similarity threshold for auto-linking
        self.auto_linking_max_links = 3 # Max links to create per auto-link trigger

        # Sequence of reflection types to cycle through
        self.reflection_type_sequence = [
            "summary", "progress", "gaps", "strengths", "plan",
        ]

        # Initialize agent state (will be overwritten by load if file exists)
        self.state = AgentState()
        # Event to signal graceful shutdown
        self._shutdown_event = asyncio.Event()
        # Lock for safely managing the background tasks set
        self._bg_tasks_lock = asyncio.Lock()
        # Semaphore to limit concurrent background tasks
        self._bg_task_semaphore = asyncio.Semaphore(MAX_CONCURRENT_BG_TASKS)
        # Placeholder for loaded tool schemas
        self.tool_schemas: List[Dict[str, Any]] = []

    # ----------------------------------------------------------- shutdown --
    async def shutdown(self) -> None:
        """
        Initiates graceful shutdown of the agent loop.

        Sets the shutdown event, cancels pending background tasks,
        and saves the final agent state.
        """
        self.logger.info("Shutdown requested.")
        self._shutdown_event.set() # Signal loops and tasks to stop
        await self._cleanup_background_tasks() # Wait for background tasks
        await self._save_agent_state() # Save final state
        self.logger.info("Agent loop shutdown complete.")

    # ----------------------------------------------------------- prompt --
    def _construct_agent_prompt(
        self, goal: str, context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Builds the system and user prompts for the agent LLM.

        Integrates the overall goal, available tools with schemas, detailed
        process instructions, cognitive guidance, and the current runtime context
        (plan, errors, feedback, **goal stack**, memories, etc.) into the prompt structure
        expected by the Anthropic API. Includes robust context truncation.
        **Enhanced with goal stack guidance, mental momentum bias, and error recovery strategies.**
        """
        # <<< Start Integration Block: Enhance _construct_agent_prompt (Goal Stack + Momentum) >>>
        # ---------- system ----------
        # Initialize the list for system prompt blocks
        system_blocks: List[str] = [
            f"You are '{AGENT_NAME}', an AI agent orchestrator using a Unified Memory System.",
            "",
            # --- GOAL CONTEXT (Logic moved outside list literal) ---
            f"Overall Goal: {goal}", # This remains the root goal
        ]

        # Extract current goal details from context and append the appropriate string
        current_goal_info = context.get("current_goal_context", {}).get("current_goal")
        if current_goal_info:
            system_blocks.append(f"Current Goal: {current_goal_info.get('description', 'N/A')} (ID: {_fmt_id(current_goal_info.get('goal_id'))}, Status: {current_goal_info.get('status', 'N/A')})")
        else:
            system_blocks.append("Current Goal: None specified (Focus on Overall Goal or plan step)")

        # Add the separator and the next section header
        system_blocks.append("") # Separator
        system_blocks.append("Available Unified Memory & Agent Tools (Use ONLY these):")

        # --- Continue appending other system blocks as before ---
        if not self.tool_schemas:
            system_blocks.append("- CRITICAL WARNING: No tools loaded. Cannot function.")
        else:
            # Define key tools to highlight in the prompt (adding Goal tools)
            essential_cognitive_tools = {
                TOOL_ADD_ACTION_DEPENDENCY, TOOL_RECORD_ARTIFACT, TOOL_HYBRID_SEARCH,
                TOOL_STORE_MEMORY, TOOL_UPDATE_MEMORY, TOOL_CREATE_LINK,
                TOOL_CREATE_THOUGHT_CHAIN, TOOL_GET_THOUGHT_CHAIN, TOOL_RECORD_THOUGHT,
                TOOL_REFLECTION, TOOL_CONSOLIDATION, TOOL_PROMOTE_MEM,
                TOOL_OPTIMIZE_WM, TOOL_AUTO_FOCUS, TOOL_GET_WORKING_MEMORY,
                TOOL_QUERY_MEMORIES, TOOL_SEMANTIC_SEARCH,
                AGENT_TOOL_UPDATE_PLAN, # Also highlight the agent's own tool
                # --- Goal Stack Tools ---
                TOOL_PUSH_SUB_GOAL, TOOL_MARK_GOAL_STATUS, TOOL_GET_GOAL_DETAILS,
            }
            # Format each tool schema for the prompt
            for schema in self.tool_schemas:
                sanitized = schema["name"]
                # Ensure mapping exists before using it
                original = self.mcp_client.server_manager.sanitized_to_original.get(sanitized, sanitized)
                desc = schema.get("description", "No description.")
                is_essential = original in essential_cognitive_tools
                prefix = "**" if is_essential else "" # Highlight key tools
                # Provide tool name, mapping, description, and schema
                # Handle different schema formats ('input_schema' vs 'parameters')
                input_schema_str = json.dumps(schema.get('input_schema', schema.get('parameters', {})))
                system_blocks.append(
                    f"\n- {prefix}Name: `{sanitized}` (Represents: `{original}`){prefix}\n"
                    f"  Desc: {desc}\n"
                    f"  Schema: {input_schema_str}"
                )
        system_blocks.append("")
        # --- Detailed Process Instructions (Enhanced for Goals & Momentum) ---
        system_blocks.extend([
            "Your Process at each step:",
            "1.  Context Analysis: Deeply analyze 'Current Context'. Note workflow status, errors (`last_error_details` - *pay attention to error `type`*), **goal stack (`current_goal_context` -> `goal_stack_summary`) and the `current_goal`**, recent actions, memories (`core_context`, `proactive_memories`), thoughts, `current_plan`, `relevant_procedures`, **`current_working_memory` (use this for immediate relevance, note `focal_memory_id` if present)**, `current_thought_chain_id`, and `meta_feedback`. Pay attention to memory `importance`/`confidence` and context component `retrieved_at` timestamps.",
            "2.  Error Handling: If `last_error_details` exists, **FIRST** reason about the error `type` and `message`. Propose a recovery strategy in your reasoning. Refer to 'Recovery Strategies' below.",
            "3.  Reasoning & Planning:",
            "    a. State step-by-step reasoning towards the **Current Goal**, integrating context and feedback. Consider `current_working_memory` for immediate context. Record key thoughts using `record_thought` and specify the `thought_chain_id` if different from `current_thought_chain_id`.",
            "    b. Evaluate `current_plan`. Is it aligned with the **Current Goal**? Is it valid? Does it address errors? Are dependencies (`depends_on`) likely met? Check for cycles.",
            "    c. **Goal Management:** If the Current Goal is too complex, break it down by using `push_sub_goal` with a clear `description`. When a sub-goal is achieved or fails, use `mark_goal_status` with the `goal_id` and appropriate `status` ('completed' or 'failed').", # Goal instructions
            "    d. **Action Dependencies:** If planning Step B requires output from Step A (action ID 'a123'), include `\"depends_on\": [\"a123\"]` in Step B's plan object.",
            "    e. **Artifact Tracking:** If planning to use a tool that creates a file/data, plan a subsequent step to call `record_artifact`. If needing a previously created artifact, plan to use `get_artifacts` or `get_artifact_by_id` first.",
            "    f. **Direct Memory Management:** If you synthesize a critical new fact, insight, or procedure, plan to use `store_memory` to explicitly save it. If you find strong evidence contradicting a stored memory, plan to use `update_memory` to correct it. Provide clear `content`, `memory_type`, `importance`, and `confidence`.",
            "    g. **Custom Thought Chains:** If tackling a distinct sub-problem or exploring a complex tangent, consider creating a new reasoning thread using `create_thought_chain`. Provide a clear `title`. Subsequent related thoughts should specify the new `thought_chain_id`.",
            "    h. **Linking:** Identify potential memory relationships (causal, supportive, contradictory). Plan to use `create_memory_link` with specific `link_type`s (e.g., `SUPPORTS`, `CONTRADICTS`, `CAUSAL`, `REFERENCES`).",
            "    i. **Search:** Prefer `hybrid_search_memories` for mixed queries needing relevance and keyword matching. Use `search_semantic_memories` for pure conceptual similarity.",
            "    j. **Update Plan Tool / Repair:** Use `agent:update_plan` ONLY for significant changes (error recovery, major strategy shift, multi-step insertion/removal, fixing validation issues like cycles). **If `needs_replan` is true in context, prioritize using this tool to fix the plan.** Do NOT use it for simply marking a step complete.",
            "4.  Action Decision: Choose **ONE** action based on the *first planned step* in your current plan:",
            "    *   Call Memory/Other Tool: Select the most precise tool (including goal tools like `push_sub_goal` or `mark_goal_status`). Provide args per schema. **Mandatory:** Call `create_workflow` if context shows 'No Active Workflow'.", # Added goal tools example
            "    *   Record Thought: Use `record_thought` for logging reasoning, questions, hypotheses etc. Specify `thought_chain_id` if not the current one.",
            "    *   Update Plan Tool: Call `agent:update_plan` with the **complete, repaired** plan structure if major changes are needed or validation failed.",
            "    *   Signal Completion: If the **Current Goal** is MET (use `mark_goal_status`) OR the Overall Goal is MET (respond ONLY with the text \"Goal Achieved:\" followed by a brief summary).", # Clarified goal completion
            "5.  Output Format: Respond **ONLY** with the valid JSON for the chosen tool call OR the \"Goal Achieved:\" text."
        ])
        # --- Key Considerations (Enhanced for Goals & Momentum) ---
        system_blocks.extend([
            "\nKey Considerations:",
            "*   Goal Focus: Always work towards the **Current Goal** (top of the stack). Use `push_sub_goal` and `mark_goal_status` to manage the hierarchy.",
            "*   **Mental Momentum:** Prioritize completing the current plan steps if progress is steady. Justify significant deviations or replanning efforts.", # Momentum instruction
            "*   Dependencies & Cycles: Ensure `depends_on` actions are likely complete. Avoid circular dependencies. Use `get_action_details` if unsure.",
            "*   Artifacts: Track outputs (`record_artifact`), retrieve inputs (`get_artifacts`/`get_artifact_by_id`).",
            "*   Memory: Store important learned info (`store_memory`). Update incorrect info (`update_memory`). Use confidence scores.",
            "*   Thought Chains: Use `create_thought_chain` for complex sub-problems. Use the correct `thought_chain_id` when recording thoughts.",
            "*   Linking: Use specific `link_type`s to build the knowledge graph.",
            "*   Focus: Leverage `current_working_memory` for immediate context. Note the `focal_memory_id`.",
            "*   Errors: Prioritize error analysis and recovery based on `last_error_details.type`."
        ])
        # --- Recovery Strategies Guidance ---
        system_blocks.extend([
            "\nRecovery Strategies based on `last_error_details.type`:",
            "*   `InvalidInputError`: Review tool schema, arguments, and context. Correct the arguments and retry OR choose a different tool/step.",
            "*   `DependencyNotMetError`: Use `get_action_details` on dependency IDs to check status. Adjust plan order using `agent:update_plan` or wait.",
            "*   `ServerUnavailable` / `NetworkError`: The tool's server might be down. Try a different tool, wait, or adjust the plan.",
            "*   `APILimitError` / `RateLimitError`: The external API (e.g., LLM) is busy. Plan to wait (record a thought) before retrying the step.",
            "*   `ToolExecutionError` / `ToolInternalError`: The tool failed internally. Analyze the error message. Maybe try different arguments, use an alternative tool, or adjust the plan.",
            "*   `PlanUpdateError`: The plan structure you proposed was invalid. Re-examine the plan steps and dependencies, then try `agent:update_plan` again with a valid list.",
            "*   `PlanValidationError`: The proposed plan has logical issues (e.g., cycles). Debug dependencies and propose a corrected plan structure using `agent:update_plan`.",
            "*   `CancelledError`: The previous action was cancelled. Re-evaluate the current step.",
            "*   `GoalManagementError`: An error occurred managing the goal stack (e.g., trying to mark a non-existent goal). Review `current_goal_context` and the goal stack logic.", # Added Goal error
            "*   `UnknownError` / `UnexpectedExecutionError`: Analyze the error message carefully. Try to understand the cause. You might need to simplify the step, use a different approach, or ask for clarification via `record_thought` if stuck."
        ])
        system_prompt = "\n".join(system_blocks)
        # <<< End Integration Block: Enhance _construct_agent_prompt >>>

        # ---------- user ----------
        # Construct the user part of the prompt, including truncated context
        context_json = _truncate_context(context) # Apply robust truncation
        user_blocks = [
            "Current Context:",
            "```json",
            context_json,
            "```",
            "",
            "Current Plan:",
            "```json",
            # Serialize current plan steps (ensure model_dump handles exclude_none)
            json.dumps(
                [step.model_dump(exclude_none=True) for step in self.state.current_plan],
                indent=2,
                ensure_ascii=False,
            ),
            "```",
            "",
            # Include summary of the last action taken
            f"Last Action Summary:\n{self.state.last_action_summary}\n",
        ]
        # If there was an error in the previous step, include details prominently
        if self.state.last_error_details:
            user_blocks += [
                "**CRITICAL: Address Last Error Details**:", # Highlight error
                "```json",
                # Use default=str for safe serialization of potential complex error objects
                json.dumps(self.state.last_error_details, indent=2, default=str),
                "```",
                "",
            ]
        # If there's feedback from meta-cognitive tasks, include it
        if self.state.last_meta_feedback:
            user_blocks += [
                "**Meta-Cognitive Feedback**:", # Highlight feedback
                self.state.last_meta_feedback,
                "",
            ]
        # Reiterate the overall goal and the final instruction
        user_blocks += [
            # Reiterate current goal from context for emphasis
            f"Current Goal Reminder: {context.get('current_goal_context', {}).get('current_goal', {}).get('description', 'Overall Goal')}",
            "",
            # Updated instruction emphasizing error/plan repair AND goal management
            "Instruction: Analyze context & errors (use recovery strategies if needed). Reason step-by-step towards the Current Goal. Evaluate and **REPAIR** the plan if `needs_replan` is true or errors indicate plan issues (use `agent:update_plan`). Manage goals using `push_sub_goal` or `mark_goal_status` if needed. Otherwise, decide ONE action based on the *first planned step*: call a tool (output tool_use JSON), record a thought (`record_thought`), or signal completion (use `mark_goal_status` for sub-goals or output 'Goal Achieved: ...' for overall goal).",
        ]
        user_prompt = "\n".join(user_blocks)

        # Return structure for Anthropic API (user prompt combines system instructions and current state)
        # Note: Anthropic recommends placing system prompts outside the 'messages' list if using their client directly.
        # Here, we combine them into the user message content as per the original structure.
        # >>>>> PRESERVED ORIGINAL PROMPT STRUCTURE <<<<<
        return [{"role": "user", "content": system_prompt + "\n---\n" + user_prompt}]


    # ---------------------------------------------------------- bg‚Äëtask utils --
    def _background_task_done(self, task: asyncio.Task) -> None:
        """Callback attached to background tasks upon completion."""
        # Schedule the safe cleanup coroutine to avoid blocking the callback
        asyncio.create_task(self._background_task_done_safe(task))

    async def _background_task_done_safe(self, task: asyncio.Task) -> None:
        """
        Safely removes a completed task from the tracking set, releases the semaphore,
        and logs any exceptions. Ensures thread-safety using an asyncio Lock.
        """
        # <<< Start Integration Block: Enhance _background_task_done_safe (Phase 1, Step 5) >>>
        was_present = False
        async with self._bg_tasks_lock: # Acquire lock before modifying the set
            if task in self.state.background_tasks:
                 self.state.background_tasks.discard(task)
                 was_present = True

        # Release the semaphore ONLY if the task was successfully removed from the set
        # This prevents releasing the semaphore multiple times if the callback somehow fires twice
        if was_present:
            try:
                self._bg_task_semaphore.release()
                log.debug(f"Released semaphore. Count: {self._bg_task_semaphore._value}. Task: {task.get_name()}")
            except ValueError:
                 # This can happen if release is called more times than acquire (should not normally occur)
                 log.warning(f"Semaphore release attempt failed for task {task.get_name()}. Already fully released?")
            except Exception as sem_err:
                 log.error(f"Unexpected error releasing semaphore for task {task.get_name()}: {sem_err}")

        # Log cancellation or exceptions after releasing the lock and semaphore
        if task.cancelled():
            self.logger.debug(f"Background task {task.get_name()} was cancelled.")
            return
        # Check if the task encountered an exception
        exc = task.exception()
        if exc:
            # Log the exception details
            self.logger.error(
                # Provide task name/info for better debugging
                f"Background task {task.get_name()} failed: {type(exc).__name__}",
                exc_info=(type(exc), exc, exc.__traceback__), # Provide full traceback info
            )
        # <<< End Integration Block: Enhance _background_task_done_safe >>>

    def _start_background_task(self, coro_fn, *args, **kwargs) -> asyncio.Task:
        """
        Creates and starts an asyncio task for a background operation.

        Acquires a semaphore slot before starting. Includes timeout handling.
        Captures essential state (workflow_id, context_id) at the time of creation
        to ensure the background task operates on the correct context, even if the
        main agent state changes before the task runs. Adds the task to the
        tracking set for cleanup.

        Args:
            coro_fn: The async function (coroutine) to run in the background.
                     Must accept `self` as the first argument if it's an instance method.
            *args: Positional arguments to pass to `coro_fn`.
            **kwargs: Keyword arguments to pass to `coro_fn`.

        Returns:
            The created asyncio.Task object.
        """
        # <<< Start Integration Block: Enhance _start_background_task (Phase 1, Step 5) >>>
        # Snapshot critical state needed by the background task at the moment of creation
        snapshot_wf_id = self.state.workflow_id
        snapshot_ctx_id = self.state.context_id
        # Add other state variables here if specific background tasks need them

        # Define an async wrapper function to execute the coroutine with snapshotted state
        async def _wrapper():
            # Acquire semaphore before running the actual work
            log.debug(f"Waiting for semaphore... Task: {asyncio.current_task().get_name()}. Current count: {self._bg_task_semaphore._value}")
            await self._bg_task_semaphore.acquire()
            log.debug(f"Acquired semaphore. Task: {asyncio.current_task().get_name()}. New count: {self._bg_task_semaphore._value}")
            try:
                # Run the actual coroutine with timeout
                await asyncio.wait_for(
                    coro_fn(
                        self, # Pass the agent instance
                        *args,
                        workflow_id=snapshot_wf_id, # Pass snapshotted workflow_id
                        context_id=snapshot_ctx_id, # Pass snapshotted context_id
                        **kwargs,
                    ),
                    timeout=BACKGROUND_TASK_TIMEOUT_SECONDS
                )
            except asyncio.TimeoutError:
                # Log timeout specifically
                self.logger.warning(f"Background task {asyncio.current_task().get_name()} timed out after {BACKGROUND_TASK_TIMEOUT_SECONDS}s.")
            except Exception:
                # Log other exceptions, they will also be logged by the done callback
                self.logger.debug(f"Exception caught within background task wrapper {asyncio.current_task().get_name()}. Will be logged by done callback.")
            # finally:
            #     # Ensure semaphore is released *within the task itself*
            #     # This is handled by the done callback now to simplify wrapper logic
            #     # self._bg_task_semaphore.release()
            #     # log.debug(f"Released semaphore via FINALLY. Task: {asyncio.current_task().get_name()}")
            #     pass

        # Create the asyncio task
        # Naming the task helps with debugging
        task_name = f"bg_{coro_fn.__name__}_{_fmt_id(snapshot_wf_id)}_{random.randint(100,999)}"
        task = asyncio.create_task(_wrapper(), name=task_name)

        # Schedule adding the task to the tracking set safely using another task
        # This avoids potential blocking if the lock is held
        asyncio.create_task(self._add_bg_task(task))

        # Add the completion callback to handle cleanup and logging (this callback now also releases semaphore)
        task.add_done_callback(self._background_task_done)
        self.logger.debug(f"Started background task: {task.get_name()} for WF {_fmt_id(snapshot_wf_id)}")
        return task
        # <<< End Integration Block: Enhance _start_background_task >>>


    async def _add_bg_task(self, task: asyncio.Task) -> None:
        """Safely add a task to the background task set using the lock."""
        async with self._bg_tasks_lock:
            self.state.background_tasks.add(task)

    async def _cleanup_background_tasks(self) -> None:
        """
        Cancels all pending background tasks and awaits their completion.
        Called during graceful shutdown. Ensures semaphores are released.
        """
        # <<< Start Integration Block: Enhance _cleanup_background_tasks (Phase 1, Step 5) >>>
        tasks_to_cleanup: List[asyncio.Task] = []
        async with self._bg_tasks_lock: # Acquire lock to safely get the list
            # Create a copy to iterate over, as the set might be modified by callbacks
            tasks_to_cleanup = list(self.state.background_tasks)

        if not tasks_to_cleanup:
            self.logger.debug("No background tasks to clean up.")
            return

        self.logger.info(f"Cleaning up {len(tasks_to_cleanup)} background tasks‚Ä¶")
        cancelled_tasks = []
        already_done_tasks = []

        # Cancel running tasks
        for t in tasks_to_cleanup:
            if not t.done():
                # Cancel any task that hasn't finished yet
                t.cancel()
                cancelled_tasks.append(t)
            else:
                 already_done_tasks.append(t)

        # Wait for all tasks (including those just cancelled) to finish
        # return_exceptions=True prevents gather from stopping on the first exception
        results = await asyncio.gather(*tasks_to_cleanup, return_exceptions=True)

        # Log the outcome of each task cleanup
        for i, res in enumerate(results):
            task = tasks_to_cleanup[i]
            task_name = task.get_name()
            if isinstance(res, asyncio.CancelledError):
                # This is expected for tasks that were cancelled above
                self.logger.debug(f"Task {task_name} successfully cancelled during cleanup.")
            elif isinstance(res, Exception):
                # Log any unexpected errors that occurred during task execution/cleanup
                self.logger.error(f"Task {task_name} raised an exception during cleanup: {res}")
            else:
                # Task completed normally before/during cleanup or was already done
                self.logger.debug(f"Task {task_name} finalized during cleanup (completed normally or already done).")

            # --- Ensure semaphore is released ---
            # The done callback should handle release, but we add a check here as a failsafe
            # during shutdown, especially if the callback didn't run or failed itself.
            # This is tricky because we don't know if the task acquired the semaphore before being cancelled.
            # A potentially safer approach is *not* to release here unless we are certain the task
            # acquired it and didn't release it. Releasing without acquiring increments the semaphore count.
            # Given the complexity, we rely on the robust done_callback for release.
            # Adding a log message if the semaphore count seems wrong at the end might be useful.

        # Clear the tracking set after all tasks are handled
        async with self._bg_tasks_lock:
            self.state.background_tasks.clear()

        # Final check on semaphore count after cleanup
        final_sem_count = self._bg_task_semaphore._value
        if final_sem_count != MAX_CONCURRENT_BG_TASKS:
             self.logger.warning(f"Semaphore count is {final_sem_count} after cleanup, expected {MAX_CONCURRENT_BG_TASKS}. Some tasks might not have released.")

        self.logger.info("Background tasks cleanup finished.")
        # <<< End Integration Block: Enhance _cleanup_background_tasks >>>


    # ------------------------------------------------------- token estimator --
    async def _estimate_tokens_anthropic(self, data: Any) -> int:
        """
        Estimates token count for given data using the Anthropic client.

        Handles serialization of non-string data and provides a fallback
        heuristic (chars/4) if the API call fails.
        """
        if data is None:
            return 0
        try:
            if not self.anthropic_client:
                # This should ideally be caught during initialization
                raise RuntimeError("Anthropic client unavailable for token estimation")

            # Convert data to string if it's not already (e.g., dict, list)
            text_to_count = data if isinstance(data, str) else json.dumps(data, default=str, ensure_ascii=False)

            # Use the actual count_tokens method from the anthropic client
            token_count = await self.anthropic_client.count_tokens(text_to_count)
            return int(token_count) # Ensure result is an integer

        except Exception as e:
            # Log the specific error from the API call
            self.logger.warning(f"Token estimation via Anthropic API failed: {e}. Using fallback.")
            # Fallback heuristic: Estimate based on character count
            try:
                text_representation = data if isinstance(data, str) else json.dumps(data, default=str, ensure_ascii=False)
                return len(text_representation) // 4 # Rough approximation
            except Exception as fallback_e:
                # Log error if even the fallback fails
                self.logger.error(f"Token estimation fallback failed: {fallback_e}")
                return 0 # Return 0 if all estimation methods fail

    # --------------------------------------------------------------- retry util --
    async def _with_retries(
        self,
        coro_fun, # The async function to call
        *args,
        max_retries: int = 3,
        # Exceptions to retry on (can be customized per call)
        retry_exceptions: Tuple[type[BaseException], ...] = (
            ToolError, ToolInputError, # Specific MCP errors
            asyncio.TimeoutError, ConnectionError, # Common network issues
            APIConnectionError, RateLimitError, # Anthropic network issues
            APIStatusError, # Treat potentially transient API status errors as retryable
        ),
        retry_backoff: float = 2.0, # Exponential backoff factor
        jitter: Tuple[float, float] = (0.1, 0.5), # Random jitter range (min_sec, max_sec)
        **kwargs,
    ):
        """
        Generic retry wrapper for coroutine functions with exponential backoff and jitter.

        Args:
            coro_fun: The async function to execute and potentially retry.
            *args: Positional arguments for `coro_fun`.
            max_retries: Maximum number of total attempts (1 initial + max_retries-1 retries).
            retry_exceptions: Tuple of exception types that trigger a retry.
            retry_backoff: Multiplier for exponential backoff calculation.
            jitter: Tuple (min, max) defining the range for random delay added to backoff.
            **kwargs: Keyword arguments for `coro_fun`.

        Returns:
            The result of `coro_fun` upon successful execution.

        Raises:
            The last exception encountered if all retry attempts fail.
            asyncio.CancelledError: If cancellation occurs during the retry loop or wait.
        """
        attempt = 0
        last_exception = None # Store the last exception for re-raising
        while True:
            try:
                # Attempt to execute the coroutine
                return await coro_fun(*args, **kwargs)
            except retry_exceptions as e:
                attempt += 1
                last_exception = e # Store the exception
                # Check if max retries have been reached
                if attempt >= max_retries:
                    self.logger.error(f"{coro_fun.__name__} failed after {max_retries} attempts. Last error: {e}")
                    raise # Re-raise the last encountered exception
                # Calculate delay with exponential backoff and random jitter
                delay = (retry_backoff ** (attempt - 1)) + random.uniform(*jitter)
                self.logger.warning(
                    f"{coro_fun.__name__} failed ({type(e).__name__}: {str(e)[:100]}...); retry {attempt}/{max_retries} in {delay:.2f}s"
                )
                # Check for shutdown signal *before* sleeping
                if self._shutdown_event.is_set():
                    self.logger.warning(f"Shutdown signaled during retry wait for {coro_fun.__name__}. Aborting retry.")
                    # Raise CancelledError to stop the process cleanly if shutdown occurs during wait
                    raise asyncio.CancelledError(f"Shutdown during retry for {coro_fun.__name__}") from last_exception
                # Wait for the calculated delay
                await asyncio.sleep(delay)
            except asyncio.CancelledError:
                 # Propagate cancellation immediately if caught
                 self.logger.info(f"Coroutine {coro_fun.__name__} was cancelled during retry loop.")
                 raise


    # ---------------------------------------------------------------- state I/O --
    async def _save_agent_state(self) -> None:
        """
        Saves the current agent state to a JSON file atomically.

        Uses a temporary file and `os.replace` for atomicity. Includes fsync
        for robustness against crashes. Serializes the AgentState dataclass,
        handling nested structures like the plan, goal stack, and tool stats. Excludes
        transient fields like `background_tasks`.
        """
        # Create a dictionary from the dataclass state
        state_dict = dataclasses.asdict(self.state)
        # Add a timestamp for when the state was saved
        state_dict["timestamp"] = datetime.now(timezone.utc).isoformat()
        # Ensure background_tasks (non-serializable Set[Task]) is removed
        state_dict.pop("background_tasks", None)
        # Convert defaultdict to regular dict for saving tool stats
        state_dict["tool_usage_stats"] = {
            k: dict(v) for k, v in self.state.tool_usage_stats.items()
        }
        # Convert PlanStep Pydantic objects to dictionaries for saving
        state_dict["current_plan"] = [
            step.model_dump(exclude_none=True) for step in self.state.current_plan
        ]
        # --- ADDED: Ensure goal_stack (list of dicts) is saved correctly ---
        state_dict["goal_stack"] = self.state.goal_stack # Already a list of dicts

        try:
            # Ensure the directory for the state file exists
            self.agent_state_file.parent.mkdir(parents=True, exist_ok=True)
            # Define a temporary file path for atomic write
            tmp_file = self.agent_state_file.with_suffix(f".tmp_{os.getpid()}") # Process-specific avoids collisions
            # Write to the temporary file asynchronously
            async with aiofiles.open(tmp_file, "w", encoding='utf-8') as f:
                # Dump the state dictionary to JSON with indentation
                await f.write(json.dumps(state_dict, indent=2, ensure_ascii=False))
                # Ensure data is written to the OS buffer
                await f.flush()
                # Ensure data is physically written to disk (crucial for crash recovery)
                try:
                    os.fsync(f.fileno())
                except OSError as e:
                    # fsync might fail on some systems/filesystems (e.g., network drives)
                    self.logger.warning(f"os.fsync failed during state save: {e} (Continuing, but save might not be fully durable)")

            # Atomically replace the old state file with the new temporary file
            os.replace(tmp_file, self.agent_state_file)
            self.logger.debug(f"State saved atomically ‚Üí {self.agent_state_file}")
        except Exception as e:
            # Log any errors during the save process
            self.logger.error(f"Failed to save agent state: {e}", exc_info=True)
            # Attempt to clean up the temporary file if it exists after an error
            if 'tmp_file' in locals() and tmp_file.exists():
                try:
                    os.remove(tmp_file)
                except OSError as rm_err:
                     self.logger.error(f"Failed to remove temporary state file {tmp_file}: {rm_err}")


    async def _load_agent_state(self) -> None:
        """
        Loads agent state from the JSON file.

        Handles file not found, JSON decoding errors, and potential mismatches
        between the saved state structure and the current `AgentState` dataclass
        (missing keys use defaults, extra keys are ignored with a warning).
        Ensures critical fields like thresholds and the **goal stack** are initialized
        even if loading fails.
        """
        # Check if the state file exists
        if not self.agent_state_file.exists():
            # If no file, initialize with defaults, ensuring thresholds are set
            self.state = AgentState(
                 current_reflection_threshold=BASE_REFLECTION_THRESHOLD,
                 current_consolidation_threshold=BASE_CONSOLIDATION_THRESHOLD
                 # Other fields will use their dataclass defaults
            )
            self.logger.info("No prior state file found. Starting fresh with default state.")
            return
        # Try loading the state file
        try:
            async with aiofiles.open(self.agent_state_file, "r", encoding='utf-8') as f:
                # Read and parse the JSON data
                data = json.loads(await f.read())

            # Prepare keyword arguments for AgentState constructor
            kwargs: Dict[str, Any] = {}
            processed_keys = set() # Track keys successfully processed from the file

            # Iterate through the fields defined in the AgentState dataclass
            for fld in dataclasses.fields(AgentState):
                # Skip fields that are not meant to be initialized (like background_tasks)
                if not fld.init:
                    continue

                name = fld.name
                processed_keys.add(name)

                # Check if the field exists in the loaded data
                if name in data:
                    value = data[name]
                    # Handle specific fields needing type conversion or validation
                    if name == "current_plan":
                        try:
                            # Validate and convert saved plan steps back to PlanStep objects
                            if isinstance(value, list):
                                kwargs["current_plan"] = [PlanStep(**d) for d in value]
                            else:
                                raise TypeError("Saved plan is not a list")
                        except (ValidationError, TypeError) as e:
                            # If plan loading fails, reset to default plan
                            self.logger.warning(f"Plan reload failed: {e}. Resetting plan.")
                            kwargs["current_plan"] = [PlanStep(description=DEFAULT_PLAN_STEP)]
                    elif name == "tool_usage_stats":
                        # Reconstruct defaultdict structure for tool stats
                        dd = _default_tool_stats()
                        if isinstance(value, dict):
                            for k, v_dict in value.items():
                                if isinstance(v_dict, dict):
                                    # Ensure required keys exist with correct types
                                    dd[k]["success"] = int(v_dict.get("success", 0))
                                    dd[k]["failure"] = int(v_dict.get("failure", 0))
                                    dd[k]["latency_ms_total"] = float(v_dict.get("latency_ms_total", 0.0))
                        kwargs["tool_usage_stats"] = dd
                    # --- ADDED: Handle loading goal_stack ---
                    elif name == "goal_stack":
                        if isinstance(value, list):
                             # Basic validation: ensure items are dicts (more complex validation could be added)
                             if all(isinstance(item, dict) for item in value):
                                 kwargs[name] = value
                             else:
                                 self.logger.warning("Invalid goal_stack format in saved state. Resetting.")
                                 kwargs[name] = [] # Reset to empty list
                        else:
                             self.logger.warning("goal_stack in saved state is not a list. Resetting.")
                             kwargs[name] = []
                    # Add handling for other complex types here if necessary in the future
                    else:
                        # Directly assign the loaded value if no special handling is needed
                        kwargs[name] = value
                else:
                    # Field defined in AgentState but missing in saved data
                    self.logger.debug(f"Field '{name}' not found in saved state. Using default.")
                    # Use the dataclass default factory or default value if defined
                    if fld.default_factory is not dataclasses.MISSING:
                        kwargs[name] = fld.default_factory()
                    elif fld.default is not dataclasses.MISSING:
                        kwargs[name] = fld.default
                    # Explicitly handle potentially missing thresholds if they didn't have defaults
                    elif name == "current_reflection_threshold":
                        kwargs[name] = BASE_REFLECTION_THRESHOLD
                    elif name == "current_consolidation_threshold":
                        kwargs[name] = BASE_CONSOLIDATION_THRESHOLD
                    # Handle missing goal stack/ID explicitly
                    elif name == "goal_stack":
                         kwargs[name] = [] # Default to empty list
                    elif name == "current_goal_id":
                         kwargs[name] = None # Default to None
                    # Otherwise, the field will be missing if it had no default and wasn't saved

            # Warn about extra keys found in the file but not defined in the current AgentState
            # This helps detect state format drift or old fields
            extra_keys = set(data.keys()) - processed_keys - {"timestamp"} # Exclude meta timestamp key
            if extra_keys:
                self.logger.warning(f"Ignoring unknown keys found in state file: {extra_keys}")

            # Create the AgentState instance using the processed keyword arguments
            # >>>>> PRESERVED ORIGINAL LOADING LOGIC FOR UNHANDLED FIELDS <<<<<
            # Note: This implicitly handles fields not explicitly checked above,
            # relying on the dataclass constructor to handle types if possible.
            # It's generally safer to handle complex types explicitly as done for plan/stats.
            temp_state = AgentState(**kwargs)

            # --- Validation and Correction after loading ---
            # Ensure mandatory fields (like thresholds) have values AFTER construction,
            # using defaults if somehow missed or loading failed for them.
            if not isinstance(temp_state.current_reflection_threshold, int):
                 self.logger.warning(f"Invalid loaded reflection threshold ({temp_state.current_reflection_threshold}). Resetting to base.")
                 temp_state.current_reflection_threshold = BASE_REFLECTION_THRESHOLD
            else:
                 # Ensure loaded threshold is within bounds
                 temp_state.current_reflection_threshold = max(MIN_REFLECTION_THRESHOLD, min(MAX_REFLECTION_THRESHOLD, temp_state.current_reflection_threshold))

            if not isinstance(temp_state.current_consolidation_threshold, int):
                 self.logger.warning(f"Invalid loaded consolidation threshold ({temp_state.current_consolidation_threshold}). Resetting to base.")
                 temp_state.current_consolidation_threshold = BASE_CONSOLIDATION_THRESHOLD
            else:
                 # Ensure loaded threshold is within bounds
                 temp_state.current_consolidation_threshold = max(MIN_CONSOLIDATION_THRESHOLD, min(MAX_CONSOLIDATION_THRESHOLD, temp_state.current_consolidation_threshold))

            # Ensure goal stack consistency
            if not isinstance(temp_state.goal_stack, list):
                self.logger.warning("Loaded goal_stack is not a list. Resetting.")
                temp_state.goal_stack = []
            # Ensure current_goal_id points to a goal actually in the stack (or is None)
            if temp_state.current_goal_id and not any(g.get('goal_id') == temp_state.current_goal_id for g in temp_state.goal_stack):
                 self.logger.warning(f"Loaded current_goal_id {_fmt_id(temp_state.current_goal_id)} not found in loaded goal_stack. Resetting.")
                 # Set current_goal_id to the top of the loaded stack, or None if stack is empty
                 temp_state.current_goal_id = temp_state.goal_stack[-1].get('goal_id') if temp_state.goal_stack else None

            # Assign the potentially corrected state
            self.state = temp_state
            self.logger.info(f"Loaded state from {self.agent_state_file}; current loop {self.state.current_loop}")

        except (json.JSONDecodeError, TypeError, FileNotFoundError) as e:
            # Handle common file loading or parsing errors
            self.logger.error(f"State load failed: {e}. Resetting to default state.", exc_info=True)
            # Reset to a clean default state on failure
            self.state = AgentState(
                current_reflection_threshold=BASE_REFLECTION_THRESHOLD,
                current_consolidation_threshold=BASE_CONSOLIDATION_THRESHOLD
            )
        except Exception as e:
            # Catch any other unexpected errors during state loading
            self.logger.critical(f"Unexpected error loading state: {e}. Resetting to default state.", exc_info=True)
            self.state = AgentState(
                current_reflection_threshold=BASE_REFLECTION_THRESHOLD,
                current_consolidation_threshold=BASE_CONSOLIDATION_THRESHOLD
            )


    # --------------------------------------------------- tool‚Äëlookup helper --
    def _find_tool_server(self, tool_name: str) -> Optional[str]:
        """
        Finds an active server providing the specified tool via MCPClient's Server Manager.

        Args:
            tool_name: The original, potentially sanitized, tool name (e.g., "unified_memory:store_memory").

        Returns:
            The name of the active server providing the tool, "AGENT_INTERNAL" for the plan update tool,
            or None if the tool is not found on any active server.
        """
        # Ensure MCP Client and its server manager are available
        if not self.mcp_client or not self.mcp_client.server_manager:
            self.logger.error("MCP Client or Server Manager not available for tool lookup.")
            return None

        sm = self.mcp_client.server_manager
        # Check registered tools first (uses original tool names)
        if tool_name in sm.tools:
            server_name = sm.tools[tool_name].server_name
            # Verify the server is currently connected and considered active
            if server_name in sm.active_sessions:
                self.logger.debug(f"Found tool '{tool_name}' on active server '{server_name}'.")
                return server_name
            else:
                # Tool is known but its server is not currently active
                self.logger.debug(f"Server '{server_name}' for tool '{tool_name}' is registered but not active.")
                return None

        # Handle core tools if a server named "CORE" is active
        # (Assuming core tools follow a "core:" prefix convention)
        if tool_name.startswith("core:") and "CORE" in sm.active_sessions:
            self.logger.debug(f"Found core tool '{tool_name}' on active CORE server.")
            return "CORE"

        # Handle the agent's internal plan update tool
        if tool_name == AGENT_TOOL_UPDATE_PLAN:
            self.logger.debug(f"Internal tool '{tool_name}' does not require a server.")
            return "AGENT_INTERNAL" # Return a special marker

        # If the tool is not found in registered tools or core tools
        self.logger.debug(f"Tool '{tool_name}' not found on any active server.")
        return None


    # ------------------------------------------------------------ initialization --
    async def initialize(self) -> bool:
        """
        Initializes the Agent Master Loop.

        Loads prior agent state, fetches available tool schemas from MCPClient,
        validates the presence of essential tools (including goal tools),
        checks the validity of any loaded workflow and goal state,
        and sets the initial thought chain ID.

        Returns:
            True if initialization is successful, False otherwise.
        """
        self.logger.info("Initializing Agent loop ‚Ä¶")
        # Load state from file first
        await self._load_agent_state()

        # Ensure context_id matches workflow_id if context_id was missing after load
        # This maintains consistency, assuming context usually maps 1:1 with workflow initially
        if self.state.workflow_id and not self.state.context_id:
            self.state.context_id = self.state.workflow_id
            self.logger.info(f"Initialized context_id from loaded workflow_id: {_fmt_id(self.state.workflow_id)}")

        try:
            # Check if MCPClient's server manager is ready
            if not self.mcp_client.server_manager:
                self.logger.error("MCP Client server manager not initialized.")
                return False

            # Fetch all available tool schemas formatted for the LLM (e.g., Anthropic format)
            all_tools = self.mcp_client.server_manager.format_tools_for_anthropic()

            # Manually inject the schema for the internal agent plan-update tool
            plan_step_schema = PlanStep.model_json_schema()
            # Remove 'title' if Pydantic adds it automatically, as it's not part of PlanStep definition
            plan_step_schema.pop('title', None)
            all_tools.append(
                {
                    "name": AGENT_TOOL_UPDATE_PLAN, # Use sanitized name expected by LLM
                    "description": "Replace the agent's current plan with a new list of plan steps. Use this for significant replanning or error recovery.",
                    "input_schema": { # Anthropic uses 'input_schema'
                        "type": "object",
                        "properties": {
                            "plan": {
                                "type": "array",
                                "description": "Complete new plan as a list of PlanStep objects.",
                                "items": plan_step_schema, # Embed the generated PlanStep schema
                            }
                        },
                        "required": ["plan"],
                    },
                }
            )

            # Filter the fetched schemas to keep only those relevant to this agent
            # (Unified Memory tools and the internal agent tool)
            self.tool_schemas = []
            loaded_tool_names = set()
            for sc in all_tools:
                # Map the sanitized name back to the original for filtering
                original_name = self.mcp_client.server_manager.sanitized_to_original.get(sc["name"], sc["name"])
                # Keep if it's a unified_memory tool or the internal agent plan update tool
                if original_name.startswith("unified_memory:") or sc["name"] == AGENT_TOOL_UPDATE_PLAN:
                    self.tool_schemas.append(sc)
                    loaded_tool_names.add(original_name)

            self.logger.info(f"Loaded {len(self.tool_schemas)} relevant tool schemas: {loaded_tool_names}")

            # Verify that essential tools for core functionality are available
            essential = [
                TOOL_CREATE_WORKFLOW, TOOL_RECORD_ACTION_START, TOOL_RECORD_ACTION_COMPLETION,
                TOOL_RECORD_THOUGHT, TOOL_STORE_MEMORY, TOOL_GET_WORKING_MEMORY,
                TOOL_HYBRID_SEARCH, # Essential for enhanced context gathering
                TOOL_GET_CONTEXT,   # Essential for core context
                TOOL_REFLECTION,    # Essential for meta-cognition loop
                TOOL_CONSOLIDATION, # Essential for meta-cognition loop
                TOOL_GET_WORKFLOW_DETAILS, # Needed for setting default chain ID on load
                # --- ADDED: Check essential Goal Stack tools ---
                TOOL_PUSH_SUB_GOAL,
                TOOL_MARK_GOAL_STATUS,
                # TOOL_GET_GOAL_DETAILS, # Maybe not essential to *start*, but needed for context
            ]
            # Check availability using the server lookup helper
            missing = [t for t in essential if not self._find_tool_server(t)]
            if missing:
                # Log as error because agent functionality will be significantly impaired
                self.logger.error(f"Missing essential tools: {missing}. Agent functionality WILL BE impaired.")
                # Depending on desired strictness, could return False here to halt initialization

            # Check the validity of the workflow ID loaded from the state file
            # Determine the top workflow ID from the stack or the primary ID
            top_wf = (self.state.workflow_stack[-1] if self.state.workflow_stack else None) or self.state.workflow_id
            if top_wf and not await self._check_workflow_exists(top_wf):
                # If the loaded workflow doesn't exist anymore, reset workflow-specific state
                self.logger.warning(
                    f"Stored workflow '{_fmt_id(top_wf)}' not found in UMS; resetting workflow-specific state."
                )
                # Preserve non-workflow specific state like stats and dynamic thresholds
                preserved_stats = self.state.tool_usage_stats
                pres_ref_thresh = self.state.current_reflection_threshold
                pres_con_thresh = self.state.current_consolidation_threshold
                # Reset state, keeping only preserved items
                self.state = AgentState(
                    tool_usage_stats=preserved_stats,
                    current_reflection_threshold=pres_ref_thresh,
                    current_consolidation_threshold=pres_con_thresh
                    # All other fields reset to defaults (including goal_stack, current_goal_id)
                )
                # Save the reset state immediately
                await self._save_agent_state()

            # --- ADDED: Validate loaded goal stack consistency ---
            await self._validate_goal_stack_on_load()

            # Initialize the current thought chain ID if a workflow exists but the chain ID is missing
            # This ensures thoughts are recorded correctly after loading state
            if self.state.workflow_id and not self.state.current_thought_chain_id:
                await self._set_default_thought_chain_id() # Attempt to find/set the primary chain

            self.logger.info("Agent loop initialization complete.")
            return True # Initialization successful
        except Exception as e:
            # Catch any unexpected errors during initialization
            self.logger.critical(f"Agent loop initialization failed: {e}", exc_info=True)
            return False # Initialization failed


    async def _set_default_thought_chain_id(self):
        """
        Sets the `current_thought_chain_id` in the agent state to the primary
        (usually first created) thought chain associated with the current workflow.
        """
        # Determine the current workflow ID from the stack or primary state
        current_wf_id = self.state.workflow_stack[-1] if self.state.workflow_stack else self.state.workflow_id
        if not current_wf_id:
            self.logger.debug("Cannot set default thought chain ID: No active workflow.")
            return # Cannot set if no workflow is active

        get_details_tool = TOOL_GET_WORKFLOW_DETAILS # Tool needed to fetch chains

        # Check if the necessary tool is available
        if self._find_tool_server(get_details_tool):
            try:
                # Call the tool internally to get workflow details, including thought chains
                details = await self._execute_tool_call_internal(
                    get_details_tool,
                    {
                        "workflow_id": current_wf_id,
                        "include_thoughts": True, # Must include thoughts to get chain info
                        "include_actions": False, # Not needed for this task
                        "include_artifacts": False,
                        "include_memories": False
                    },
                    record_action=False # Internal setup action, don't log as agent action
                )
                # Check if the tool call was successful and returned data
                if details.get("success"):
                    thought_chains = details.get("thought_chains")
                    # Check if thought_chains is a non-empty list
                    if isinstance(thought_chains, list) and thought_chains:
                        # Assume the first chain in the list is the primary one (usually ordered by creation time)
                        first_chain = thought_chains[0]
                        chain_id = first_chain.get("thought_chain_id")
                        if chain_id:
                            # Set the state variable
                            self.state.current_thought_chain_id = chain_id
                            self.logger.info(f"Set current_thought_chain_id to primary chain: {_fmt_id(self.state.current_thought_chain_id)} for workflow {_fmt_id(current_wf_id)}")
                            return # Successfully set
                        else:
                             # Log warning if the found chain object is missing the ID
                             self.logger.warning(f"Primary thought chain found for workflow {current_wf_id}, but it lacks an ID in the details.")
                    else:
                         # Log warning if no chains were found for the workflow
                         self.logger.warning(f"Could not find any thought chains in details for workflow {current_wf_id}.")
                else:
                    # Log the error message returned by the tool if it failed
                    self.logger.error(f"Tool '{get_details_tool}' failed while trying to get default thought chain: {details.get('error')}")

            except Exception as e:
                # Log any exceptions encountered during the tool call itself
                self.logger.error(f"Error fetching workflow details for default chain: {e}", exc_info=False)
        else:
            # Log warning if the required tool is unavailable
            self.logger.warning(f"Cannot set default thought chain ID: Tool '{get_details_tool}' unavailable.")

        # Fallback message if the chain ID couldn't be set for any reason
        self.logger.info(f"Could not determine primary thought chain ID for WF {_fmt_id(current_wf_id)}. Will use default on first thought.")


    async def _check_workflow_exists(self, workflow_id: str) -> bool:
        """
        Efficiently checks if a given workflow ID exists using the UMS.

        Args:
            workflow_id: The workflow ID to check.

        Returns:
            True if the workflow exists, False otherwise.
        """
        self.logger.debug(f"Checking existence of workflow {_fmt_id(workflow_id)} using {TOOL_GET_WORKFLOW_DETAILS}.")
        tool_name = TOOL_GET_WORKFLOW_DETAILS
        # Check if the required tool is available
        if not self._find_tool_server(tool_name):
            self.logger.error(f"Cannot check workflow existence: Tool {tool_name} unavailable.")
            # If tool is unavailable, we cannot confirm existence, assume False for safety
            return False
        try:
            # Call get_workflow_details with minimal includes for efficiency
            result = await self._execute_tool_call_internal(
                tool_name,
                {
                    "workflow_id": workflow_id,
                    "include_actions": False,
                    "include_artifacts": False,
                    "include_thoughts": False,
                    "include_memories": False
                },
                record_action=False # This is an internal check
            )
            # If the tool call returns success=True, the workflow exists
            return isinstance(result, dict) and result.get("success", False)
        except ToolInputError as e:
            # A ToolInputError often indicates the ID was not found
            self.logger.debug(f"Workflow {_fmt_id(workflow_id)} likely not found (ToolInputError: {e}).")
            return False
        except Exception as e:
            # Log other errors encountered during the check
            self.logger.error(f"Error checking workflow {_fmt_id(workflow_id)} existence: {e}", exc_info=False)
            # Assume not found if an error occurs
            return False

    # <<< Start Integration Block: Goal Stack Validation Helper >>>
    async def _validate_goal_stack_on_load(self):
        """
        Validates the loaded goal stack against the UMS.

        Checks if the goals in the stack still exist and belong to the correct
        workflow context. Removes invalid goals from the stack.
        This assumes a UMS tool `get_goal_details` exists.
        """
        if not self.state.goal_stack:
            return # Nothing to validate if stack is empty

        tool_name = TOOL_GET_GOAL_DETAILS # Assumed UMS tool
        if not self._find_tool_server(tool_name):
            self.logger.warning(f"Cannot validate goal stack: Tool {tool_name} unavailable. Loaded stack may be invalid.")
            return

        current_wf_id = self.state.workflow_id
        if not current_wf_id:
            self.logger.warning("Cannot validate goal stack: No active workflow ID.")
            # Clear the stack if there's no workflow context
            self.state.goal_stack = []
            self.state.current_goal_id = None
            return

        valid_goals = []
        needs_update = False
        original_stack_ids = [g.get('goal_id') for g in self.state.goal_stack if g.get('goal_id')]

        for goal_dict in self.state.goal_stack:
            goal_id = goal_dict.get('goal_id')
            if not goal_id:
                self.logger.warning(f"Found goal with missing ID in loaded stack: {goal_dict}. Removing.")
                needs_update = True
                continue

            try:
                # Check goal existence and workflow association
                goal_details = await self._execute_tool_call_internal(
                    tool_name, {"goal_id": goal_id}, record_action=False
                )
                # Assuming the tool returns 'success' and 'workflow_id' if found
                if goal_details.get("success") and goal_details.get("workflow_id") == current_wf_id:
                    valid_goals.append(goal_dict) # Keep the goal if valid
                else:
                    self.logger.warning(f"Removing goal {_fmt_id(goal_id)} from stack: Not found or wrong workflow ({goal_details.get('workflow_id')}) in UMS.")
                    needs_update = True
            except Exception as e:
                 self.logger.error(f"Error validating goal {_fmt_id(goal_id)}: {e}. Removing from stack.")
                 needs_update = True

        if needs_update:
            self.logger.info(f"Goal stack updated after validation. Original IDs: {[_fmt_id(g) for g in original_stack_ids]}, Valid IDs: {[_fmt_id(g.get('goal_id')) for g in valid_goals]}")
            self.state.goal_stack = valid_goals
            # Update current_goal_id to the top of the validated stack
            self.state.current_goal_id = self.state.goal_stack[-1].get('goal_id') if self.state.goal_stack else None
            self.logger.info(f"Reset current_goal_id after stack validation: {_fmt_id(self.state.current_goal_id)}")

        # If stack became empty after validation, ensure current_goal_id is None
        elif not self.state.goal_stack:
             self.state.current_goal_id = None

    # <<< End Integration Block: Goal Stack Validation Helper >>>

    # <<< Start Integration Block: Plan Cycle Detection Helper (Phase 1, Step 3) >>>
    def _detect_plan_cycle(self, plan: List[PlanStep]) -> bool:
        """
        Detects cyclic dependencies in the agent's plan using Depth First Search.

        Args:
            plan: The list of PlanStep objects representing the current plan.

        Returns:
            True if a cycle is detected, False otherwise.
        """
        if not plan: return False # Empty plan has no cycles

        adj: Dict[str, Set[str]] = defaultdict(set) # Adjacency list: step_id -> set(dependency_step_ids)
        plan_step_ids = {step.id for step in plan} # Set of all valid step IDs in the current plan

        # Build the adjacency list from depends_on relationships
        for step in plan:
            for dep_id in step.depends_on:
                # Only add dependency if the target step actually exists in the current plan
                if dep_id in plan_step_ids:
                    adj[step.id].add(dep_id)
                else:
                    # Log if a dependency points to a non-existent step (potential issue)
                    self.logger.warning(f"Plan step {_fmt_id(step.id)} depends on non-existent step {_fmt_id(dep_id)} in current plan.")

        # DFS state tracking:
        # path: nodes currently in the recursion stack for the current DFS path
        # visited: nodes that have been completely explored (all descendants visited)
        path: Set[str] = set()
        visited: Set[str] = set()

        def dfs(node_id: str) -> bool:
            """Recursive DFS function. Returns True if a cycle is detected."""
            path.add(node_id) # Mark node as currently visiting
            visited.add(node_id) # Mark node as visited

            # Explore neighbors (dependencies)
            for neighbor_id in adj[node_id]:
                if neighbor_id in path: # Cycle detected! Neighbor is already in the current path.
                    self.logger.warning(f"Dependency cycle detected involving steps: {_fmt_id(node_id)} -> {_fmt_id(neighbor_id)}")
                    return True
                if neighbor_id not in visited: # If neighbor not visited yet, recurse
                    if dfs(neighbor_id):
                        return True # Propagate cycle detection signal up

            # Finished exploring node_id's descendants, remove from current path
            path.remove(node_id)
            return False # No cycle found starting from this node

        # Run DFS from each node in the plan to check all potential cycles
        for step_id in plan_step_ids:
            if step_id not in visited:
                if dfs(step_id):
                    return True # Cycle found

        # If DFS completes for all nodes without finding a cycle
        return False
    # <<< End Integration Block: Plan Cycle Detection Helper >>>

    # ------------------------------------------------ dependency check --
    async def _check_prerequisites(self, ids: List[str]) -> Tuple[bool, str]:
        """
        Checks if all specified prerequisite action IDs have status 'completed'.

        Args:
            ids: A list of action IDs to check.

        Returns:
            A tuple: (bool: True if all completed, False otherwise,
                      str: Reason for failure or "All dependencies completed.")
        """
        # If no IDs are provided, prerequisites are met by default
        if not ids:
            return True, "No dependencies listed."

        tool_name = TOOL_GET_ACTION_DETAILS # Tool needed to get action status
        # Check if the tool is available
        if not self._find_tool_server(tool_name):
            self.logger.error(f"Cannot check prerequisites: Tool {tool_name} unavailable.")
            return False, f"Tool {tool_name} unavailable."

        self.logger.debug(f"Checking prerequisites: {[_fmt_id(item_id) for item_id in ids]}")
        try:
            # Call the tool internally to get details for the specified action IDs
            res = await self._execute_tool_call_internal(
                tool_name,
                {"action_ids": ids, "include_dependencies": False}, # Don't need nested dependencies for this check
                record_action=False # Internal check
            )

            # Check if the tool call itself failed
            if not res.get("success"):
                error_msg = res.get("error", "Unknown error during dependency check.")
                self.logger.warning(f"Dependency check failed: {error_msg}")
                return False, f"Failed to check dependencies: {error_msg}"

            # Process the returned action details
            actions_found = res.get("actions", [])
            found_ids = {a.get("action_id") for a in actions_found}
            # Check if any requested dependency IDs were not found
            missing_ids = list(set(ids) - found_ids)
            if missing_ids:
                self.logger.warning(f"Dependency actions not found: {[_fmt_id(item_id) for item_id in missing_ids]}")
                return False, f"Dependency actions not found: {[_fmt_id(item_id) for item_id in missing_ids]}"

            # Check the status of each found dependency action
            incomplete_actions = []
            for action in actions_found:
                if action.get("status") != ActionStatus.COMPLETED.value:
                    # Collect details of incomplete actions for the reason message
                    incomplete_actions.append(
                        f"'{action.get('title', _fmt_id(action.get('action_id')))}' (Status: {action.get('status', 'UNKNOWN')})"
                    )

            # If any actions are not completed, prerequisites are not met
            if incomplete_actions:
                reason = f"Dependencies not completed: {', '.join(incomplete_actions)}"
                self.logger.warning(reason)
                return False, reason

            # If all checks passed, prerequisites are met
            self.logger.debug("All dependencies completed.")
            return True, "All dependencies completed."

        except Exception as e:
            # Log any exceptions during the prerequisite check
            self.logger.error(f"Error during prerequisite check: {e}", exc_info=True)
            return False, f"Exception checking prerequisites: {str(e)}"


    # ---------------------------------------------------- action recording --
    async def _record_action_start_internal(
        self,
        tool_name: str,
        tool_args: Dict[str, Any],
        planned_dependencies: Optional[List[str]] = None,
    ) -> Optional[str]:
        """
        Records the start of an action using the UMS tool.

        Optionally records dependencies declared for this action.

        Args:
            tool_name: The name of the tool being executed.
            tool_args: The arguments passed to the tool.
            planned_dependencies: Optional list of action IDs this action depends on.

        Returns:
            The generated `action_id` if successful, otherwise None.
        """
        start_tool = TOOL_RECORD_ACTION_START # Tool for recording action start
        # Check if the recording tool is available
        if not self._find_tool_server(start_tool):
            self.logger.error(f"Cannot record action start: Tool '{start_tool}' unavailable.")
            return None

        # Get the current workflow ID from the state
        current_wf_id = self.state.workflow_stack[-1] if self.state.workflow_stack else self.state.workflow_id
        if not current_wf_id:
            self.logger.warning("Cannot record action start: No active workflow ID in state.")
            return None

        # Prepare the payload for the recording tool
        payload = {
            "workflow_id": current_wf_id,
            # Generate a basic title based on the tool name
            "title": f"Execute: {tool_name.split(':')[-1]}", # Use only the tool name part
            "action_type": ActionType.TOOL_USE.value, # Assume it's a tool use action
            "tool_name": tool_name,
            "tool_args": tool_args, # Pass the arguments being used
            "reasoning": f"Agent initiated tool call: {tool_name}", # Basic reasoning
            # Status will likely be set to IN_PROGRESS by the tool itself
        }

        action_id: Optional[str] = None
        try:
            # Call the recording tool internally (don't record this recording action)
            res = await self._execute_tool_call_internal(
                start_tool, payload, record_action=False
            )
            # Check if the recording was successful and returned an action ID
            if res.get("success"):
                action_id = res.get("action_id")
                if action_id:
                    self.logger.debug(f"Action started: {_fmt_id(action_id)} for tool {tool_name}")
                    # If dependencies were provided, record them *after* getting the action ID
                    if planned_dependencies:
                        await self._record_action_dependencies_internal(action_id, planned_dependencies)
                else:
                    # Log if the tool succeeded but didn't return an ID
                    self.logger.warning(f"Tool {start_tool} succeeded but returned no action_id.")
            else:
                # Log if the recording tool itself failed
                self.logger.error(f"Failed to record action start for {tool_name}: {res.get('error')}")

        except Exception as e:
            # Log any exceptions during the recording process
            self.logger.error(f"Exception recording action start for {tool_name}: {e}", exc_info=True)

        return action_id # Return the action ID or None


    async def _record_action_dependencies_internal(
        self,
        source_id: str, # The action being started
        target_ids: List[str], # The actions it depends on
    ) -> None:
        """
        Records dependencies (source_id REQUIRES target_id) in the UMS.

        Args:
            source_id: The ID of the action that has dependencies.
            target_ids: A list of action IDs that `source_id` depends on.
        """
        # Basic validation
        if not source_id or not target_ids:
            self.logger.debug("Skipping dependency recording: Missing source or target IDs.")
            return
        # Filter out empty/invalid target IDs and self-references
        valid_target_ids = {tid for tid in target_ids if tid and tid != source_id}
        if not valid_target_ids:
            self.logger.debug(f"No valid dependencies to record for source action {_fmt_id(source_id)}.")
            return

        dep_tool = TOOL_ADD_ACTION_DEPENDENCY # Tool for adding dependencies
        # Check tool availability
        if not self._find_tool_server(dep_tool):
            self.logger.error(f"Cannot record dependencies: Tool '{dep_tool}' unavailable.")
            return

        # Get current workflow ID (should match the source action's workflow)
        current_wf_id = self.state.workflow_stack[-1] if self.state.workflow_stack else self.state.workflow_id
        if not current_wf_id:
            self.logger.warning(f"Cannot record dependencies for action {_fmt_id(source_id)}: No active workflow ID.")
            return

        self.logger.debug(f"Recording {len(valid_target_ids)} dependencies for action {_fmt_id(source_id)}: depends on {[_fmt_id(tid) for tid in valid_target_ids]}")

        # Create tasks to record each dependency concurrently
        tasks = []
        for target_id in valid_target_ids:
            args = {
                # workflow_id might be inferred by the tool, but pass for robustness
                # "workflow_id": current_wf_id, # Removed as UMS tool infers from action IDs
                "source_action_id": source_id,
                "target_action_id": target_id,
                "dependency_type": "requires", # Assuming 'requires' is the default/most common type here
            }
            # Call the dependency tool internally for each target ID
            task = asyncio.create_task(
                self._execute_tool_call_internal(dep_tool, args, record_action=False)
            )
            tasks.append(task)

        # Wait for all dependency recording tasks to complete
        results = await asyncio.gather(*tasks, return_exceptions=True)
        target_list = list(valid_target_ids) # Ensure consistent ordering for results
        # Log any failures encountered during dependency recording
        for i, res in enumerate(results):
            target_id = target_list[i] # Get corresponding target ID
            if isinstance(res, Exception):
                self.logger.error(f"Error recording dependency {_fmt_id(source_id)} -> {_fmt_id(target_id)}: {res}", exc_info=False)
            elif isinstance(res, dict) and not res.get("success"):
                # Log if the tool call itself failed
                self.logger.warning(f"Failed recording dependency {_fmt_id(source_id)} -> {_fmt_id(target_id)}: {res.get('error')}")
            # else: Successfully recorded


    async def _record_action_completion_internal(
        self,
        action_id: str,
        result: Dict[str, Any], # The final result dict from the tool execution
    ) -> None:
        """
        Records the completion or failure status and result for a given action ID.

        Args:
            action_id: The ID of the action to mark as completed/failed.
            result: The result dictionary returned by `_execute_tool_call_internal`.
        """
        completion_tool = TOOL_RECORD_ACTION_COMPLETION # Tool for recording completion
        # Check tool availability
        if not self._find_tool_server(completion_tool):
            self.logger.error(f"Cannot record action completion: Tool '{completion_tool}' unavailable.")
            return

        # Determine the final status based on the 'success' key in the result dict
        status = (
            ActionStatus.COMPLETED.value
            if isinstance(result, dict) and result.get("success")
            else ActionStatus.FAILED.value
        )

        # Get current workflow ID (should match the action's workflow)
        current_wf_id = self.state.workflow_stack[-1] if self.state.workflow_stack else self.state.workflow_id
        if not current_wf_id:
            self.logger.warning(f"Cannot record completion for action {_fmt_id(action_id)}: No active workflow ID.")
            return

        # Prepare payload for the completion tool
        payload = {
            # Pass workflow_id for context, though tool might infer from action_id
            # "workflow_id": current_wf_id, # Removed as UMS tool infers from action ID
            "action_id": action_id,
            "status": status,
            # Pass the entire result dictionary to be stored/summarized by the UMS tool
            "tool_result": result,
            # Optionally, extract a summary here if needed by the tool, but UMS likely handles this
            # "summary": result.get("summary") or result.get("message") or str(result.get("data"))[:100]
        }

        try:
            # Call the completion tool internally
            completion_result = await self._execute_tool_call_internal(
                completion_tool, payload, record_action=False # Don't record this meta-action
            )
            # Log success or failure of the recording itself
            if completion_result.get("success"):
                self.logger.debug(f"Action completion recorded for {_fmt_id(action_id)} (Status: {status})")
            else:
                self.logger.error(f"Failed to record action completion for {_fmt_id(action_id)}: {completion_result.get('error')}")
        except Exception as e:
            # Log any exceptions during the completion recording call
            self.logger.error(f"Exception recording action completion for {_fmt_id(action_id)}: {e}", exc_info=True)

    # ---------------------------------------------------- auto‚Äëlink helper --
    async def _run_auto_linking(
        self,
        memory_id: str, # The ID of the newly created/updated memory
        *,
        workflow_id: Optional[str], # Snapshotted workflow ID
        context_id: Optional[str], # Snapshotted context ID (unused here but passed)
    ) -> None:
        """
        Background task to find semantically similar memories and automatically
        create links to them. Uses richer link types based on memory types.
        """
        # (Keep the integrated _run_auto_linking method logic)
        # Check if the agent's current workflow matches the one snapshotted when the task was created
        # Also check if shutdown has been requested
        if workflow_id != self.state.workflow_id or self._shutdown_event.is_set():
            self.logger.debug(f"Skipping auto-linking for {_fmt_id(memory_id)}: Workflow changed ({_fmt_id(self.state.workflow_id)} vs {_fmt_id(workflow_id)}) or shutdown signaled.")
            return

        try:
            # Validate inputs
            if not memory_id or not workflow_id:
                self.logger.debug(f"Skipping auto-linking: Missing memory_id ({_fmt_id(memory_id)}) or workflow_id ({_fmt_id(workflow_id)}).")
                return

            # Introduce a small random delay to distribute load
            await asyncio.sleep(random.uniform(*AUTO_LINKING_DELAY_SECS))
            # Check shutdown again after sleep
            if self._shutdown_event.is_set(): return

            self.logger.debug(f"Attempting auto-linking for memory {_fmt_id(memory_id)} in workflow {_fmt_id(workflow_id)}...")

            # 1. Get details of the source memory (the one just created/updated)
            source_mem_details_result = await self._execute_tool_call_internal(
                TOOL_GET_MEMORY_BY_ID, {"memory_id": memory_id, "include_links": False}, record_action=False
            )
            # Ensure retrieval succeeded and the memory still belongs to the expected workflow
            if not source_mem_details_result.get("success") or source_mem_details_result.get("workflow_id") != workflow_id:
                self.logger.warning(f"Auto-linking failed for {_fmt_id(memory_id)}: Couldn't retrieve source memory or workflow mismatch.")
                return
            source_mem = source_mem_details_result # Result is the memory dict

            # 2. Determine text for similarity search (use description or truncated content)
            query_text = source_mem.get("description", "") or source_mem.get("content", "")[:200] # Limit content length
            if not query_text:
                self.logger.debug(f"Skipping auto-linking for {_fmt_id(memory_id)}: No description or content for query.")
                return

            # 3. Perform semantic search for similar memories within the same workflow
            # Prefer hybrid search if available, fall back to semantic
            search_tool = TOOL_HYBRID_SEARCH if self._find_tool_server(TOOL_HYBRID_SEARCH) else TOOL_SEMANTIC_SEARCH
            if not self._find_tool_server(search_tool):
                self.logger.warning(f"Skipping auto-linking: Tool {search_tool} unavailable.")
                return

            search_args = {
                "workflow_id": workflow_id, # Search within the snapshotted workflow
                "query": query_text,
                "limit": self.auto_linking_max_links + 1, # Fetch one extra to filter self
                "threshold": self.auto_linking_threshold, # Use configured threshold
                "include_content": False # Don't need full content of similar items
            }
            # Adjust weights if using hybrid search (prioritize semantic similarity)
            if search_tool == TOOL_HYBRID_SEARCH:
                search_args.update({"semantic_weight": 0.8, "keyword_weight": 0.2})

            similar_results = await self._execute_tool_call_internal(
                search_tool, search_args, record_action=False
            )
            if not similar_results.get("success"):
                self.logger.warning(f"Auto-linking search failed for {_fmt_id(memory_id)}: {similar_results.get('error')}")
                return

            # 4. Process results and create links
            link_count = 0
            # Determine which key holds the similarity score based on the tool used
            score_key = "hybrid_score" if search_tool == TOOL_HYBRID_SEARCH else "similarity"

            for similar_mem_summary in similar_results.get("memories", []):
                # Check shutdown flag frequently during loop
                if self._shutdown_event.is_set(): break

                target_id = similar_mem_summary.get("memory_id")
                similarity_score = similar_mem_summary.get(score_key, 0.0)

                # Skip linking to self
                if not target_id or target_id == memory_id: continue

                # 5. Get details of the potential target memory for richer link type inference
                target_mem_details_result = await self._execute_tool_call_internal(
                    TOOL_GET_MEMORY_BY_ID, {"memory_id": target_id, "include_links": False}, record_action=False
                )
                # Ensure target retrieval succeeded and it's in the same workflow
                if not target_mem_details_result.get("success") or target_mem_details_result.get("workflow_id") != workflow_id:
                    self.logger.debug(f"Skipping link target {_fmt_id(target_id)}: Not found or workflow mismatch.")
                    continue
                target_mem = target_mem_details_result

                # 6. Infer a more specific link type based on memory types
                inferred_link_type = LinkType.RELATED.value # Default link type
                source_type = source_mem.get("memory_type")
                target_type = target_mem.get("memory_type")

                # Example inference rules (can be expanded)
                if source_type == MemoryType.INSIGHT.value and target_type == MemoryType.FACT.value: inferred_link_type = LinkType.SUPPORTS.value
                elif source_type == MemoryType.FACT.value and target_type == MemoryType.INSIGHT.value: inferred_link_type = LinkType.SUPPORTS.value # Or maybe GENERALIZES/SPECIALIZES?
                elif source_type == MemoryType.QUESTION.value and target_type == MemoryType.FACT.value: inferred_link_type = LinkType.REFERENCES.value
                # >>>>> PRESERVED ORIGINAL LINK TYPE RULES <<<<<
                # (Can add more rules here based on analysis)
                # elif source_type == MemoryType.HYPOTHESIS.value and target_type == MemoryType.EVIDENCE.value: inferred_link_type = LinkType.SUPPORTS.value # Assuming evidence supports hypothesis
                # elif source_type == MemoryType.EVIDENCE.value and target_type == MemoryType.HYPOTHESIS.value: inferred_link_type = LinkType.SUPPORTS.value
                # ... add other rules ...

                # 7. Create the link using the UMS tool
                link_tool_name = TOOL_CREATE_LINK
                if not self._find_tool_server(link_tool_name):
                    self.logger.warning(f"Cannot create link: Tool {link_tool_name} unavailable.")
                    break # Stop trying if link tool is missing

                link_args = {
                    # workflow_id is usually inferred by the tool from memory IDs
                    "source_memory_id": memory_id,
                    "target_memory_id": target_id,
                    "link_type": inferred_link_type,
                    "strength": round(similarity_score, 3), # Use similarity score as strength
                    "description": f"Auto-link ({inferred_link_type}) based on similarity ({score_key})"
                }
                link_result = await self._execute_tool_call_internal(
                    link_tool_name, link_args, record_action=False # Don't record link creation as primary action
                )

                # Log success or failure of link creation
                if link_result.get("success"):
                    link_count += 1
                    self.logger.debug(f"Auto-linked memory {_fmt_id(memory_id)} to {_fmt_id(target_id)} ({inferred_link_type}, score: {similarity_score:.2f})")
                else:
                    # Log failure, but continue trying other potential links
                    self.logger.warning(f"Failed to auto-create link {_fmt_id(memory_id)}->{_fmt_id(target_id)}: {link_result.get('error')}")

                # Stop if max links reached for this source memory
                if link_count >= self.auto_linking_max_links:
                    self.logger.debug(f"Reached auto-linking limit ({self.auto_linking_max_links}) for memory {_fmt_id(memory_id)}.")
                    break
                # Small delay between creating links if multiple are found
                await asyncio.sleep(0.1)

        except Exception as e:
            # Catch and log any errors occurring within the background task itself
            self.logger.warning(f"Error in auto-linking task for {_fmt_id(memory_id)}: {e}", exc_info=False)


    # ---------------------------------------------------- promotion helper --
    async def _check_and_trigger_promotion(
        self,
        memory_id: str, # The ID of the memory to check
        *,
        workflow_id: Optional[str], # Snapshotted workflow ID
        context_id: Optional[str], # Snapshotted context ID (unused but passed)
    ):
        """
        Checks if a specific memory meets criteria for promotion to a higher
        cognitive level and calls the UMS tool to perform the promotion if eligible.
        Intended to be run as a background task.
        """
        # (Keep the integrated _check_and_trigger_promotion method logic)
        # Abort if workflow context has changed or shutdown is signaled
        if workflow_id != self.state.workflow_id or self._shutdown_event.is_set():
            self.logger.debug(f"Skipping promotion check for {_fmt_id(memory_id)}: Workflow changed ({_fmt_id(self.state.workflow_id)} vs {_fmt_id(workflow_id)}) or shutdown.")
            return

        promotion_tool_name = TOOL_PROMOTE_MEM # Tool for checking/promoting
        # Basic validation
        if not memory_id or not self._find_tool_server(promotion_tool_name):
            self.logger.debug(f"Skipping promotion check for {_fmt_id(memory_id)}: Invalid ID or tool '{promotion_tool_name}' unavailable.")
            return

        try:
            # Optional slight delay
            await asyncio.sleep(random.uniform(0.1, 0.4))
            # Check shutdown again after sleep
            if self._shutdown_event.is_set(): return

            self.logger.debug(f"Checking promotion potential for memory {_fmt_id(memory_id)} in workflow {_fmt_id(workflow_id)}...")
            # Execute the promotion check tool internally
            # Workflow ID is likely inferred by the tool from memory_id
            promotion_result = await self._execute_tool_call_internal(
                promotion_tool_name, {"memory_id": memory_id}, record_action=False # Don't record check as action
            )

            # Log the outcome of the promotion check
            if promotion_result.get("success"):
                if promotion_result.get("promoted"):
                    # Log successful promotion clearly
                    self.logger.info(f"Memory {_fmt_id(memory_id)} promoted from {promotion_result.get('previous_level')} to {promotion_result.get('new_level')}.", emoji_key="arrow_up")
                else:
                    # Log the reason if promotion didn't occur but check was successful
                    self.logger.debug(f"Memory {_fmt_id(memory_id)} not promoted: {promotion_result.get('reason')}")
            else:
                 # Log if the promotion check tool itself failed
                 self.logger.warning(f"Promotion check tool failed for {_fmt_id(memory_id)}: {promotion_result.get('error')}")

        except Exception as e:
            # Log any exceptions within the promotion check task
            self.logger.warning(f"Error in memory promotion check task for {_fmt_id(memory_id)}: {e}", exc_info=False)


    # ------------------------------------------------------ execute tool call --
    async def _execute_tool_call_internal(
        self,
        tool_name: str,
        arguments: Dict[str, Any],
        record_action: bool = True,
        planned_dependencies: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        Central handler for executing tool calls via MCPClient.

        Includes:
        - Server lookup.
        - Automatic injection of workflow/context IDs.
        - Prerequisite dependency checking.
        - Handling of the internal AGENT_TOOL_UPDATE_PLAN.
        - Optional recording of action start/completion/dependencies.
        - Retry logic for idempotent tools.
        - Result parsing and standardization.
        - **Enhanced error handling/categorization** and state updates (last_error_details).
        - Triggering relevant background tasks (auto-linking, promotion check).
        - Updating last_action_summary state.
        - Handling workflow and **goal stack** side effects.
        """
        # <<< Start Integration Block: Enhance _execute_tool_call_internal (Goal Stack Side Effects) >>>
        # --- Step 1: Server Lookup ---
        target_server = self._find_tool_server(tool_name)
        # Handle case where tool server is not found, except for the internal agent tool
        if not target_server and tool_name != AGENT_TOOL_UPDATE_PLAN:
            err = f"Tool server unavailable for {tool_name}"
            self.logger.error(err)
            # Set error details for the main loop to see - Enhanced Category
            self.state.last_error_details = {"tool": tool_name, "error": err, "type": "ServerUnavailable", "status_code": 503}
            # Return a failure dictionary consistent with other error returns
            return {"success": False, "error": err, "status_code": 503} # 503 Service Unavailable

        # --- Step 2: Context Injection ---
        # Get current workflow context IDs from state
        current_wf_id = (self.state.workflow_stack[-1] if self.state.workflow_stack else self.state.workflow_id)
        current_ctx_id = self.state.context_id
        current_goal_id = self.state.current_goal_id # Get current goal ID

        # Make a copy to avoid modifying the original arguments dict passed in
        final_arguments = arguments.copy()
        # Inject workflow_id if missing and relevant for the tool
        if (
            final_arguments.get("workflow_id") is None # Only if not already provided
            and current_wf_id # Only if a workflow is active
            and tool_name # Check if tool_name is valid
            not in { # Exclude tools that don't operate on a specific workflow
                TOOL_CREATE_WORKFLOW, # Creates a new one
                TOOL_LIST_WORKFLOWS, # Lists across potentially many
                "core:list_servers", # Core MCP tool
                "core:get_tool_schema", # Core MCP tool
                AGENT_TOOL_UPDATE_PLAN, # Internal agent tool
                TOOL_PUSH_SUB_GOAL, # Assumes UMS handles workflow association
                TOOL_MARK_GOAL_STATUS, # Uses goal_id, workflow inferred
                TOOL_GET_GOAL_DETAILS, # Uses goal_id
            }
        ):
            final_arguments["workflow_id"] = current_wf_id
        # Inject context_id if missing and relevant for the tool
        if (
            final_arguments.get("context_id") is None # Only if not already provided
            and current_ctx_id # Only if a context ID is set
            and tool_name # Check if tool_name is valid
            in { # Tools known to operate on a specific cognitive context ID
                TOOL_GET_WORKING_MEMORY,
                TOOL_OPTIMIZE_WM,
                TOOL_AUTO_FOCUS,
                # Add others here if they accept/require context_id
            }
        ):
            final_arguments["context_id"] = current_ctx_id
        # Inject current thought chain ID for thought recording if not specified
        if (
            final_arguments.get("thought_chain_id") is None # Only if not already provided
            and self.state.current_thought_chain_id # Only if a chain ID is set
            and tool_name == TOOL_RECORD_THOUGHT # Only for the record_thought tool
        ):
            final_arguments["thought_chain_id"] = self.state.current_thought_chain_id
        # --- ADDED: Inject parent goal ID for push_sub_goal if not provided ---
        if (
             final_arguments.get("parent_goal_id") is None # Only if not already provided
             and current_goal_id # Only if a goal is currently active
             and tool_name == TOOL_PUSH_SUB_GOAL # Only for this tool
        ):
             final_arguments["parent_goal_id"] = current_goal_id

        # --- Step 3: Dependency Check ---
        # If dependencies were declared for this action, check them first
        if planned_dependencies:
            ok, reason = await self._check_prerequisites(planned_dependencies)
            if not ok:
                # If dependencies not met, log warning, set error state, and return failure
                err_msg = f"Prerequisites not met for {tool_name}: {reason}"
                self.logger.warning(err_msg)
                # Store detailed error info for the LLM - Enhanced Category
                self.state.last_error_details = {"tool": tool_name, "error": err_msg, "type": "DependencyNotMetError", "dependencies": planned_dependencies, "status_code": 412}
                # Signal that replanning is needed due to dependency failure
                self.state.needs_replan = True
                return {"success": False, "error": err_msg, "status_code": 412} # 412 Precondition Failed
            else:
                # Log if dependencies were checked and met
                self.logger.info(f"Prerequisites {[_fmt_id(dep) for dep in planned_dependencies]} met for {tool_name}.")

        # --- Step 4: Handle Internal Agent Tool ---
        # Directly handle the AGENT_TOOL_UPDATE_PLAN without calling MCPClient
        if tool_name == AGENT_TOOL_UPDATE_PLAN:
            try:
                new_plan_data = final_arguments.get("plan", [])
                # Validate the structure of the provided plan data
                if not isinstance(new_plan_data, list):
                    raise ValueError("`plan` argument must be a list of step objects.")
                # Convert list of dicts to list of PlanStep objects (validates structure)
                validated_plan = [PlanStep(**p) for p in new_plan_data]

                # --- Plan Cycle Detection ---
                if self._detect_plan_cycle(validated_plan):
                    err_msg = "Proposed plan contains a dependency cycle."
                    self.logger.error(err_msg)
                    self.state.last_error_details = {"tool": tool_name, "error": err_msg, "type": "PlanValidationError", "proposed_plan": new_plan_data}
                    self.state.needs_replan = True # Force replan again
                    return {"success": False, "error": err_msg}

                # Replace the agent's current plan
                self.state.current_plan = validated_plan
                # Plan was explicitly updated, so replan flag can be cleared
                self.state.needs_replan = False
                self.logger.info(f"Internal plan update successful. New plan has {len(validated_plan)} steps.")
                # Clear any previous errors after a successful plan update
                self.state.last_error_details = None
                self.state.consecutive_error_count = 0
                return {"success": True, "message": f"Plan updated with {len(validated_plan)} steps."}
            except (ValidationError, TypeError, ValueError) as e:
                # Handle errors during plan validation or application
                err_msg = f"Failed to validate/apply new plan: {e}"
                self.logger.error(err_msg)
                # Store error details for the LLM - Enhanced Category
                self.state.last_error_details = {"tool": tool_name, "error": err_msg, "type": "PlanUpdateError", "proposed_plan": final_arguments.get("plan")}
                # Increment error count for internal failures too? Decide policy. Yes, for now.
                self.state.consecutive_error_count += 1
                # Failed plan update requires another attempt at planning
                self.state.needs_replan = True
                return {"success": False, "error": err_msg}

        # --- Step 5: Record Action Start (Optional) ---
        action_id: Optional[str] = None
        # Determine if this tool call should be recorded as a primary agent action
        # Exclude internal/meta tools and calls where record_action is explicitly False
        should_record = record_action and tool_name not in self._INTERNAL_OR_META_TOOLS
        if should_record:
            # Call internal helper to record the action start and dependencies
            action_id = await self._record_action_start_internal(
                 tool_name, final_arguments, planned_dependencies # Pass potentially modified args and dependencies
            )
            # Note: _record_action_start_internal now handles calling _record_action_dependencies_internal

        # --- Step 6: Execute Tool Call (with Retries) ---
        # Define the actual async function to call the tool via MCPClient
        async def _do_call():
            # Ensure None values are stripped *before* sending to MCPClient execute_tool
            # Although MCPClient likely handles this, this adds robustness.
            call_args = {k: v for k, v in final_arguments.items() if v is not None}
            # Target server must be valid here because AGENT_INTERNAL was handled earlier
            return await self.mcp_client.execute_tool(target_server, tool_name, call_args)

        # Get the stats dictionary for this specific tool
        record_stats = self.state.tool_usage_stats[tool_name]
        # Decide if the tool is safe to automatically retry on failure
        idempotent = tool_name in {
            # Read-only operations are generally safe to retry
            TOOL_GET_CONTEXT, TOOL_GET_MEMORY_BY_ID, TOOL_SEMANTIC_SEARCH,
            TOOL_HYBRID_SEARCH, TOOL_GET_ACTION_DETAILS, TOOL_LIST_WORKFLOWS,
            TOOL_COMPUTE_STATS, TOOL_GET_WORKING_MEMORY, TOOL_GET_LINKED_MEMORIES,
            TOOL_GET_ARTIFACTS, TOOL_GET_ARTIFACT_BY_ID, TOOL_GET_ACTION_DEPENDENCIES,
            TOOL_GET_THOUGHT_CHAIN, TOOL_GET_WORKFLOW_DETAILS,
            TOOL_GET_GOAL_DETAILS, # Getting goal details is idempotent (NEW)
            # Some meta operations might be considered retry-safe
            TOOL_SUMMARIZE_TEXT,
        }

        start_ts = time.time() # Record start time for latency calculation
        res = {} # Initialize result dictionary

        try:
            # Execute the tool call using the retry wrapper
            raw = await self._with_retries(
                _do_call,
                max_retries=3 if idempotent else 1, # Retry only idempotent tools (3 attempts total)
                # Specify exceptions that should trigger a retry attempt
                retry_exceptions=(
                    ToolError, ToolInputError, # Specific MCP errors
                    asyncio.TimeoutError, ConnectionError, # Common network issues
                    APIConnectionError, RateLimitError, APIStatusError, # Anthropic/LLM network/API issues
                ),
            )
            # Calculate execution latency
            latency_ms = (time.time() - start_ts) * 1000
            record_stats["latency_ms_total"] += latency_ms

            # --- Step 7: Process and Standardize Result ---
            # Handle different result formats returned by MCPClient/tools
            if isinstance(raw, dict) and ("success" in raw or "isError" in raw):
                # Assume standard MCP result format with success/isError flag
                is_error = raw.get("isError", not raw.get("success", True))
                # Extract content or error message
                content = raw.get("content", raw.get("error", raw.get("data")))
                if is_error:
                    res = {"success": False, "error": str(content), "status_code": raw.get("status_code")}
                else:
                    # If content itself has a standard structure, use it directly
                    if isinstance(content, dict) and "success" in content:
                        res = content
                    else: # Otherwise, wrap the content under a 'data' key for consistency
                        res = {"success": True, "data": content}
            elif isinstance(raw, dict): # Handle plain dictionaries without explicit success/isError
                 # Assume success if no error indicators present
                 res = {"success": True, "data": raw}
            else:
                # Handle non-dict results (e.g., simple strings, numbers, booleans)
                res = {"success": True, "data": raw}

            # --- Step 8: State Updates and Background Triggers on SUCCESS ---
            if res.get("success"):
                # Update success stats for the tool
                record_stats["success"] += 1

                # --- Background Triggers Integration ---
                # Snapshot the workflow ID *before* potentially starting background tasks
                # (This ensures tasks operate on the workflow active during the trigger event)
                current_wf_id_snapshot = self.state.workflow_stack[-1] if self.state.workflow_stack else self.state.workflow_id  # noqa: F841

                # Trigger auto-linking after storing/updating a memory or recording an artifact with a linked memory
                if tool_name in [TOOL_STORE_MEMORY, TOOL_UPDATE_MEMORY] and res.get("memory_id"):
                    mem_id = res["memory_id"]
                    self.logger.debug(f"Queueing auto-link check for memory {_fmt_id(mem_id)}")
                    # Start background task, passing the memory ID
                    self._start_background_task(AgentMasterLoop._run_auto_linking, memory_id=mem_id)
                    # Note: _start_background_task automatically snapshots workflow_id/context_id
                if tool_name == TOOL_RECORD_ARTIFACT and res.get("linked_memory_id"):
                    linked_mem_id = res["linked_memory_id"]
                    self.logger.debug(f"Queueing auto-link check for memory linked to artifact: {_fmt_id(linked_mem_id)}")
                    self._start_background_task(AgentMasterLoop._run_auto_linking, memory_id=linked_mem_id)

                # Trigger promotion check after retrieving memories
                if tool_name in [TOOL_GET_MEMORY_BY_ID, TOOL_QUERY_MEMORIES, TOOL_HYBRID_SEARCH, TOOL_SEMANTIC_SEARCH, TOOL_GET_WORKING_MEMORY]:
                    mem_ids_to_check = set() # Use set to avoid duplicate checks
                    potential_mems = []
                    # Extract memory IDs from various possible result structures
                    if tool_name == TOOL_GET_MEMORY_BY_ID:
                        # Result might be the memory dict directly or nested under 'data'
                        mem_data = res if "memory_id" in res else res.get("data", {})
                        if isinstance(mem_data, dict): potential_mems = [mem_data]
                    elif tool_name == TOOL_GET_WORKING_MEMORY:
                        potential_mems = res.get("working_memories", [])
                        # Also check the focal memory if present
                        focus_id = res.get("focal_memory_id")
                        if focus_id: mem_ids_to_check.add(focus_id)
                    else: # Query/Search results typically under 'memories' key
                         potential_mems = res.get("memories", [])

                    # Add IDs from the list/dict structures found
                    if isinstance(potential_mems, list):
                        # Limit checks to the top few most relevant results to avoid overload
                        mem_ids_to_check.update(
                            m.get("memory_id") for m in potential_mems[:3] # Check top 3 retrieved
                            if isinstance(m, dict) and m.get("memory_id") # Ensure it's a dict with an ID
                        )

                    # Start background tasks for each unique, valid memory ID found
                    for mem_id in filter(None, mem_ids_to_check): # Filter out any None IDs
                         self.logger.debug(f"Queueing promotion check for retrieved memory {_fmt_id(mem_id)}")
                         self._start_background_task(AgentMasterLoop._check_and_trigger_promotion, memory_id=mem_id)
                # --- Background Triggers Integration End ---

                # Update current thought chain ID if a new one was just created successfully
                if tool_name == TOOL_CREATE_THOUGHT_CHAIN and res.get("success"):
                    # Find the chain ID in the result (might be root or under 'data')
                    chain_data = res if "thought_chain_id" in res else res.get("data", {})
                    if isinstance(chain_data, dict):
                        new_chain_id = chain_data.get("thought_chain_id")
                        if new_chain_id:
                            self.state.current_thought_chain_id = new_chain_id
                            self.logger.info(f"Switched current thought chain to newly created: {_fmt_id(new_chain_id)}")

            else: # Tool failed
                # Update failure stats
                record_stats["failure"] += 1
                # Ensure error details are captured from the result for the LLM context
                # Enhance error details with categorization
                error_type = "ToolExecutionError" # Default category
                status_code = res.get("status_code")
                error_message = res.get("error", "Unknown failure")
                if status_code == 412: error_type = "DependencyNotMetError"
                elif status_code == 503: error_type = "ServerUnavailable"
                elif "input" in str(error_message).lower() or "validation" in str(error_message).lower(): error_type = "InvalidInputError" # Basic keyword check
                elif "timeout" in str(error_message).lower(): error_type = "NetworkError" # Assuming timeout implies network
                # --- ADDED: Categorize goal management errors ---
                elif tool_name in [TOOL_PUSH_SUB_GOAL, TOOL_MARK_GOAL_STATUS] and ("not found" in str(error_message).lower() or "invalid" in str(error_message).lower()):
                     error_type = "GoalManagementError"

                self.state.last_error_details = {
                    "tool": tool_name,
                    "args": arguments, # Log the arguments that caused failure
                    "error": error_message,
                    "status_code": status_code,
                    "type": error_type # Store the categorized error type
                }
                # Log the categorized error
                self.logger.warning(f"Tool {tool_name} failed. Type: {error_type}, Error: {error_message}")


            # --- Step 9: Update Last Action Summary ---
            # Create a concise summary of the action's outcome for the next prompt
            summary = ""
            if res.get("success"):
                # Try to find a meaningful summary field in the result or its 'data' payload
                summary_keys = ["summary", "message", "memory_id", "action_id", "artifact_id", "link_id", "chain_id", "state_id", "report", "visualization", "goal_id"] # Added goal_id
                data_payload = res.get("data", res) # Check 'data' key or root level
                if isinstance(data_payload, dict):
                    for k in summary_keys:
                        if k in data_payload and data_payload[k]:
                            # Format IDs concisely, use string value for others
                            summary = f"{k}: {_fmt_id(data_payload[k]) if 'id' in k else str(data_payload[k])}"
                            break
                    else: # Fallback if no specific key found in dict
                        data_str = str(data_payload)[:70] # Preview the dict
                        summary = f"Success. Data: {data_str}..." if len(str(data_payload)) > 70 else f"Success. Data: {data_str}"
                else: # Handle non-dict data payload
                     data_str = str(data_payload)[:70] # Preview the data
                     summary = f"Success. Data: {data_str}..." if len(str(data_payload)) > 70 else f"Success. Data: {data_str}"
            else: # If failed
                # Use the structured error details for a more informative summary
                err_type = self.state.last_error_details.get("type", "Unknown") if self.state.last_error_details else "Unknown"
                err_msg = str(res.get('error', 'Unknown Error'))[:100]
                summary = f"Failed ({err_type}): {err_msg}" # Include error type
                if res.get('status_code'): summary += f" (Code: {res['status_code']})" # Add status code if available

            # Update the state variable
            self.state.last_action_summary = f"{tool_name} -> {summary}"
            # Log the outcome
            self.logger.info(self.state.last_action_summary, emoji_key="checkered_flag" if res.get('success') else "warning")


        # --- Step 10: Exception Handling for Tool Call/Retries ---
        # Updated Exception handling to add error categorization
        except (ToolError, ToolInputError) as e:
            # Handle specific MCP exceptions caught during execution or retries
            err_str = str(e); status_code = getattr(e, 'status_code', None)
            # Determine category based on type and status code
            error_type = "InvalidInputError" if isinstance(e, ToolInputError) else "ToolInternalError"
            if status_code == 412: error_type = "DependencyNotMetError"
            self.logger.error(f"Tool Error executing {tool_name}: {err_str}", exc_info=False) # Don't need full trace for these
            res = {"success": False, "error": err_str, "status_code": status_code}
            record_stats["failure"] += 1 # Record failure
            # Store categorized error details for the LLM
            self.state.last_error_details = {"tool": tool_name, "args": arguments, "error": err_str, "type": error_type, "status_code": status_code}
            self.state.last_action_summary = f"{tool_name} -> Failed ({error_type}): {err_str[:100]}"

        except APIConnectionError as e:
            err_str = f"LLM API Connection Error: {e}"
            self.logger.error(err_str, exc_info=False)
            res = {"success": False, "error": err_str}
            record_stats["failure"] += 1
            self.state.last_error_details = {"tool": tool_name, "args": arguments, "error": err_str, "type": "NetworkError"}
            self.state.last_action_summary = f"{tool_name} -> Failed: NetworkError"
        except RateLimitError as e:
            err_str = f"LLM Rate Limit Error: {e}"
            self.logger.error(err_str, exc_info=False)
            res = {"success": False, "error": err_str}
            record_stats["failure"] += 1
            self.state.last_error_details = {"tool": tool_name, "args": arguments, "error": err_str, "type": "APILimitError"}
            self.state.last_action_summary = f"{tool_name} -> Failed: APILimitError"
        except APIStatusError as e:
            err_str = f"LLM API Error {e.status_code}: {e.message}"
            self.logger.error(f"Anthropic API status error: {e.status_code} - {e.response}", exc_info=False)
            res = {"success": False, "error": err_str, "status_code": e.status_code}
            record_stats["failure"] += 1
            self.state.last_error_details = {"tool": tool_name, "args": arguments, "error": err_str, "type": "APIError", "status_code": e.status_code}
            self.state.last_action_summary = f"{tool_name} -> Failed: APIError ({e.status_code})"
        except asyncio.TimeoutError as e: # Catch timeouts from retry wrapper or internal calls
            err_str = f"Operation timed out: {e}"
            self.logger.error(f"Timeout executing {tool_name}: {err_str}", exc_info=False)
            res = {"success": False, "error": err_str}
            record_stats["failure"] += 1
            self.state.last_error_details = {"tool": tool_name, "args": arguments, "error": err_str, "type": "TimeoutError"}
            self.state.last_action_summary = f"{tool_name} -> Failed: Timeout"

        except asyncio.CancelledError:
             # Handle task cancellation gracefully (e.g., due to shutdown signal)
             err_str = "Tool execution cancelled."
             self.logger.warning(f"{tool_name} execution was cancelled.")
             res = {"success": False, "error": err_str, "status_code": 499} # Use 499 Client Closed Request
             record_stats["failure"] += 1 # Count cancellation as failure for stats
             self.state.last_error_details = {"tool": tool_name, "args": arguments, "error": err_str, "type": "CancelledError"}
             self.state.last_action_summary = f"{tool_name} -> Cancelled"
             # Re-raise cancellation to potentially stop the loop if needed
             raise

        except Exception as e:
            # Catch any other unexpected errors during execution or retries
            err_str = str(e)
            self.logger.error(f"Unexpected Error executing {tool_name}: {err_str}", exc_info=True) # Log full traceback
            res = {"success": False, "error": f"Unexpected error: {err_str}"}
            record_stats["failure"] += 1 # Record failure
            # Store categorized error
            self.state.last_error_details = {"tool": tool_name, "args": arguments, "error": err_str, "type": "UnexpectedExecutionError"}
            self.state.last_action_summary = f"{tool_name} -> Failed: Unexpected error."

        # --- Step 11: Record Action Completion (if start was recorded) ---
        if action_id:
            # Pass the final result 'res' (success or failure dict) to the completion recorder
            await self._record_action_completion_internal(action_id, res)

        # --- Step 12: Handle Workflow & Goal Side Effects ---
        # Call this *after* execution and completion recording, using the final 'res'
        await self._handle_workflow_and_goal_side_effects(tool_name, final_arguments, res)

        # Return the final processed result dictionary
        return res
        # <<< End Integration Block: Enhance _execute_tool_call_internal >>>


    async def _handle_workflow_and_goal_side_effects(self, tool_name: str, arguments: Dict, result_content: Dict):
        """
        Handles agent state changes triggered by specific tool outcomes,
        including workflow creation/termination and goal stack updates.
        """
        # <<< Start Integration Block: Goal Stack Side Effects >>>
        # --- Side effects for Workflow Creation ---
        if tool_name == TOOL_CREATE_WORKFLOW and result_content.get("success"):
            new_wf_id = result_content.get("workflow_id")
            primary_chain_id = result_content.get("primary_thought_chain_id")
            parent_wf_id = arguments.get("parent_workflow_id") # Parent WF from original args
            wf_title = result_content.get("title", "Untitled Workflow")
            wf_goal_desc = result_content.get("goal", "Achieve objectives")

            if new_wf_id:
                # Set the agent's primary workflow ID and context ID to the new one
                self.state.workflow_id = new_wf_id
                self.state.context_id = new_wf_id # Align context ID

                # Manage the workflow stack
                is_sub_workflow = parent_wf_id and parent_wf_id in self.state.workflow_stack
                if is_sub_workflow:
                    self.state.workflow_stack.append(new_wf_id)
                    log_prefix = "sub-"
                else: # New root workflow or parent not on stack
                    self.state.workflow_stack = [new_wf_id]
                    log_prefix = "new "

                # Set the current thought chain ID
                self.state.current_thought_chain_id = primary_chain_id

                # --- Goal Stack Update for New Workflow ---
                # If it's a new root workflow, reset the goal stack and create a root goal
                if not is_sub_workflow:
                    self.state.goal_stack = []
                    self.state.current_goal_id = None
                    # Try to create a root goal in the UMS for this new workflow
                    if self._find_tool_server(TOOL_PUSH_SUB_GOAL):
                        try:
                            goal_res = await self._execute_tool_call_internal(
                                TOOL_PUSH_SUB_GOAL,
                                {
                                    "workflow_id": new_wf_id,
                                    "description": wf_goal_desc,
                                    # Parent is None for the root goal
                                },
                                record_action=False
                            )
                            if goal_res.get("success") and goal_res.get("goal_id"):
                                new_goal = {"goal_id": goal_res["goal_id"], "description": wf_goal_desc, "status": "active"}
                                self.state.goal_stack.append(new_goal)
                                self.state.current_goal_id = new_goal["goal_id"]
                                self.logger.info(f"Created root goal {_fmt_id(self.state.current_goal_id)} for {log_prefix}workflow {_fmt_id(new_wf_id)}.")
                            else:
                                self.logger.warning(f"Failed to create root goal via UMS for new workflow {_fmt_id(new_wf_id)}: {goal_res.get('error')}")
                        except Exception as goal_err:
                             self.logger.error(f"Error creating root goal for new workflow {_fmt_id(new_wf_id)}: {goal_err}")
                    else:
                        self.logger.warning(f"Cannot create root goal for new workflow: Tool {TOOL_PUSH_SUB_GOAL} unavailable.")
                # If it's a sub-workflow, the goal that triggered its creation should already be on the stack.
                # We don't automatically create a new goal here; the LLM should explicitly push one if needed.

                self.logger.info(f"Switched to {log_prefix}workflow: {_fmt_id(new_wf_id)}. Current chain: {_fmt_id(primary_chain_id)}. Current goal: {_fmt_id(self.state.current_goal_id)}", emoji_key="label")

                # Reset plan, errors, and replan flag for the new workflow context
                self.state.current_plan = [PlanStep(description=f"Start {log_prefix}workflow: '{wf_title}'. Goal: {wf_goal_desc}.")]
                self.state.consecutive_error_count = 0
                self.state.needs_replan = False
                self.state.last_error_details = None

        # --- Side effects for Pushing a Sub-Goal ---
        elif tool_name == TOOL_PUSH_SUB_GOAL and result_content.get("success"):
            new_goal = result_content.get("goal") # Assuming tool returns the created goal object
            if isinstance(new_goal, dict) and new_goal.get("goal_id"):
                 # Add the new goal to the agent's state stack
                 self.state.goal_stack.append(new_goal)
                 # Set the new goal as the current goal
                 self.state.current_goal_id = new_goal["goal_id"]
                 self.logger.info(f"Pushed new sub-goal {_fmt_id(self.state.current_goal_id)} onto stack: '{new_goal.get('description', '')[:50]}...'. Stack depth: {len(self.state.goal_stack)}", emoji_key="arrow_down")
                 # Force replan to address the new sub-goal
                 self.state.needs_replan = True
                 self.state.current_plan = [PlanStep(description=f"Start new sub-goal: '{new_goal.get('description', '')[:50]}...'")]
                 self.state.last_error_details = None # Clear errors when pushing new goal
            else:
                 self.logger.warning(f"Tool {TOOL_PUSH_SUB_GOAL} succeeded but did not return valid goal data: {result_content}")

        # --- Side effects for Marking Goal Status ---
        elif tool_name == TOOL_MARK_GOAL_STATUS and result_content.get("success"):
            goal_id_marked = arguments.get("goal_id")
            new_status = arguments.get("status")

            # Update the goal status within the agent's state stack
            goal_found_in_stack = False
            for goal in self.state.goal_stack:
                if goal.get("goal_id") == goal_id_marked:
                    goal["status"] = new_status
                    goal_found_in_stack = True
                    self.logger.info(f"Updated goal {_fmt_id(goal_id_marked)} status in state stack to: {new_status}.")
                    break

            if not goal_found_in_stack:
                 self.logger.warning(f"Goal {_fmt_id(goal_id_marked)} (marked {new_status}) not found in current agent goal stack: {self.state.goal_stack}")

            # If the goal marked was the *current* goal and it's now completed/failed
            if goal_id_marked == self.state.current_goal_id and new_status in ["completed", "failed"]:
                # Pop the completed/failed goal from the stack
                if self.state.goal_stack and self.state.goal_stack[-1].get("goal_id") == goal_id_marked:
                    finished_goal = self.state.goal_stack.pop()
                    self.logger.info(f"Popped goal {_fmt_id(finished_goal.get('goal_id'))} (status: {new_status}) from stack.")
                else:
                     self.logger.warning(f"Attempted to pop goal {_fmt_id(goal_id_marked)}, but it wasn't at the top of the stack.")

                # Set the new current goal to the one now at the top
                self.state.current_goal_id = self.state.goal_stack[-1].get("goal_id") if self.state.goal_stack else None
                self.logger.info(f"Returning focus to goal: {_fmt_id(self.state.current_goal_id) if self.state.current_goal_id else 'Overall Goal'}. Stack depth: {len(self.state.goal_stack)}", emoji_key="arrow_up")

                # Check if the stack is now empty (meaning the root goal was completed/failed)
                if not self.state.goal_stack:
                     self.logger.info("Goal stack empty. Overall goal presumed finished.")
                     # Set overall goal achieved flag *only if the last goal was completed*
                     self.state.goal_achieved_flag = (new_status == "completed")
                     # Optionally update workflow status if stack is empty
                     if self.state.workflow_id and self._find_tool_server(TOOL_UPDATE_WORKFLOW_STATUS):
                         wf_status = WorkflowStatus.COMPLETED.value if self.state.goal_achieved_flag else WorkflowStatus.FAILED.value
                         await self._execute_tool_call_internal(
                             TOOL_UPDATE_WORKFLOW_STATUS,
                             {
                                 "workflow_id": self.state.workflow_id,
                                 "status": wf_status,
                                 "completion_message": f"Overall goal marked {wf_status} after stack completion."
                             },
                             record_action=False
                         )
                     # Clear plan when overall goal is done
                     self.state.current_plan = []
                else:
                    # Force replan to address the parent goal after returning from sub-goal
                    self.state.needs_replan = True
                    self.state.current_plan = [PlanStep(description=f"Returned from sub-goal {_fmt_id(goal_id_marked)} (status: {new_status}). Re-assess current goal: {_fmt_id(self.state.current_goal_id)}.")]
                self.state.last_error_details = None # Clear error details when goal status changes

        # --- Side effects for Workflow Status Update (Completion/Failure/Abandonment) ---
        elif tool_name == TOOL_UPDATE_WORKFLOW_STATUS and result_content.get("success"):
            status = arguments.get("status") # Status requested in the tool call
            wf_id_updated = arguments.get("workflow_id") # Workflow that was updated

            # Check if the *currently active* workflow (top of stack) was the one updated
            if wf_id_updated and self.state.workflow_stack and wf_id_updated == self.state.workflow_stack[-1]:
                # Check if the new status is a terminal one
                is_terminal = status in [
                    WorkflowStatus.COMPLETED.value,
                    WorkflowStatus.FAILED.value,
                    WorkflowStatus.ABANDONED.value
                ]

                if is_terminal:
                    # Remove the finished workflow from the stack
                    finished_wf = self.state.workflow_stack.pop()
                    parent_wf_id = self.state.workflow_stack[-1] if self.state.workflow_stack else None

                    # --- Goal Stack Update for Completed Sub-Workflow ---
                    # Find the goal on the *parent's* stack that likely corresponds to this finished sub-workflow.
                    # This is heuristic - assumes the goal *currently active* when the sub-workflow finished is the relevant one.
                    # A more robust system might store the initiating goal ID when the sub-workflow is created.
                    corresponding_goal_id = self.state.current_goal_id # Goal active *before* popping WF stack
                    if corresponding_goal_id and parent_wf_id: # Ensure we have a goal and a parent WF to mark it in
                        # Infer goal status from workflow status
                        goal_status_to_set = "completed" if status == WorkflowStatus.COMPLETED.value else "failed"
                        self.logger.info(f"Attempting to mark goal {_fmt_id(corresponding_goal_id)} as {goal_status_to_set} due to sub-workflow {_fmt_id(finished_wf)} completion.")
                        if self._find_tool_server(TOOL_MARK_GOAL_STATUS):
                             mark_res = await self._execute_tool_call_internal(
                                 TOOL_MARK_GOAL_STATUS,
                                 {
                                     "goal_id": corresponding_goal_id,
                                     "status": goal_status_to_set,
                                     "reason": f"Sub-workflow {_fmt_id(finished_wf)} finished with status: {status}"
                                 },
                                 record_action=False
                             )
                             # This call will trigger the goal stack popping logic above if successful
                             if not mark_res.get("success"):
                                 self.logger.warning(f"Failed to automatically mark goal {_fmt_id(corresponding_goal_id)} after sub-workflow completion: {mark_res.get('error')}")
                        else:
                             self.logger.warning(f"Cannot mark goal status after sub-workflow completion: Tool {TOOL_MARK_GOAL_STATUS} unavailable.")
                    elif corresponding_goal_id and not parent_wf_id:
                         self.logger.info(f"Root workflow {_fmt_id(finished_wf)} finished. Corresponding goal {_fmt_id(corresponding_goal_id)} likely represents overall completion.")
                         # The MARK_GOAL_STATUS logic above should handle the root goal completion and set goal_achieved_flag
                    else:
                         self.logger.warning(f"Sub-workflow {_fmt_id(finished_wf)} finished, but couldn't identify corresponding goal to mark.")


                    # --- Update Agent State Based on Workflow Stack ---
                    if parent_wf_id:
                        # If there's a parent workflow remaining on the stack, return to it
                        self.state.workflow_id = parent_wf_id
                        self.state.context_id = self.state.workflow_id # Realign context ID
                        # Set thought chain (should align with parent)
                        await self._set_default_thought_chain_id()
                        # Note: Current goal ID should have been updated by the MARK_GOAL_STATUS call above
                        self.logger.info(f"Sub-workflow {_fmt_id(finished_wf)} finished ({status}). Returning to parent {_fmt_id(self.state.workflow_id)}. Current chain: {_fmt_id(self.state.current_thought_chain_id)}. Current Goal: {_fmt_id(self.state.current_goal_id)}", emoji_key="arrow_left")
                        # Force replan in the parent context
                        self.state.needs_replan = True
                        self.state.current_plan = [PlanStep(description=f"Returned from sub-workflow {_fmt_id(finished_wf)} (status: {status}). Re-assess current goal: {_fmt_id(self.state.current_goal_id)}.")]
                        self.state.last_error_details = None # Clear error details from the sub-task
                    else:
                        # If the stack is empty, the root workflow finished
                        self.logger.info(f"Root workflow {_fmt_id(finished_wf)} finished with status: {status}.")
                        # Clear active workflow state
                        self.state.workflow_id = None
                        self.state.context_id = None
                        self.state.current_thought_chain_id = None
                        # The goal_achieved_flag should have been set by the MARK_GOAL_STATUS logic when the stack became empty
                        # Clear the plan as the workflow is over
                        self.state.current_plan = []
        # <<< End Integration Block: Goal Stack Side Effects >>>


    async def _apply_heuristic_plan_update(self, last_decision: Dict[str, Any], last_tool_result_content: Optional[Dict[str, Any]] = None):
        """
        Applies heuristic updates to the plan based on the last action's outcome
        when the LLM doesn't explicitly call `agent:update_plan`.

        This acts as a default progression mechanism. It marks steps completed
        on success, handles failures by marking the step failed and inserting
        an analysis step, and updates meta-cognitive counters.
        """
        # <<< Start Integration Block: Heuristic Plan Update Method (Phase 1, Step 3) >>>
        self.logger.info("Applying heuristic plan update (fallback)...", emoji_key="clipboard")

        # Handle case where plan might be empty (shouldn't usually happen)
        if not self.state.current_plan:
            self.logger.warning("Plan is empty during heuristic update, adding default re-evaluation step.")
            self.state.current_plan = [PlanStep(description="Fallback: Re-evaluate situation.")]
            self.state.needs_replan = True # Force replan if plan was empty
            return

        # Get the step the agent was working on (assumed to be the first)
        current_step = self.state.current_plan[0]
        decision_type = last_decision.get("decision") # What the LLM decided to do

        action_successful = False # Flag to track if the action succeeded for counter updates
        tool_name_executed = last_decision.get("tool_name") # Tool name if a tool was called

        # --- Update plan based on decision type and success ---
        # Case 1: LLM called a tool (and it wasn't AGENT_TOOL_UPDATE_PLAN)
        if decision_type == "call_tool" and tool_name_executed != AGENT_TOOL_UPDATE_PLAN:
            # Check the success status from the tool execution result
            tool_success = isinstance(last_tool_result_content, dict) and last_tool_result_content.get("success", False)
            action_successful = tool_success

            if tool_success:
                # On success, mark step completed and remove from plan
                current_step.status = ActionStatus.COMPLETED.value # Use enum value
                # Generate a concise summary for the plan step
                summary = "Success."
                if isinstance(last_tool_result_content, dict):
                     # Prioritize specific meaningful keys from the result
                     summary_keys = ["summary", "message", "memory_id", "action_id", "artifact_id", "link_id", "chain_id", "state_id", "report", "visualization", "goal_id"] # Added goal_id
                     data_payload = last_tool_result_content.get("data", last_tool_result_content) # Look in 'data' or root
                     if isinstance(data_payload, dict):
                         for k in summary_keys:
                              if k in data_payload and data_payload[k]:
                                   # Format IDs or use string representation
                                   summary = f"{k}: {_fmt_id(data_payload[k]) if 'id' in k else str(data_payload[k])}"
                                   break
                         else: # Fallback preview for dicts
                              data_str = str(data_payload)[:70]
                              summary = f"Success. Data: {data_str}..." if len(str(data_payload)) > 70 else f"Success. Data: {data_str}"
                     else: # Handle non-dict data payload
                          data_str = str(data_payload)[:70]
                          summary = f"Success. Data: {data_str}..." if len(str(data_payload)) > 70 else f"Success. Data: {data_str}"

                current_step.result_summary = summary[:150] # Add summary to step, truncated
                self.state.current_plan.pop(0) # Remove completed step from the front
                # If the plan is now empty, add a final analysis step
                if not self.state.current_plan:
                    self.logger.info("Plan completed. Adding final analysis step.")
                    self.state.current_plan.append(PlanStep(description="Plan finished. Analyze overall result and decide if goal is met."))
                self.state.needs_replan = False # Success usually doesn't require immediate replan
            else: # Tool failed
                current_step.status = ActionStatus.FAILED.value # Mark step as failed
                # Extract error message for summary
                error_msg = "Unknown failure"
                if isinstance(last_tool_result_content, dict):
                     # Use the enhanced error details if available
                     error_details = self.state.last_error_details
                     if error_details:
                         error_msg = f"Type: {error_details.get('type', 'Unknown')}, Msg: {error_details.get('error', 'Unknown')}"
                     else: # Fallback to basic error message
                         error_msg = str(last_tool_result_content.get('error', 'Unknown failure'))

                current_step.result_summary = f"Failure: {error_msg[:150]}" # Add error summary
                # Keep the failed step in the plan for context.
                # Insert an analysis step *after* the failed step, if one isn't already there.
                if len(self.state.current_plan) < 2 or not self.state.current_plan[1].description.startswith("Analyze failure of step"):
                    self.state.current_plan.insert(1, PlanStep(description=f"Analyze failure of step '{current_step.description[:30]}...' and replan."))
                self.state.needs_replan = True # Failure always requires replanning

        # Case 2: LLM decided to record a thought
        elif decision_type == "thought_process":
            action_successful = True # Recording a thought is considered a successful step completion heuristically
            current_step.status = ActionStatus.COMPLETED.value
            current_step.result_summary = f"Thought Recorded: {last_decision.get('content','')[:50]}..." # Summary based on thought
            self.state.current_plan.pop(0) # Remove completed step
            # If plan is empty after thought, add next step prompt
            if not self.state.current_plan:
                self.logger.info("Plan completed after thought. Adding next action step.")
                self.state.current_plan.append(PlanStep(description="Decide next action based on recorded thought and overall goal."))
            self.state.needs_replan = False # Recording a thought doesn't force replan

        # Case 3: LLM signaled completion
        elif decision_type == "complete":
            action_successful = True # Achieving goal is success
            # Overwrite plan with a final step (status handled in main loop)
            self.state.current_plan = [PlanStep(description="Goal Achieved. Finalizing.", status="completed")]
            self.state.needs_replan = False

        # Case 4: Handle errors or unexpected decisions (including AGENT_TOOL_UPDATE_PLAN failure)
        else:
            action_successful = False # Mark as failure for counter updates
            # Only mark the *current plan step* as failed if it wasn't the plan update tool itself that caused the error state
            if tool_name_executed != AGENT_TOOL_UPDATE_PLAN:
                current_step.status = ActionStatus.FAILED.value
                # Use the last action summary (which should contain the error) for the result summary
                err_summary = self.state.last_action_summary or "Unknown agent error"
                current_step.result_summary = f"Agent/Tool Error: {err_summary[:100]}..."
                # Insert re-evaluation step if not already present
                if len(self.state.current_plan) < 2 or not self.state.current_plan[1].description.startswith("Re-evaluate due to agent error"):
                     self.state.current_plan.insert(1, PlanStep(description="Re-evaluate due to agent error or unclear decision."))
            # Always set needs_replan if an error occurred or the decision was unexpected
            self.state.needs_replan = True

        # --- Update Meta-Cognitive Counters ---
        if action_successful:
            # Reset error counter on any successful progression
            self.state.consecutive_error_count = 0

            # Increment success counters *only* if the action wasn't internal/meta
            # Check if a tool was executed and if it's not in the excluded set
            if tool_name_executed and tool_name_executed not in self._INTERNAL_OR_META_TOOLS:
                 # Use float increments for flexibility
                 self.state.successful_actions_since_reflection += 1.0
                 self.state.successful_actions_since_consolidation += 1.0
                 self.logger.debug(f"Incremented success counters R:{self.state.successful_actions_since_reflection:.1f}, C:{self.state.successful_actions_since_consolidation:.1f} after successful action: {tool_name_executed}")
            elif decision_type == "thought_process":
                 # Option: Count thoughts as partial progress (e.g., 0.5)
                 self.state.successful_actions_since_reflection += 0.5 # Example: count thought as half an action
                 self.state.successful_actions_since_consolidation += 0.5
                 self.logger.debug(f"Incremented success counters R:{self.state.successful_actions_since_reflection:.1f}, C:{self.state.successful_actions_since_consolidation:.1f} after thought recorded.")

        else: # Action failed or was an error condition handled above
            # Increment consecutive error count
            self.state.consecutive_error_count += 1
            self.logger.warning(f"Consecutive error count increased to: {self.state.consecutive_error_count}")
            # Reset reflection counter immediately on error to encourage faster reflection
            if self.state.successful_actions_since_reflection > 0:
                 self.logger.info(f"Resetting reflection counter due to error (was {self.state.successful_actions_since_reflection:.1f}).")
                 self.state.successful_actions_since_reflection = 0
                 # Policy Decision: Keep consolidation counter running unless error rate is very high (handled in _adapt_thresholds)

        # --- Log Final Plan State ---
        log_plan_msg = f"Plan updated heuristically. Steps remaining: {len(self.state.current_plan)}. "
        if self.state.current_plan:
            next_step = self.state.current_plan[0]
            depends_str = f"Depends: {[_fmt_id(d) for d in next_step.depends_on]}" if next_step.depends_on else "Depends: None"
            log_plan_msg += f"Next: '{next_step.description[:60]}...' (Status: {next_step.status}, {depends_str})"
        else:
            log_plan_msg += "Plan is now empty."
        self.logger.info(log_plan_msg, emoji_key="clipboard")
        # <<< End Integration Block: Heuristic Plan Update Method >>>


    # ------------------------------------------------ adaptive thresholds --
    def _adapt_thresholds(self, stats: Dict[str, Any]) -> None:
        """
        Adjusts reflection and consolidation thresholds based on memory statistics,
        tool usage patterns, and **progress stability** to dynamically control
        meta-cognition frequency. Includes "Mental Momentum" bias.
        """
        # <<< Start Integration Block: Enhance _adapt_thresholds (Goal Stack + Momentum) >>>
        # Validate stats input
        if not stats or not stats.get("success"):
             self.logger.warning("Cannot adapt thresholds: Invalid or failed stats received.")
             return

        self.logger.debug(f"Adapting thresholds based on stats: {stats}")
        # Use dampening factor from constant
        adjustment_dampening = THRESHOLD_ADAPTATION_DAMPENING
        changed = False # Flag to log if any threshold changed

        # --- Consolidation Threshold Adaptation ---
        episodic_count = stats.get("by_level", {}).get(MemoryLevel.EPISODIC.value, 0)
        total_memories = stats.get("total_memories", 1) # Avoid division by zero
        episodic_ratio = episodic_count / total_memories if total_memories > 0 else 0
        # Target range for episodic ratio (e.g., ideally keep it below 30%)
        target_episodic_ratio_upper = 0.30
        target_episodic_ratio_lower = 0.10
        # Calculate deviation from the middle of the target range
        mid_target_ratio = (target_episodic_ratio_upper + target_episodic_ratio_lower) / 2
        ratio_deviation = episodic_ratio - mid_target_ratio
        # Calculate adjustment: more negative deviation (low ratio) -> increase threshold
        # More positive deviation (high ratio) -> decrease threshold
        # Scale adjustment based on current threshold (larger thresholds allow bigger steps)
        consolidation_adjustment = -math.ceil(ratio_deviation * self.state.current_consolidation_threshold * 2.0) # Factor of 2 controls sensitivity

        # Apply dampening to the adjustment
        dampened_adjustment = int(consolidation_adjustment * adjustment_dampening)
        if dampened_adjustment != 0:
            old_threshold = self.state.current_consolidation_threshold
            # Calculate potential new threshold, enforcing MIN/MAX bounds
            potential_new = max(MIN_CONSOLIDATION_THRESHOLD, min(MAX_CONSOLIDATION_THRESHOLD, old_threshold + dampened_adjustment))
            # Apply change only if it's different from the current threshold
            if potential_new != old_threshold:
                change_direction = "Lowering" if dampened_adjustment < 0 else "Raising"
                self.logger.info(
                    f"{change_direction} consolidation threshold: {old_threshold} -> {potential_new} "
                    f"(Episodic Ratio: {episodic_ratio:.1%}, Deviation: {ratio_deviation:+.1%}, Adjustment: {dampened_adjustment})"
                )
                self.state.current_consolidation_threshold = potential_new
                changed = True

        # --- Reflection Threshold Adaptation ---
        # Use tool usage stats accumulated in agent state
        total_calls = sum(v.get("success", 0) + v.get("failure", 0) for v in self.state.tool_usage_stats.values())
        total_failures = sum(v.get("failure", 0) for v in self.state.tool_usage_stats.values())
        # Calculate failure rate, avoid division by zero, require minimum calls for statistical significance
        min_calls_for_rate = 5 # Need at least 5 calls to calculate a meaningful rate
        failure_rate = (total_failures / total_calls) if total_calls >= min_calls_for_rate else 0.0
        # Target failure rate (e.g., aim for below 10%)
        target_failure_rate = 0.10
        failure_deviation = failure_rate - target_failure_rate
        # Calculate adjustment: more positive deviation (high failure) -> decrease threshold
        # More negative deviation (low failure) -> increase threshold
        reflection_adjustment = -math.ceil(failure_deviation * self.state.current_reflection_threshold * 3.0) # Factor of 3 controls sensitivity

        # --- Mental Momentum Bias ---
        is_stable_progress = (failure_rate < target_failure_rate * 0.5) and (self.state.consecutive_error_count == 0)
        if is_stable_progress and reflection_adjustment >= 0: # Only apply bias if adjustment is non-negative (i.e., increasing or zero)
             momentum_bias = math.ceil(reflection_adjustment * (MOMENTUM_THRESHOLD_BIAS_FACTOR - 1.0)) # Calculate the *additional* bias
             reflection_adjustment += momentum_bias # Add the positive bias
             self.logger.debug(f"Applying Mental Momentum: Adding +{momentum_bias} bias to reflection threshold adjustment (Stable progress).")

        # Apply dampening to the (potentially biased) adjustment
        dampened_adjustment = int(reflection_adjustment * adjustment_dampening)
        if dampened_adjustment != 0 and total_calls >= min_calls_for_rate: # Only adjust if enough calls
            old_threshold = self.state.current_reflection_threshold
            # Calculate potential new threshold, enforcing MIN/MAX bounds
            potential_new = max(MIN_REFLECTION_THRESHOLD, min(MAX_REFLECTION_THRESHOLD, old_threshold + dampened_adjustment))
            # Apply change only if different
            if potential_new != old_threshold:
                change_direction = "Lowering" if dampened_adjustment < 0 else "Raising"
                momentum_tag = " (+Momentum)" if is_stable_progress and momentum_bias > 0 else ""
                self.logger.info(
                    f"{change_direction} reflection threshold: {old_threshold} -> {potential_new} "
                    f"(Failure Rate: {failure_rate:.1%}, Deviation: {failure_deviation:+.1%}, Adjustment: {dampened_adjustment}{momentum_tag})"
                )
                self.state.current_reflection_threshold = potential_new
                changed = True

        # Log if no changes were made
        if not changed:
             self.logger.debug("No threshold adjustments triggered based on current stats/heuristics.")
        # <<< End Integration Block: Enhance _adapt_thresholds >>>


    # ------------------------------------------------ periodic task runner --
    async def _run_periodic_tasks(self):
        """
        Runs scheduled cognitive maintenance and enhancement tasks periodically.

        Checks intervals and thresholds to trigger tasks like reflection,
        consolidation, working memory optimization, focus updates, promotion checks,
        statistics computation, threshold adaptation, and memory maintenance.
        Executes triggered tasks sequentially within the loop cycle.
        """
        # Prevent running if no workflow or shutting down
        if not self.state.workflow_id or not self.state.context_id or self._shutdown_event.is_set():
            return

        # List to hold tasks scheduled for this cycle: (tool_name, args_dict)
        tasks_to_run: List[Tuple[str, Dict]] = []
        # List to track reasons for triggering tasks (for logging)
        trigger_reasons: List[str] = []

        # Check tool availability once per cycle for efficiency
        reflection_tool_available = self._find_tool_server(TOOL_REFLECTION) is not None
        consolidation_tool_available = self._find_tool_server(TOOL_CONSOLIDATION) is not None
        optimize_wm_tool_available = self._find_tool_server(TOOL_OPTIMIZE_WM) is not None
        auto_focus_tool_available = self._find_tool_server(TOOL_AUTO_FOCUS) is not None
        promote_mem_tool_available = self._find_tool_server(TOOL_PROMOTE_MEM) is not None
        stats_tool_available = self._find_tool_server(TOOL_COMPUTE_STATS) is not None
        maintenance_tool_available = self._find_tool_server(TOOL_DELETE_EXPIRED_MEMORIES) is not None

        # --- Tier 1: Highest Priority - Stats Check & Adaptation ---
        # Increment counter for stats adaptation interval
        self.state.loops_since_stats_adaptation += 1
        # Check if interval reached
        if self.state.loops_since_stats_adaptation >= STATS_ADAPTATION_INTERVAL:
            if stats_tool_available:
                trigger_reasons.append("StatsInterval")
                try:
                    # Fetch current statistics for the workflow
                    stats = await self._execute_tool_call_internal(
                        TOOL_COMPUTE_STATS, {"workflow_id": self.state.workflow_id}, record_action=False
                    )
                    if stats.get("success"):
                        # Adapt thresholds based on the fetched stats
                        self._adapt_thresholds(stats)
                        # Example: Potentially trigger consolidation *now* if stats show high episodic count
                        episodic_count = stats.get("by_level", {}).get(MemoryLevel.EPISODIC.value, 0)
                        # Trigger if count significantly exceeds the *current dynamic* threshold
                        if episodic_count > (self.state.current_consolidation_threshold * 2.0) and consolidation_tool_available:
                            # Check if consolidation isn't already scheduled for other reasons this cycle
                            if not any(task[0] == TOOL_CONSOLIDATION for task in tasks_to_run):
                                self.logger.info(f"High episodic count ({episodic_count}) detected via stats, scheduling consolidation.")
                                # Schedule consolidation task
                                tasks_to_run.append((TOOL_CONSOLIDATION, {
                                    "workflow_id": self.state.workflow_id,
                                    "consolidation_type": "summary", # Default to summary
                                    # Filter consolidation sources to episodic memories
                                    "query_filter": {"memory_level": MemoryLevel.EPISODIC.value},
                                    "max_source_memories": self.consolidation_max_sources
                                }))
                                trigger_reasons.append(f"HighEpisodic({episodic_count})")
                                # Reset consolidation counter as we're triggering it now based on stats
                                self.state.successful_actions_since_consolidation = 0
                    else:
                        # Log if stats computation failed
                        self.logger.warning(f"Failed to compute stats for adaptation: {stats.get('error')}")
                except Exception as e:
                    # Log errors during the stats/adaptation process
                    self.logger.error(f"Error during stats computation/adaptation: {e}", exc_info=False)
                finally:
                     # Reset the interval counter regardless of success/failure
                     self.state.loops_since_stats_adaptation = 0
            else:
                # Log if stats tool is unavailable
                self.logger.warning(f"Skipping stats/adaptation: Tool {TOOL_COMPUTE_STATS} not available")

        # --- Tier 2: Reflection & Consolidation (Based on dynamic thresholds) ---
        # Reflection Trigger (Check replan flag OR success counter vs. *dynamic* threshold)
        needs_reflection = self.state.needs_replan or self.state.successful_actions_since_reflection >= self.state.current_reflection_threshold
        if needs_reflection:
            if reflection_tool_available:
                # Check if reflection isn't already scheduled this cycle
                if not any(task[0] == TOOL_REFLECTION for task in tasks_to_run):
                    # Cycle through different reflection types for variety
                    reflection_type = self.reflection_type_sequence[self.state.reflection_cycle_index % len(self.reflection_type_sequence)]
                    # Schedule reflection task
                    tasks_to_run.append((TOOL_REFLECTION, {"workflow_id": self.state.workflow_id, "reflection_type": reflection_type}))
                    # Log the specific reason for triggering
                    reason_str = f"ReplanNeeded({self.state.needs_replan})" if self.state.needs_replan else f"SuccessCount({self.state.successful_actions_since_reflection:.1f}>={self.state.current_reflection_threshold})"
                    trigger_reasons.append(f"Reflect({reason_str})")
                    # Reset the success counter and advance the cycle index
                    self.state.successful_actions_since_reflection = 0
                    self.state.reflection_cycle_index += 1
            else:
                # Log if tool unavailable, still reset counter to prevent immediate re-trigger
                self.logger.warning(f"Skipping reflection: Tool {TOOL_REFLECTION} not available")
                self.state.successful_actions_since_reflection = 0

        # Consolidation Trigger (Check success counter vs. *dynamic* threshold)
        needs_consolidation = self.state.successful_actions_since_consolidation >= self.state.current_consolidation_threshold
        if needs_consolidation:
            if consolidation_tool_available:
                # Check if consolidation isn't already scheduled this cycle
                if not any(task[0] == TOOL_CONSOLIDATION for task in tasks_to_run):
                    # Schedule consolidation task (e.g., summarize episodic memories)
                    tasks_to_run.append((TOOL_CONSOLIDATION, {
                        "workflow_id": self.state.workflow_id,
                        "consolidation_type": "summary",
                        "query_filter": {"memory_level": MemoryLevel.EPISODIC.value},
                        "max_source_memories": self.consolidation_max_sources
                    }))
                    trigger_reasons.append(f"ConsolidateThreshold({self.state.successful_actions_since_consolidation:.1f}>={self.state.current_consolidation_threshold})")
                    # Reset the success counter
                    self.state.successful_actions_since_consolidation = 0
            else:
                # Log if tool unavailable, still reset counter
                self.logger.warning(f"Skipping consolidation: Tool {TOOL_CONSOLIDATION} not available")
                self.state.successful_actions_since_consolidation = 0

        # --- Tier 3: Optimization & Focus (Based on loop interval) ---
        # Increment optimization interval counter
        self.state.loops_since_optimization += 1
        # Check if interval reached
        if self.state.loops_since_optimization >= OPTIMIZATION_LOOP_INTERVAL:
            # Schedule working memory optimization if tool available
            if optimize_wm_tool_available:
                tasks_to_run.append((TOOL_OPTIMIZE_WM, {"context_id": self.state.context_id}))
                trigger_reasons.append("OptimizeInterval")
            else:
                self.logger.warning(f"Skipping optimization: Tool {TOOL_OPTIMIZE_WM} not available")

            # Schedule automatic focus update if tool available
            if auto_focus_tool_available:
                tasks_to_run.append((TOOL_AUTO_FOCUS, {"context_id": self.state.context_id}))
                trigger_reasons.append("FocusUpdate")
            else:
                self.logger.warning(f"Skipping auto-focus: Tool {TOOL_AUTO_FOCUS} not available")

            # Reset the interval counter
            self.state.loops_since_optimization = 0

        # --- Tier 4: Promotion Check (Based on loop interval) ---
        # Increment promotion check interval counter
        self.state.loops_since_promotion_check += 1
        # Check if interval reached
        if self.state.loops_since_promotion_check >= MEMORY_PROMOTION_LOOP_INTERVAL:
            if promote_mem_tool_available:
                # Schedule the internal check function, not the tool directly
                tasks_to_run.append(("CHECK_PROMOTIONS", {})) # Special marker task name
                trigger_reasons.append("PromotionInterval")
            else:
                self.logger.warning(f"Skipping promotion check: Tool {TOOL_PROMOTE_MEM} unavailable.")
            # Reset the interval counter
            self.state.loops_since_promotion_check = 0

        # --- Tier 5: Lowest Priority - Maintenance ---
        # Increment maintenance interval counter
        self.state.loops_since_maintenance += 1
        # Check if interval reached
        if self.state.loops_since_maintenance >= MAINTENANCE_INTERVAL:
            if maintenance_tool_available:
                # Schedule memory expiration task
                tasks_to_run.append((TOOL_DELETE_EXPIRED_MEMORIES, {})) # No args needed usually
                trigger_reasons.append("MaintenanceInterval")
                self.state.loops_since_maintenance = 0 # Reset interval counter
            else:
                # Log if tool unavailable
                self.logger.warning(f"Skipping maintenance: Tool {TOOL_DELETE_EXPIRED_MEMORIES} not available")


        # --- Execute Scheduled Tasks ---
        if tasks_to_run:
            unique_reasons_str = ', '.join(sorted(set(trigger_reasons))) # Log unique reasons
            self.logger.info(f"Running {len(tasks_to_run)} periodic tasks (Triggers: {unique_reasons_str})...", emoji_key="brain")

            # Optional: Prioritize tasks (e.g., run maintenance first)
            tasks_to_run.sort(key=lambda x: 0 if x[0] == TOOL_DELETE_EXPIRED_MEMORIES else 1 if x[0] == TOOL_COMPUTE_STATS else 2)

            # Execute tasks sequentially in this loop cycle
            for tool_name, args in tasks_to_run:
                # Check shutdown flag before each task execution
                if self._shutdown_event.is_set():
                    self.logger.info("Shutdown detected during periodic tasks, aborting remaining.")
                    break
                try:
                    # Handle the special internal promotion check task
                    if tool_name == "CHECK_PROMOTIONS":
                        await self._trigger_promotion_checks() # Call the helper method
                        continue # Move to next scheduled task

                    # Execute standard UMS tool calls
                    self.logger.debug(f"Executing periodic task: {tool_name} with args: {args}")
                    result_content = await self._execute_tool_call_internal(
                        tool_name, args, record_action=False # Don't record periodic tasks as agent actions
                    )

                    # --- Meta-Cognition Feedback Loop ---
                    # Check if the task was reflection or consolidation and if it succeeded
                    if tool_name in [TOOL_REFLECTION, TOOL_CONSOLIDATION] and result_content.get('success'):
                        feedback = ""
                        # Extract the relevant content from the result dictionary
                        if tool_name == TOOL_REFLECTION:
                            feedback = result_content.get("content", "")
                        elif tool_name == TOOL_CONSOLIDATION:
                             feedback = result_content.get("consolidated_content", "")

                        # Handle cases where result might be nested under 'data'
                        if not feedback and isinstance(result_content.get("data"), dict):
                            if tool_name == TOOL_REFLECTION:
                                feedback = result_content["data"].get("content", "")
                            elif tool_name == TOOL_CONSOLIDATION:
                                feedback = result_content["data"].get("consolidated_content", "")

                        # If feedback content exists, store a summary for the next main loop iteration
                        if feedback:
                            # Create a concise summary (e.g., first line)
                            feedback_summary = str(feedback).split('\n', 1)[0][:150]
                            self.state.last_meta_feedback = f"Feedback from {tool_name.split(':')[-1]}: {feedback_summary}..."
                            self.logger.info(f"Received meta-feedback: {self.state.last_meta_feedback}")
                            # Force replan after receiving significant feedback
                            self.state.needs_replan = True
                        else:
                            # Log if the task succeeded but returned no content for feedback
                            self.logger.debug(f"Periodic task {tool_name} succeeded but provided no feedback content.")

                except Exception as e:
                    # Log errors from periodic tasks but allow the agent loop to continue
                    self.logger.warning(f"Periodic task {tool_name} failed: {e}", exc_info=False)

                # Optional small delay between periodic tasks within a cycle
                await asyncio.sleep(0.1)


    async def _trigger_promotion_checks(self):
        """
        Queries for recently accessed memories eligible for promotion
        (Episodic -> Semantic, Semantic -> Procedural) and schedules
        background checks for each candidate using `_check_and_trigger_promotion`.
        """
        # Ensure a workflow is active
        if not self.state.workflow_id:
             self.logger.debug("Skipping promotion check: No active workflow.")
             return

        self.logger.debug("Running periodic promotion check for recent memories...")
        query_tool_name = TOOL_QUERY_MEMORIES # Tool for searching memories
        # Check tool availability
        if not self._find_tool_server(query_tool_name):
            self.logger.warning(f"Skipping promotion check: Tool {query_tool_name} unavailable.")
            return

        candidate_memory_ids = set() # Use set to store unique IDs
        try:
            # 1. Find recent Episodic memories (potential for Semantic promotion)
            episodic_args = {
                "workflow_id": self.state.workflow_id,
                "memory_level": MemoryLevel.EPISODIC.value,
                "sort_by": "last_accessed", # Prioritize recently used
                "sort_order": "DESC",
                "limit": 5, # Check top N recent/relevant
                "include_content": False # Don't need content for this check
            }
            episodic_results = await self._execute_tool_call_internal(query_tool_name, episodic_args, record_action=False)
            if episodic_results.get("success"):
                mems = episodic_results.get("memories", [])
                if isinstance(mems, list):
                    # Add valid memory IDs to the candidate set
                    candidate_memory_ids.update(m.get('memory_id') for m in mems if isinstance(m, dict) and m.get('memory_id'))

            # 2. Find recent Semantic memories of PROCEDURE or SKILL type (potential for Procedural promotion)
            semantic_args = {
                "workflow_id": self.state.workflow_id,
                "memory_level": MemoryLevel.SEMANTIC.value,
                # No type filter here yet, filter after retrieval
                "sort_by": "last_accessed",
                "sort_order": "DESC",
                "limit": 5,
                "include_content": False
            }
            semantic_results = await self._execute_tool_call_internal(query_tool_name, semantic_args, record_action=False)
            if semantic_results.get("success"):
                mems = semantic_results.get("memories", [])
                if isinstance(mems, list):
                     # Filter for specific types eligible for promotion to Procedural
                     candidate_memory_ids.update(
                          m.get('memory_id') for m in mems
                          if isinstance(m, dict) and m.get('memory_id') and
                          m.get('memory_type') in [MemoryType.PROCEDURE.value, MemoryType.SKILL.value] # Check type
                     )

            # 3. Schedule background checks for each candidate
            if candidate_memory_ids:
                self.logger.debug(f"Checking {len(candidate_memory_ids)} memories for potential promotion: {[_fmt_id(item_id) for item_id in candidate_memory_ids]}")
                # Create background tasks to check each memory individually
                promo_tasks = []
                for mem_id in candidate_memory_ids:
                     # Use _start_background_task to correctly snapshot state and manage task
                     task = self._start_background_task(AgentMasterLoop._check_and_trigger_promotion, memory_id=mem_id)
                     promo_tasks.append(task)
                # Option: Wait for these checks if promotion status is needed immediately,
                # otherwise let them run truly in the background. For now, fire-and-forget.
                # await asyncio.gather(*promo_tasks, return_exceptions=True) # Uncomment to wait
            else:
                # Log if no eligible candidates were found
                self.logger.debug("No recently accessed, eligible memories found for promotion check.")
        except Exception as e:
            # Log errors occurring during the query phase
            self.logger.error(f"Error during periodic promotion check query: {e}", exc_info=False)

    # ================================================================= context gather --
    async def _gather_context(self) -> Dict[str, Any]:
        """
        Gathers comprehensive context for the agent LLM.

        Includes:
        - Core context (recent actions, important memories, key thoughts) via TOOL_GET_CONTEXT.
        - **Current goal stack context (NEW)**.
        - Current working memory via TOOL_GET_WORKING_MEMORY **(Prioritized)**.
        - Proactively searched memories relevant to the current plan step **(Limited Fetch)**.
        - Relevant procedural memories **(Limited Fetch)**.
        - Summary of links around a focal or important memory **(Limited Fetch)**.
        - **Freshness indicators** for components.
        - Handles potential errors during retrieval.
        - Initiates context compression if token estimates exceed thresholds.
        """
        # <<< Start Integration Block: Enhance _gather_context (Goal Stack Context) >>>
        self.logger.info("Gathering comprehensive context...", emoji_key="satellite")
        start_time = time.time()
        retrieval_timestamp = datetime.now(timezone.utc).isoformat() # Timestamp for freshness

        # Initialize context dictionary with placeholders and essential state
        base_context = {
            # Core agent state info
            "current_loop": self.state.current_loop,
            "workflow_id": self.state.workflow_id, # Include current WF ID
            "context_id": self.state.context_id, # Include current Context ID
            "current_plan": [p.model_dump(exclude_none=True) for p in self.state.current_plan], # Current plan state
            "last_action_summary": self.state.last_action_summary,
            "consecutive_error_count": self.state.consecutive_error_count,
            "last_error_details": copy.deepcopy(self.state.last_error_details), # Deep copy error details
            "needs_replan": self.state.needs_replan, # Include replan flag in context
            "workflow_stack": self.state.workflow_stack,
            "meta_feedback": self.state.last_meta_feedback, # Include feedback from last meta task
            "current_thought_chain_id": self.state.current_thought_chain_id,
            # Placeholders for dynamically fetched context components
            "current_goal_context": None, # (NEW) Dict: {"retrieved_at": ..., "current_goal": {...}, "goal_stack_summary": [...]}
            "core_context": None, # From TOOL_GET_CONTEXT
            "current_working_memory": None, # From TOOL_GET_WORKING_MEMORY (will be dict)
            "proactive_memories": None, # Dict: {"retrieved_at": ..., "memories": [...]}
            "relevant_procedures": None, # Dict: {"retrieved_at": ..., "memories": [...]}
            "contextual_links": None, # Dict: {"retrieved_at": ..., "summary": {...}}
            "compression_summary": None, # If compression is applied
            "status": "Gathering...", # Initial status
            "errors": [] # List to collect errors during gathering
        }
        # Clear feedback after adding it to context
        self.state.last_meta_feedback = None

        # Determine the current workflow and context IDs from state
        current_workflow_id = self.state.workflow_stack[-1] if self.state.workflow_stack else self.state.workflow_id
        current_context_id = self.state.context_id

        # If no workflow is active, return immediately
        if not current_workflow_id:
            base_context["status"] = "No Active Workflow"
            base_context["message"] = "Agent must create or load a workflow."
            self.logger.warning(base_context["message"])
            return base_context

        # --- Fetch Goal Stack Context (NEW) ---
        goal_context_data = {"retrieved_at": retrieval_timestamp, "current_goal": None, "goal_stack_summary": []}
        if self.state.goal_stack:
             # Provide summary of the stack (limited depth)
             goal_context_data["goal_stack_summary"] = self.state.goal_stack[-CONTEXT_GOAL_STACK_SHOW_LIMIT:]
             # Fetch details for the current goal (top of stack) if tool exists
             current_goal_id = self.state.current_goal_id
             if current_goal_id and self._find_tool_server(TOOL_GET_GOAL_DETAILS):
                 try:
                     goal_details_res = await self._execute_tool_call_internal(
                         TOOL_GET_GOAL_DETAILS, {"goal_id": current_goal_id}, record_action=False
                     )
                     if goal_details_res.get("success") and isinstance(goal_details_res.get("goal"), dict):
                         goal_context_data["current_goal"] = goal_details_res["goal"] # Store full details of current goal
                     else:
                          err_msg = f"Failed to get details for current goal {_fmt_id(current_goal_id)}: {goal_details_res.get('error')}"
                          goal_context_data["current_goal"] = {"goal_id": current_goal_id, "error": err_msg} # Store error marker
                          base_context["errors"].append(err_msg)
                          self.logger.warning(err_msg)
                 except Exception as e:
                      err_msg = f"Exception getting goal details for {_fmt_id(current_goal_id)}: {e}"
                      goal_context_data["current_goal"] = {"goal_id": current_goal_id, "error": err_msg}
                      base_context["errors"].append(err_msg)
                      self.logger.error(err_msg, exc_info=False)
             elif current_goal_id: # If tool missing, use basic info from state
                goal_context_data["current_goal"] = next # If tool missing, use basic info from state stack if available
                current_goal_from_stack = next((g for g in self.state.goal_stack if g.get('goal_id') == current_goal_id), None)
                if current_goal_from_stack:
                    goal_context_data["current_goal"] = current_goal_from_stack # Use basic info from state
                    self.logger.debug(f"Using basic goal info from state stack for current goal {_fmt_id(current_goal_id)} (Tool {TOOL_GET_GOAL_DETAILS} unavailable).")
                else: # This case should be rare if state is consistent
                    err_msg = f"Current goal ID {_fmt_id(current_goal_id)} exists, but details unavailable (Tool missing or goal not in stack)."
                    goal_context_data["current_goal"] = {"goal_id": current_goal_id, "error": err_msg}
                    base_context["errors"].append(err_msg)
                    self.logger.warning(err_msg)
        else: # No goals on the stack
             goal_context_data["current_goal"] = None
             goal_context_data["goal_stack_summary"] = []
        # Assign the gathered goal context to the main context dictionary
        base_context["current_goal_context"] = goal_context_data
        self.logger.debug(f"Gathered goal context. Current Goal ID: {_fmt_id(self.state.current_goal_id)}")

        # --- Fetch Core Context (e.g., Recent Actions, Important Memories, Key Thoughts) ---
        if self._find_tool_server(TOOL_GET_CONTEXT):
            try:
                # Fetch core context with predefined limits (FETCH_LIMIT constants)
                core_ctx_result = await self._execute_tool_call_internal(
                    TOOL_GET_CONTEXT,
                    {
                        "workflow_id": current_workflow_id,
                        # Use FETCH limits here, truncation to SHOW limits happens later if needed
                        "recent_actions_limit": CONTEXT_RECENT_ACTIONS_FETCH_LIMIT,
                        "important_memories_limit": CONTEXT_IMPORTANT_MEMORIES_FETCH_LIMIT,
                        "key_thoughts_limit": CONTEXT_KEY_THOUGHTS_FETCH_LIMIT,
                    },
                    record_action=False # Internal context fetch
                )
                if core_ctx_result.get("success"):
                    # Store the successful result and add freshness timestamp
                    base_context["core_context"] = core_ctx_result
                    base_context["core_context"]["retrieved_at"] = retrieval_timestamp # Add freshness
                    # Clean up redundant success/timing info from the nested result
                    base_context["core_context"].pop("success", None)
                    base_context["core_context"].pop("processing_time", None)
                    self.logger.debug(f"Successfully retrieved core context via {TOOL_GET_CONTEXT}.")
                else:
                    err_msg = f"Core context retrieval ({TOOL_GET_CONTEXT}) failed: {core_ctx_result.get('error')}"
                    base_context["errors"].append(err_msg)
                    self.logger.warning(err_msg)
            except Exception as e:
                err_msg = f"Core context retrieval exception: {e}"
                self.logger.error(err_msg, exc_info=False)
                base_context["errors"].append(err_msg)
        else:
            self.logger.warning(f"Skipping core context: Tool {TOOL_GET_CONTEXT} unavailable.")

        # --- Get Current Working Memory (Provides active memories and focal point) ---
        focal_mem_id_from_wm: Optional[str] = None # Store focal ID if found
        working_mem_list_from_wm: List[Dict] = [] # Store memory list if found
        if current_context_id and self._find_tool_server(TOOL_GET_WORKING_MEMORY):
            try:
                wm_result = await self._execute_tool_call_internal(
                    TOOL_GET_WORKING_MEMORY,
                    {
                        "context_id": current_context_id,
                        "include_content": False, # Keep context lighter
                        "include_links": False # Links fetched separately if needed
                    },
                    record_action=False
                )
                if wm_result.get("success"):
                    # Store the entire result dict and add freshness
                    base_context["current_working_memory"] = wm_result
                    base_context["current_working_memory"]["retrieved_at"] = retrieval_timestamp # Add freshness
                    # Clean up redundant fields
                    base_context["current_working_memory"].pop("success", None)
                    base_context["current_working_memory"].pop("processing_time", None)
                    # Extract focal ID and memory list for later use
                    focal_mem_id_from_wm = wm_result.get("focal_memory_id")
                    working_mem_list_from_wm = wm_result.get("working_memories", [])
                    # Log count of retrieved working memories
                    wm_count = len(working_mem_list_from_wm)
                    self.logger.info(f"Retrieved {wm_count} items from working memory (Context: {_fmt_id(current_context_id)}). Focal: {_fmt_id(focal_mem_id_from_wm)}")
                else:
                    err_msg = f"Working memory retrieval failed: {wm_result.get('error')}"
                    base_context["errors"].append(err_msg)
                    self.logger.warning(err_msg)
            except Exception as e:
                err_msg = f"Working memory retrieval exception: {e}"
                self.logger.error(err_msg, exc_info=False)
                base_context["errors"].append(err_msg)
        else:
            self.logger.warning(f"Skipping working memory retrieval: Context ID missing or tool {TOOL_GET_WORKING_MEMORY} unavailable.")


        # --- Goal-Directed Proactive Memory Retrieval (Using Hybrid Search) ---
        # Find memories relevant to the current plan step OR current goal description
        # Determine the primary query source: plan step or goal description
        query_source_desc = "Achieve main goal" # Default if no plan/goal
        if self.state.current_plan:
            query_source_desc = self.state.current_plan[0].description
        elif goal_context_data.get("current_goal"):
            query_source_desc = goal_context_data["current_goal"].get("description", query_source_desc)

        # Formulate a query based on the current step/goal
        proactive_query = f"Information relevant to planning or executing: {query_source_desc}"
        # Prefer hybrid search, fallback to semantic
        search_tool_proactive = TOOL_HYBRID_SEARCH if self._find_tool_server(TOOL_HYBRID_SEARCH) else TOOL_SEMANTIC_SEARCH
        if self._find_tool_server(search_tool_proactive):
            search_args = {
                "workflow_id": current_workflow_id,
                "query": proactive_query,
                "limit": CONTEXT_PROACTIVE_MEMORIES_FETCH_LIMIT, # Use FETCH limit
                "include_content": False # Keep context light
            }
            # Adjust weights for hybrid search
            if search_tool_proactive == TOOL_HYBRID_SEARCH:
                search_args.update({"semantic_weight": 0.7, "keyword_weight": 0.3}) # Prioritize semantics a bit
            try:
                result_content = await self._execute_tool_call_internal(
                    search_tool_proactive, search_args, record_action=False
                )
                if result_content.get("success"):
                    proactive_mems = result_content.get("memories", [])
                    # Determine score key based on tool used
                    score_key = "hybrid_score" if search_tool_proactive == TOOL_HYBRID_SEARCH else "similarity"
                    # Format results for context, include freshness
                    base_context["proactive_memories"] = {
                        "retrieved_at": retrieval_timestamp,
                        "memories": [
                            {
                                "memory_id": m.get("memory_id"),
                                "description": m.get("description"),
                                "score": round(m.get(score_key, 0), 3), # Include score
                                "type": m.get("memory_type") # Include type
                             }
                            for m in proactive_mems # Iterate through results
                        ]
                    }
                    if base_context["proactive_memories"]["memories"]:
                        self.logger.info(f"Retrieved {len(base_context['proactive_memories']['memories'])} proactive memories using {search_tool_proactive.split(':')[-1]}.")
                else:
                    err_msg = f"Proactive memory search ({search_tool_proactive}) failed: {result_content.get('error')}"
                    base_context["errors"].append(err_msg)
                    self.logger.warning(err_msg)
            except Exception as e:
                err_msg = f"Proactive memory search exception: {e}"
                self.logger.warning(err_msg, exc_info=False)
                base_context["errors"].append(err_msg)
        else:
            self.logger.warning("Skipping proactive memory search: No suitable search tool available.")

        # --- Fetch Relevant Procedural Memories (Using Hybrid Search) ---
        # Find procedural memories related to the current step/goal
        search_tool_proc = TOOL_HYBRID_SEARCH if self._find_tool_server(TOOL_HYBRID_SEARCH) else TOOL_SEMANTIC_SEARCH
        if self._find_tool_server(search_tool_proc):
            # Formulate a query focused on finding procedures/how-to steps based on the same source description
            proc_query = f"How to accomplish step-by-step: {query_source_desc}"
            search_args = {
                "workflow_id": current_workflow_id,
                "query": proc_query,
                "limit": CONTEXT_PROCEDURAL_MEMORIES_FETCH_LIMIT, # Use FETCH limit
                "memory_level": MemoryLevel.PROCEDURAL.value, # Explicitly filter for procedural level
                "include_content": False # Keep context light
            }
            if search_tool_proc == TOOL_HYBRID_SEARCH:
                search_args.update({"semantic_weight": 0.6, "keyword_weight": 0.4}) # Balanced weights maybe?
            try:
                proc_result = await self._execute_tool_call_internal(
                    search_tool_proc, search_args, record_action=False
                )
                if proc_result.get("success"):
                    proc_mems = proc_result.get("memories", [])
                    score_key = "hybrid_score" if search_tool_proc == TOOL_HYBRID_SEARCH else "similarity"
                    # Format results for context, include freshness
                    base_context["relevant_procedures"] = {
                         "retrieved_at": retrieval_timestamp,
                         "procedures": [
                            {
                                "memory_id": m.get("memory_id"),
                                "description": m.get("description"),
                                "score": round(m.get(score_key, 0), 3)
                            }
                            for m in proc_mems
                         ]
                    }
                    if base_context["relevant_procedures"]["procedures"]:
                        self.logger.info(f"Retrieved {len(base_context['relevant_procedures']['procedures'])} relevant procedures using {search_tool_proc.split(':')[-1]}.")
                else:
                    err_msg = f"Procedure search failed: {proc_result.get('error')}"
                    base_context["errors"].append(err_msg)
                    self.logger.warning(err_msg)
            except Exception as e:
                err_msg = f"Procedure search exception: {e}"
                self.logger.warning(err_msg, exc_info=False)
                base_context["errors"].append(err_msg)
        else:
            self.logger.warning("Skipping procedure search: No suitable search tool available.")

        # --- Contextual Link Traversal (Prioritizing Focal Memory) ---
        # Find memories linked to the current focus or most relevant items
        get_linked_memories_tool = TOOL_GET_LINKED_MEMORIES
        if self._find_tool_server(get_linked_memories_tool):
            mem_id_to_traverse = None
            # 1. PRIORITIZE focal memory from working memory result
            if focal_mem_id_from_wm:
                mem_id_to_traverse = focal_mem_id_from_wm
                self.logger.debug(f"Link traversal starting from focal memory: {_fmt_id(mem_id_to_traverse)}")

            # 2. If no focal, try the first memory *in* the working memory list
            if not mem_id_to_traverse and working_mem_list_from_wm:
                first_wm_item = working_mem_list_from_wm[0]
                if isinstance(first_wm_item, dict):
                    mem_id_to_traverse = first_wm_item.get("memory_id")
                    if mem_id_to_traverse:
                        self.logger.debug(f"Link traversal starting from first working memory item: {_fmt_id(mem_id_to_traverse)}")

            # 3. If still no ID, try the first important memory from the core context
            if not mem_id_to_traverse:
                core_ctx_data = base_context.get("core_context", {})
                if isinstance(core_ctx_data, dict):
                    important_mem_list = core_ctx_data.get("important_memories", [])
                    if isinstance(important_mem_list, list) and important_mem_list:
                        first_mem = important_mem_list[0]
                        if isinstance(first_mem, dict):
                            mem_id_to_traverse = first_mem.get("memory_id")
                            if mem_id_to_traverse:
                                self.logger.debug(f"Link traversal starting from first important memory: {_fmt_id(mem_id_to_traverse)}")

            # If we found a relevant memory ID to start traversal from
            if mem_id_to_traverse:
                self.logger.debug(f"Attempting link traversal from relevant memory: {_fmt_id(mem_id_to_traverse)}...")
                try:
                    links_result_content = await self._execute_tool_call_internal(
                        get_linked_memories_tool,
                        {
                            "memory_id": mem_id_to_traverse,
                            "direction": "both", # Get incoming and outgoing links
                            "limit": CONTEXT_LINK_TRAVERSAL_FETCH_LIMIT, # Use FETCH limit
                            "include_memory_details": False # Just need link info for context
                         },
                        record_action=False
                    )
                    if links_result_content.get("success"):
                        links_data = links_result_content.get("links", {})
                        outgoing_links = links_data.get("outgoing", [])
                        incoming_links = links_data.get("incoming", [])
                        # Create a concise summary of the links found
                        link_summary = {
                            "source_memory_id": mem_id_to_traverse,
                            "outgoing_count": len(outgoing_links),
                            "incoming_count": len(incoming_links),
                            "top_links_summary": [] # List of concise link descriptions
                        }
                        # Add summaries for top outgoing links (up to SHOW limit)
                        for link in outgoing_links[:CONTEXT_LINK_TRAVERSAL_SHOW_LIMIT]:
                            link_summary["top_links_summary"].append(
                                f"OUT: {link.get('link_type', 'related')} -> {_fmt_id(link.get('target_memory_id'))}"
                            )
                        # Add summaries for top incoming links (up to SHOW limit)
                        for link in incoming_links[:CONTEXT_LINK_TRAVERSAL_SHOW_LIMIT]:
                            link_summary["top_links_summary"].append(
                                f"IN: {_fmt_id(link.get('source_memory_id'))} -> {link.get('link_type', 'related')}"
                            )
                        # Add freshness timestamp to the link summary
                        base_context["contextual_links"] = {"retrieved_at": retrieval_timestamp, "summary": link_summary}
                        self.logger.info(f"Retrieved link summary for memory {_fmt_id(mem_id_to_traverse)} ({len(outgoing_links)} out, {len(incoming_links)} in).")
                    else:
                        err_msg = f"Link retrieval ({get_linked_memories_tool}) failed: {links_result_content.get('error', 'Unknown')}"
                        base_context["errors"].append(err_msg)
                        self.logger.warning(err_msg)
                except Exception as e:
                    err_msg = f"Link retrieval exception: {e}"
                    self.logger.warning(err_msg, exc_info=False)
                    base_context["errors"].append(err_msg)
            else:
                # Log if no suitable starting point for link traversal was found
                self.logger.debug("No relevant memory found (focal/working/important) to perform link traversal from.")
        else:
            # Log if the link retrieval tool is unavailable
            self.logger.debug(f"Skipping link traversal: Tool '{get_linked_memories_tool}' unavailable.")

        # --- Context Compression Check ---
        # This should happen *after* all components are gathered
        try:
            # Estimate tokens of the fully assembled context
            estimated_tokens = await self._estimate_tokens_anthropic(base_context)
            # If estimate exceeds threshold, attempt compression
            if estimated_tokens > CONTEXT_MAX_TOKENS_COMPRESS_THRESHOLD:
                self.logger.warning(f"Context ({estimated_tokens} tokens) exceeds threshold {CONTEXT_MAX_TOKENS_COMPRESS_THRESHOLD}. Attempting summary compression.")
                # Check if summarization tool is available
                if self._find_tool_server(TOOL_SUMMARIZE_TEXT):
                    # Strategy: Summarize the potentially longest/most verbose part first
                    # Example: Summarize 'core_context' -> 'recent_actions' if it exists and is large
                    core_ctx = base_context.get("core_context")
                    actions_to_summarize = None
                    # Check structure before accessing potentially missing keys
                    if isinstance(core_ctx, dict):
                        actions_to_summarize = core_ctx.get("recent_actions")

                    # Only summarize if the actions list exists and is substantial (e.g., > 1000 chars JSON)
                    if actions_to_summarize and isinstance(actions_to_summarize, list) and len(json.dumps(actions_to_summarize, default=str)) > 1000:
                         actions_text = json.dumps(actions_to_summarize, default=str) # Serialize for summarizer
                         summary_result = await self._execute_tool_call_internal(
                              TOOL_SUMMARIZE_TEXT,
                              {
                                  "text_to_summarize": actions_text,
                                  "target_tokens": CONTEXT_COMPRESSION_TARGET_TOKENS, # Use target constant
                                  # Use specialized summarizer prompt for actions
                                  "prompt_template": "summarize_context_block", # Assuming this maps to the right prompt in UMS
                                  "context_type": "actions", # Tell summarizer what it's summarizing
                                  "workflow_id": current_workflow_id, # Provide workflow context
                                  "record_summary": False # Don't store this ephemeral summary
                              },
                              record_action=False # Internal action
                         )
                         if summary_result.get("success"):
                              # Add a note about the compression to the context
                              base_context["compression_summary"] = f"Summary of recent actions: {summary_result.get('summary', 'Summary failed.')[:150]}..."
                              # Replace the original actions list in core_context with a reference to the summary
                              if isinstance(core_ctx, dict):
                                   core_ctx["recent_actions"] = f"[Summarized: See compression_summary field]"
                              self.logger.info(f"Compressed recent actions. New context size estimate: {await self._estimate_tokens_anthropic(base_context)} tokens")
                         else:
                              err_msg = f"Context compression tool ({TOOL_SUMMARIZE_TEXT}) failed: {summary_result.get('error')}"
                              base_context["errors"].append(err_msg); self.logger.warning(err_msg)
                    else:
                         # Log if no suitable large component found for compression
                         self.logger.info("No large action list found to compress, context remains large.")
                else:
                    # Log if summarization tool is unavailable
                    self.logger.warning(f"Cannot compress context: Tool '{TOOL_SUMMARIZE_TEXT}' unavailable.")
        except Exception as e:
            # Catch errors during the estimation/compression process
            err_msg = f"Error during context compression check: {e}"
            self.logger.error(err_msg, exc_info=False)
            base_context["errors"].append(err_msg)
        # <<< End Integration Block: Context Gathering >>>

        # Final status update
        base_context["status"] = "Ready" if not base_context["errors"] else "Ready with Errors"
        self.logger.info(f"Context gathering complete. Errors: {len(base_context['errors'])}. Time: {(time.time() - start_time):.3f}s")
        return base_context


    # ----------------------------------------------------------- call‚Äëllm --
    async def _call_agent_llm(
        self, goal: str, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Calls the Anthropic LLM with the constructed prompt and context,
        parsing the response to extract the agent's decision (tool call,
        thought, or completion signal). Includes retry logic for API errors.

        Returns:
            A dictionary representing the agent's decision:
            - {"decision": "call_tool", "tool_name": str, "arguments": dict}
            - {"decision": "thought_process", "content": str}
            - {"decision": "complete", "summary": str}
            - {"decision": "error", "message": str}
            - {"decision": "plan_update", "updated_plan_steps": List[PlanStep]} (Parsed from text)
        """
        # --- 1. Prepare Prompt ---
        messages = self._construct_agent_prompt(goal, context)

        # --- 2. Define Tools for Anthropic API ---
        # Filter schemas slightly for Anthropic's specific format if needed,
        # although MCPClient's format_tools_for_anthropic should handle this.
        anthropic_tools = self.tool_schemas

        # --- 3. Execute LLM Call (with Retries) ---
        async def _do_llm_call():
            # Ensure client is available
            if not self.anthropic_client:
                raise RuntimeError("Anthropic client unavailable for LLM call")

            # Make the API call using the configured model
            response = await self.anthropic_client.messages.create(
                model=MASTER_LEVEL_AGENT_LLM_MODEL_STRING,
                max_tokens=2048, # Allow sufficient tokens for response + tool use
                system="", # System prompt is included in user message per original structure
                messages=messages, # Pass the constructed messages list
                tools=anthropic_tools, # Provide the tool schemas
                tool_choice={"type": "auto"}, # Let model decide whether to use a tool
                temperature=0.5, # Moderate temperature for balanced creativity/determinism
            )
            return response

        try:
            # Use retry wrapper for the API call
            response = await self._with_retries(
                _do_llm_call,
                max_retries=3,
                # Retry on specific Anthropic API errors and common network issues
                retry_exceptions=(
                    APIConnectionError, RateLimitError, APIStatusError,
                    asyncio.TimeoutError, ConnectionError
                ),
            )
            self.logger.debug(f"LLM Response Stop Reason: {response.stop_reason}")
            # --- 4. Parse LLM Response ---
            # Check if the response involves a tool call
            if response.stop_reason == "tool_use" and response.content:
                tool_calls = []
                # Iterate through content blocks to find tool_use blocks
                for block in response.content:
                    if block.type == "tool_use":
                        # Found a tool use block, extract name and input arguments
                        tool_name_raw = block.name # This is the sanitized name
                        tool_args = block.input or {}
                        # Map sanitized name back to original MCP name
                        original_name = self.mcp_client.server_manager.sanitized_to_original.get(tool_name_raw, tool_name_raw)
                        tool_calls.append({"tool_name": original_name, "arguments": tool_args})
                        self.logger.info(f"LLM decided to call tool: {original_name}", emoji_key="hammer")

                # Currently designed to handle ONE tool call per response
                if len(tool_calls) == 1:
                    return {"decision": "call_tool", **tool_calls[0]}
                elif len(tool_calls) > 1:
                    # If multiple tool calls are requested, log a warning and handle only the first one
                    self.logger.warning(f"LLM requested multiple tool calls ({len(tool_calls)}). Executing only the first: {tool_calls[0]['tool_name']}")
                    return {"decision": "call_tool", **tool_calls[0]}
                else: # Should not happen if stop_reason is tool_use, but handle defensively
                     err_msg = "LLM stop reason was 'tool_use' but no tool calls found in content."
                     self.logger.error(err_msg)
                     return {"decision": "error", "message": err_msg}

            # Check if the response is plain text (thought or goal completion)
            elif response.content and response.content[0].type == "text":
                response_text = response.content[0].text.strip()
                # Check for explicit Goal Achieved signal
                if response_text.upper().startswith("GOAL ACHIEVED:"):
                    summary = response_text[len("GOAL ACHIEVED:"):].strip()
                    self.logger.info(f"LLM signaled goal completion. Summary: {summary}", emoji_key="tada")
                    return {"decision": "complete", "summary": summary}

                # --- Check for Plan Update via Text ---
                # Heuristic check: Does the text contain something like "Updated Plan:", "New Plan:", etc.
                # followed by a list structure? This is less robust than the tool but provides a fallback.
                plan_marker = "updated plan:" # Case-insensitive check
                if plan_marker in response_text.lower():
                     try:
                          # Attempt to extract and parse the plan structure after the marker
                          plan_part = response_text[response_text.lower().find(plan_marker) + len(plan_marker):].strip()
                          # Simple JSON/list parsing attempt (might need more robust parsing)
                          if plan_part.startswith("[") and plan_part.endswith("]"):
                               plan_data = json.loads(plan_part)
                               if isinstance(plan_data, list):
                                    # Validate structure using Pydantic models
                                    validated_plan = [PlanStep(**step_data) for step_data in plan_data]
                                    self.logger.info(f"LLM proposed plan update via text ({len(validated_plan)} steps).")
                                    # Return the validated plan steps for application
                                    return {"decision": "plan_update", "updated_plan_steps": validated_plan}
                     except (json.JSONDecodeError, ValidationError, TypeError) as e:
                          # If parsing/validation fails, treat it as a regular thought
                          self.logger.warning(f"LLM text suggested plan update, but parsing/validation failed: {e}. Treating as thought.")
                          pass # Fall through to treat as thought

                # Otherwise, treat the text response as a thought process
                self.logger.info(f"LLM provided reasoning/thought: '{response_text[:100]}...'", emoji_key="speech_balloon")
                return {"decision": "thought_process", "content": response_text}
            else:
                # Handle unexpected empty response or unknown content type
                err_msg = f"LLM returned unexpected response content or stop reason: {response.stop_reason}, Content: {response.content}"
                self.logger.error(err_msg)
                return {"decision": "error", "message": err_msg}

        except APIStatusError as e: # Handle API status errors specifically
            err_msg = f"Anthropic API Error {e.status_code}: {e.message}"
            self.logger.error(err_msg, exc_info=False)
            return {"decision": "error", "message": err_msg}
        except Exception as e: # Catch other exceptions during API call or parsing
            err_msg = f"Error calling LLM or parsing response: {e}"
            self.logger.error(err_msg, exc_info=True)
            return {"decision": "error", "message": err_msg}


    # ------------------------------------------------------------ main loop --
    async def run(self, goal: str, max_loops: int = 100) -> None:
        """
        The main execution loop for the agent.

        Args:
            goal: The high-level goal for the agent to achieve.
            max_loops: Maximum number of iterations before stopping automatically.
        """
        self.logger.info(f"Starting agent loop. Goal: '{goal}'. Max loops: {max_loops}")

        # Initialize or ensure workflow exists
        if not self.state.workflow_id:
            self.logger.info("No active workflow found. Creating initial workflow.")
            wf_create_args = {
                "title": f"Agent Task: {goal[:50]}...",
                "goal": goal,
                "description": f"Agent workflow initiated at {datetime.now(timezone.utc).isoformat()} to achieve: {goal}",
                "tags": ["agent_run", AGENT_NAME.lower()]
            }
            wf_create_result = await self._execute_tool_call_internal(
                TOOL_CREATE_WORKFLOW, wf_create_args, record_action=False
            )
            # Note: _handle_workflow_and_goal_side_effects should set self.state.workflow_id, context_id, stack, chain_id etc.
            if not wf_create_result.get("success") or not self.state.workflow_id:
                self.logger.critical(f"Failed to create initial workflow: {wf_create_result.get('error')}. Aborting.")
                return
            # Note: The root goal should now be created within _handle_workflow_and_goal_side_effects if successful
            elif not self.state.current_goal_id:
                 self.logger.critical("Workflow created, but root goal ID was not set. Aborting.")
                 return

        elif not self.state.current_thought_chain_id:
            # If workflow loaded but no chain ID, try setting default
            await self._set_default_thought_chain_id()
        # --- ADDED: Ensure current_goal_id is set if stack isn't empty ---
        elif self.state.goal_stack and not self.state.current_goal_id:
             self.state.current_goal_id = self.state.goal_stack[-1].get("goal_id")
             self.logger.info(f"Set current_goal_id from top of loaded stack: {_fmt_id(self.state.current_goal_id)}")


        # --- Main Think-Act Loop ---
        while self.state.current_loop < max_loops and not self.state.goal_achieved_flag and not self._shutdown_event.is_set():
            self.state.current_loop += 1
            self.logger.info(f"--- Starting Loop {self.state.current_loop}/{max_loops} (WF: {_fmt_id(self.state.workflow_id)}, Goal: {_fmt_id(self.state.current_goal_id)}) ---")

            # --- 1. Periodic Cognitive Tasks ---
            try:
                 await self._run_periodic_tasks()
            except Exception as e:
                 self.logger.error(f"Error during periodic tasks: {e}", exc_info=True) # Log but continue loop

            # Check shutdown flag again after periodic tasks
            if self._shutdown_event.is_set(): break

            # --- 2. Gather Context ---
            context = await self._gather_context()

            # Check if no active workflow after context gathering (e.g., root workflow finished)
            if not self.state.workflow_id:
                 self.logger.info("No active workflow. Agent loop concluding.")
                 break # Exit loop if workflow finished

            # --- 3. Pre-computation/Pre-analysis (If needed before LLM call) ---
            # (Placeholder for future logic)

            # --- 4. Call LLM for Decision ---
            # Clear error details before LLM call (unless needs_replan is set for error recovery)
            if not self.state.needs_replan:
                 self.state.last_error_details = None
            # Call the LLM
            decision = await self._call_agent_llm(goal, context)

            # --- 5. Execute Decision ---
            tool_result_content: Optional[Dict[str, Any]] = None # Store result for heuristic update
            llm_proposed_plan = decision.get("updated_plan_steps") # Check if LLM proposed plan via text

            # Handle decision based on type
            if decision.get("decision") == "call_tool":
                tool_name = decision.get("tool_name")
                arguments = decision.get("arguments", {})

                # --- Plan Validation (Pre-Execution Check) ---
                # Ensure the intended step (likely first in plan) is valid
                if not self.state.current_plan:
                    self.logger.error("Attempting tool call but plan is empty! Forcing replan.")
                    self.state.needs_replan = True
                    self.state.last_error_details = {"error": "Plan became empty before tool call.", "type": "PlanValidationError"}
                elif not self.state.current_plan[0].description:
                    self.logger.error(f"Attempting tool call for invalid plan step (missing description)! Step ID: {self.state.current_plan[0].id}. Forcing replan.")
                    self.state.needs_replan = True
                    self.state.last_error_details = {"error": "Current plan step is invalid (missing description).", "type": "PlanValidationError", "step_id": self.state.current_plan[0].id}
                # If checks pass, execute the tool call
                elif tool_name:
                    # Extract dependencies *from the current plan step* that this tool call corresponds to
                    current_step_deps = self.state.current_plan[0].depends_on if self.state.current_plan else []
                    # Execute the tool call, passing planned dependencies for checking
                    tool_result_content = await self._execute_tool_call_internal(
                        tool_name, arguments, record_action=True, planned_dependencies=current_step_deps
                    )
                else: # Should not happen if parsing worked correctly
                    self.logger.error("LLM decided to call tool but no tool name was provided.")
                    self.state.last_error_details = {"error": "LLM tool call decision missing tool name.", "type": "LLMOutputError"}
                    tool_result_content = {"success": False, "error": "Missing tool name from LLM."}

            elif decision.get("decision") == "thought_process":
                # Record the thought provided by the LLM
                thought_content = decision.get("content")
                if thought_content:
                    # Default to 'inference' type if LLM doesn't specify
                    thought_type = ThoughtType.INFERENCE.value
                    # Advanced: Could try to infer type from text content if needed
                    tool_result_content = await self._execute_tool_call_internal(
                        TOOL_RECORD_THOUGHT,
                        {"content": thought_content, "thought_type": thought_type},
                        # Allow workflow_id and thought_chain_id injection
                        record_action=False # Recording thoughts isn't a primary world action
                    )
                else: # Handle missing content
                    self.logger.warning("LLM provided 'thought_process' decision but no content.")
                    tool_result_content = {"success": False, "error": "Missing thought content from LLM."}

            elif decision.get("decision") == "complete":
                # Goal achieved signal from LLM (Handles *overall* goal completion)
                self.logger.info(f"LLM signaled OVERALL goal completion: {decision.get('summary')}")
                self.state.goal_achieved_flag = True # Set flag to terminate loop
                # Update workflow status to completed if possible
                if self.state.workflow_id and self._find_tool_server(TOOL_UPDATE_WORKFLOW_STATUS):
                    await self._execute_tool_call_internal(
                        TOOL_UPDATE_WORKFLOW_STATUS,
                        {
                            "workflow_id": self.state.workflow_id,
                            "status": WorkflowStatus.COMPLETED.value,
                            "completion_message": decision.get('summary', 'Overall goal marked achieved by agent.')
                        },
                        record_action=False # Meta-action
                    )
                    # Note: _handle_workflow_and_goal_side_effects will clear WF state if root WF finished
                # Break the loop after handling completion
                break

            elif decision.get("decision") == "plan_update": # Handle plan parsed from text
                self.logger.info("LLM proposed plan update via text response.")
                # The plan is already validated in _call_agent_llm
                llm_proposed_plan = decision.get("updated_plan_steps") # Already validated PlanStep list
                # We'll handle application of this plan in Step 6

            elif decision.get("decision") == "error":
                 # LLM or internal error during decision making
                 self.logger.error(f"LLM decision error: {decision.get('message')}")
                 self.state.last_action_summary = f"LLM Decision Error: {decision.get('message', 'Unknown')[:100]}"
                 # Store error details if not already set by tool execution
                 if not self.state.last_error_details:
                     self.state.last_error_details = {"error": decision.get('message'), "type": "LLMError"}
                 self.state.needs_replan = True # Force replan after LLM error
                 # action_successful remains False

            # --- 6. Apply Plan Updates ---
            # Priority: Plan proposed by LLM via text > Heuristic update
            if llm_proposed_plan:
                 try:
                     # --- Plan Cycle Detection ---
                     if self._detect_plan_cycle(llm_proposed_plan):
                         err_msg = "LLM-proposed plan contains a dependency cycle. Applying heuristic update instead."
                         self.logger.error(err_msg)
                         self.state.last_error_details = {"error": err_msg, "type": "PlanValidationError", "proposed_plan": [p.model_dump() for p in llm_proposed_plan]}
                         self.state.needs_replan = True # Force replan again
                         # Fallback to heuristic update if LLM plan is invalid
                         await self._apply_heuristic_plan_update(decision, tool_result_content)
                     else:
                         # Apply the LLM's validated plan
                         self.state.current_plan = llm_proposed_plan
                         self.state.needs_replan = False # LLM provided the plan, assume it's intended
                         self.logger.info(f"Applied LLM-proposed plan update ({len(llm_proposed_plan)} steps).")
                         # Clear errors after successful LLM plan update
                         self.state.last_error_details = None
                         self.state.consecutive_error_count = 0
                 except Exception as plan_apply_err:
                      # Catch errors applying the plan (should be rare if validation passed)
                      self.logger.error(f"Error applying LLM proposed plan: {plan_apply_err}. Falling back to heuristic.", exc_info=True)
                      self.state.last_error_details = {"error": f"Failed to apply LLM plan: {plan_apply_err}", "type": "PlanUpdateError"}
                      self.state.needs_replan = True
                      await self._apply_heuristic_plan_update(decision, tool_result_content)

            elif decision.get("tool_name") != AGENT_TOOL_UPDATE_PLAN: # Heuristic only if LLM didn't explicitly update plan
                # Apply heuristic updates if LLM didn't use the plan update tool or provide a valid text plan
                await self._apply_heuristic_plan_update(decision, tool_result_content)
            # else: If AGENT_TOOL_UPDATE_PLAN was called, success/failure already handled by _execute_tool_call_internal

            # --- 7. Check Error Limit & Save State ---
            if self.state.consecutive_error_count >= MAX_CONSECUTIVE_ERRORS:
                self.logger.critical(f"Max consecutive errors ({MAX_CONSECUTIVE_ERRORS}) reached. Aborting loop.")
                # Update workflow status to failed if possible
                if self.state.workflow_id and self._find_tool_server(TOOL_UPDATE_WORKFLOW_STATUS):
                     await self._execute_tool_call_internal(
                         TOOL_UPDATE_WORKFLOW_STATUS,
                         {
                             "workflow_id": self.state.workflow_id,
                             "status": WorkflowStatus.FAILED.value,
                             "completion_message": f"Aborted after {MAX_CONSECUTIVE_ERRORS} consecutive errors."
                         },
                         record_action=False
                     )
                break # Exit loop

            # Save state at the end of each loop iteration
            await self._save_agent_state()

            # Optional: Small delay between loops
            # await asyncio.sleep(0.5)

        # --- Loop End ---
        if self._shutdown_event.is_set():
            self.logger.info("Agent loop terminated due to shutdown signal.")
        elif self.state.current_loop >= max_loops:
            self.logger.warning(f"Agent loop reached max iterations ({max_loops}). Stopping.")
        elif self.state.goal_achieved_flag:
            self.logger.info("Agent loop finished: Goal achieved.", emoji_key="tada")
        else:
             # Log reason if loop finished but goal_achieved_flag is false (e.g., error limit, no active workflow)
             if self.state.consecutive_error_count >= MAX_CONSECUTIVE_ERRORS:
                 self.logger.warning("Agent loop finished due to reaching max consecutive errors.")
             elif not self.state.workflow_id:
                 self.logger.info("Agent loop finished because no workflow is active.")
             else: # Generic case if workflow still active but loop ended
                 self.logger.warning("Agent loop finished for unexpected reason (goal not achieved, loop limit not reached).")


        # Final state save and cleanup handled by shutdown() or run_agent_process() finally block


# =============================================================================
# Driver helpers & CLI entry‚Äëpoint
# =============================================================================

async def run_agent_process(
    mcp_server_url: str,
    anthropic_key: str,
    goal: str,
    max_loops: int,
    state_file: str,
    config_file: Optional[str],
) -> None:
    """
    Sets up the MCPClient, Agent Master Loop, signal handling,
    and runs the main agent execution loop.
    """
    # Ensure MCPClient is available
    if not MCP_CLIENT_AVAILABLE:
        print("‚ùå ERROR: MCPClient dependency not met.")
        sys.exit(1)

    mcp_client_instance = None
    agent_loop_instance = None
    exit_code = 0
    # Use standard print initially, switch to client's safe_print if available
    printer = print

    try:
        printer("Instantiating MCP Client...")
        # Initialize MCP Client (pass URL and optional config path)
        mcp_client_instance = MCPClient(base_url=mcp_server_url, config_path=config_file)
        # Use safe_print for console output if provided by the client
        if hasattr(mcp_client_instance, 'safe_print') and callable(mcp_client_instance.safe_print):
            printer = mcp_client_instance.safe_print
            log.info("Using MCPClient's safe_print for output.")

        # Configure API key if not already set in config
        if not mcp_client_instance.config.api_key:
            if anthropic_key:
                printer("Using provided Anthropic API key.")
                mcp_client_instance.config.api_key = anthropic_key
                # Re-initialize the anthropic client instance within MCPClient if necessary
                mcp_client_instance.anthropic = AsyncAnthropic(api_key=anthropic_key)
            else:
                # Critical error if key is missing
                printer("‚ùå CRITICAL ERROR: Anthropic API key missing in config and not provided.")
                raise ValueError("Anthropic API key missing.")

        printer("Setting up MCP Client connections...")
        # Perform necessary setup (like connecting to servers/discovery)
        await mcp_client_instance.setup(interactive_mode=False) # Run in non-interactive mode

        printer("Instantiating Agent Master Loop...")
        # Create the agent instance, passing the initialized MCP client and state file path
        agent_loop_instance = AgentMasterLoop(mcp_client_instance=mcp_client_instance, agent_state_file=state_file)

        # --- Signal Handling Setup ---
        # Get the current asyncio event loop
        loop = asyncio.get_running_loop()
        # Create an event to signal shutdown across tasks
        stop_event = asyncio.Event()

        # Define the signal handler function
        def signal_handler_wrapper(signum):
            signal_name = signal.Signals(signum).name
            log.warning(f"Signal {signal_name} received. Initiating graceful shutdown.")
            # Set the event to signal other tasks
            stop_event.set()
            # Trigger the agent's internal shutdown method asynchronously
            if agent_loop_instance:
                 asyncio.create_task(agent_loop_instance.shutdown())
            # Avoid calling loop.stop() directly, let tasks finish gracefully

        # Register the handler for SIGINT (Ctrl+C) and SIGTERM
        for sig in [signal.SIGINT, signal.SIGTERM]:
            try:
                loop.add_signal_handler(sig, signal_handler_wrapper, sig)
                log.debug(f"Registered signal handler for {sig.name}")
            except ValueError: # Might fail if handler already registered
                log.debug(f"Signal handler for {sig.name} may already be registered.")
            except NotImplementedError: # Signal handling might not be supported (e.g., Windows sometimes)
                log.warning(f"Signal handling for {sig.name} not supported on this platform.")


        printer("Initializing agent...")
        if not await agent_loop_instance.initialize():
             printer("‚ùå Agent initialization failed. Exiting.")
             exit_code = 1
             return # Exit early if initialization fails

        printer(f"Running Agent Loop for goal: \"{goal}\"")
        # Create tasks for the main agent run and for waiting on the stop signal
        run_task = asyncio.create_task(agent_loop_instance.run(goal=goal, max_loops=max_loops))
        stop_task = asyncio.create_task(stop_event.wait())

        # Wait for either the agent run to complete OR the stop signal to be received
        done, pending = await asyncio.wait(
            {run_task, stop_task},
            return_when=asyncio.FIRST_COMPLETED # Return as soon as one task finishes
        )

        # Handle shutdown signal completion
        if stop_task in done:
             printer("\n[yellow]Shutdown signal processed. Waiting for agent task to finalize...[/yellow]")
             # If the agent task is still running, attempt to cancel it
             if run_task in pending:
                 run_task.cancel()
                 try:
                     # Wait for the cancellation to be processed
                     await run_task
                 except asyncio.CancelledError:
                      log.info("Agent run task cancelled gracefully after signal.")
                 except Exception as e:
                      # Log error if cancellation failed unexpectedly
                      log.error(f"Exception during agent run task finalization after signal: {e}", exc_info=True)
             # Set standard exit code for signal interruption (like Ctrl+C)
             exit_code = 130

        # Handle normal agent completion or error completion
        elif run_task in done:
             try:
                 run_task.result() # Check for exceptions raised by the run task
                 log.info("Agent run task completed normally.")
             except Exception as e:
                 # If the run task raised an exception, log it and set error exit code
                 printer(f"\n‚ùå Agent loop finished with error: {e}")
                 log.error("Agent run task finished with an exception:", exc_info=True)
                 exit_code = 1


    except KeyboardInterrupt:
        # Fallback for Ctrl+C if signal handler doesn't catch it (e.g., during setup)
        printer("\n[yellow]KeyboardInterrupt caught (fallback).[/yellow]")
        exit_code = 130
    except ValueError as ve: # Catch specific config/value errors during setup
        printer(f"\n‚ùå Configuration Error: {ve}")
        exit_code = 2
    except Exception as main_err:
        # Catch any other critical errors during setup or main execution
        printer(f"\n‚ùå Critical error during setup or execution: {main_err}")
        log.critical("Top-level execution error", exc_info=True)
        exit_code = 1
    finally:
        # --- Cleanup Sequence ---
        printer("Initiating final shutdown sequence...")
        # Ensure agent shutdown method is called (might be redundant if signaled, but safe)
        if agent_loop_instance and not agent_loop_instance._shutdown_event.is_set():
             printer("Ensuring agent loop shutdown...")
             await agent_loop_instance.shutdown() # Call directly if not already shutting down
        # Ensure MCP client connections are closed
        if mcp_client_instance:
            printer("Closing MCP client connections...")
            try:
                await mcp_client_instance.close()
            except Exception as close_err:
                printer(f"[red]Error closing MCP client:[/red] {close_err}")
        printer("Agent execution finished.")
        # Exit the process only if running as the main script
        if __name__ == "__main__":
            # Short delay to allow logs/output to flush before exiting
            await asyncio.sleep(0.5)
            sys.exit(exit_code)

# Main execution block when script is run directly
if __name__ == "__main__":
    # (Keep existing __main__ block for configuration loading and running)
    # Load configuration from environment variables or defaults
    MCP_SERVER_URL = os.environ.get("MCP_SERVER_URL", "http://localhost:8013")
    ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
    AGENT_GOAL = os.environ.get(
        "AGENT_GOAL", # Default goal for testing/example
        "Create workflow 'Tier 3 Test': Research Quantum Computing impact on Cryptography.",
    )
    MAX_ITER = int(os.environ.get("MAX_ITERATIONS", "30")) # Default max loops
    STATE_FILE = os.environ.get("AGENT_STATE_FILE", AGENT_STATE_FILE) # Use constant default
    CONFIG_PATH = os.environ.get("MCP_CLIENT_CONFIG") # Optional MCPClient config file path

    # Validate essential configuration
    if not ANTHROPIC_API_KEY:
        print("‚ùå ERROR: ANTHROPIC_API_KEY missing in environment variables.")
        sys.exit(1)
    if not MCP_CLIENT_AVAILABLE:
        # This check happens earlier, but keep for robustness
        print("‚ùå ERROR: MCPClient dependency missing.")
        sys.exit(1)

    # Display configuration being used before starting
    print(f"--- {AGENT_NAME} ---")
    print(f"Memory System URL: {MCP_SERVER_URL}")
    print(f"Agent Goal: {AGENT_GOAL}")
    print(f"Max Iterations: {MAX_ITER}")
    print(f"State File: {STATE_FILE}")
    print(f"Client Config: {CONFIG_PATH or 'Default internal config'}")
    print(f"Log Level: {logging.getLevelName(log.level)}")
    print("Anthropic API Key: Found")
    print("-----------------------------------------")


    # Define the main async function to run the agent process
    async def _main() -> None:
        await run_agent_process(
            MCP_SERVER_URL,
            ANTHROPIC_API_KEY,
            AGENT_GOAL,
            MAX_ITER,
            STATE_FILE,
            CONFIG_PATH,
        )

    # Run the main async function using asyncio.run()
    try:
        asyncio.run(_main())
    except KeyboardInterrupt:
        # Catch Ctrl+C during initial asyncio.run setup if signal handler isn't active yet
        print("\n[yellow]Initial KeyboardInterrupt detected. Exiting.[/yellow]")
        sys.exit(130)
\end{minted}
\end{pageablecode}

% --- APPENDICES END HERE ---

\end{document}
